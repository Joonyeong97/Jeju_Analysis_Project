{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_검증.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-5ZITZAw9vF",
        "colab_type": "code",
        "outputId": "38960be1-79a4-4ca5-8063-4dc86fa35a5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from keras import layers, models\n",
        "from __future__ import print_function\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Bidirectional\n",
        "import numpy as np\n",
        "from keras import datasets\n",
        "from keras import backend as K\n",
        "from keras.utils import plot_model\n",
        "import matplotlib\n",
        "from matplotlib import ticker\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import load_model\n",
        "\n",
        "\n",
        "batch_size = 32  # Batch size for training.\n",
        "epochs = 100  # Number of epochs to train for.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "num_samples = 10000  # Number of samples to train on.\n",
        "# Path to the data txt file on disk.\n",
        "data_path = '/content/dataset.txt'\n",
        "\n",
        "# Vectorize the data.\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text = line.split('\\t')\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    target_text = '\\t' + target_text + '\\n'\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "# 전처리\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "print('Number of samples:', len(input_texts))\n",
        "print('Number of unique input tokens:', num_encoder_tokens)\n",
        "print('Number of unique output tokens:', num_decoder_tokens)\n",
        "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
        "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
        "\n",
        "\n",
        "# 문자 -> 숫자 변환용 사전\n",
        "input_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "# 학습에 사용할 데이터를 담을 3차원 배열\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "\n",
        "# 문장을 문자 단위로 원 핫 인코딩하면서 학습용 데이터를 만듬\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
        "    for t, char in enumerate(target_text):\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
        "        if t > 0:\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
        "\n",
        "# 숫자 -> 문자 변환용 사전\n",
        "reverse_input_char_index = dict(\n",
        "    (i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict(\n",
        "    (i, char) for char, i in target_token_index.items())\n",
        "\n",
        "\n",
        "# 인코더 생성\n",
        "encoder_inputs = layers.Input(shape=(max_encoder_seq_length, num_encoder_tokens))\n",
        "# dropout 전\n",
        "# encoder = layers.GRU(latent_dim, return_sequences=True, return_state=True)\n",
        "\n",
        "encoder = layers.GRU(latent_dim,dropout=0.25,recurrent_dropout=0.25, return_sequences=True, return_state=True)\n",
        "encoder_outputs, state_h = encoder(encoder_inputs)\n",
        "\n",
        "\n",
        "# 디코더 생성\n",
        "decoder_inputs = layers.Input(shape=(max_decoder_seq_length, num_decoder_tokens))\n",
        "# dropout 전\n",
        "# decoder = layers.GRU(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder = layers.GRU(latent_dim,dropout=0.25,recurrent_dropout=0.25, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _ = decoder(decoder_inputs, initial_state=state_h)\n",
        "\n",
        "# attention 생성\n",
        "'''\n",
        "어텐션의 기본 아이디어는 디코더에서 출력 단어를 예측하는 매 시점(time step)마다, \n",
        "인코더에서의 전체 입력 문장을 다시 한 번 참고한다는 점입니다. \n",
        "단, 전체 입력 문장을 전부 다 동일한 비율로 참고하는 것이 아니라, \n",
        "해당 시점에서 예측해야할 단어와 연관이 있는 입력 단어 부분을 좀 더 \n",
        "집중(attention)해서 보게 됩니다.\n",
        "'''\n",
        "def RepeatVectorLayer(rep, axis):\n",
        "  return layers.Lambda(lambda x: K.repeat_elements(K.expand_dims(x, axis), rep, axis),\n",
        "                      lambda x: tuple((x[0],) + x[1:axis] + (rep,) + x[axis:]))\n",
        "# repeat_elements 함수는 1차원에 벡터에 b=[01] 이 있다고 가정하면\n",
        "# repeat_elements(b, rep=2, axis=0) 이렇게 적으면 행으로 \n",
        "# 0 0\n",
        "# 1 1\n",
        "# 이런식으로 증가\n",
        "\n",
        "\n",
        "# \n",
        "# K.expand_dims 은\n",
        "# K.expand_dims(n,0) 이렇게적으면 2*2 행렬이 1*2*2 행렬로\n",
        "# K.expand_dims(n,1) 이렇게적으면 2*2 행렬이 2*1*2 행렬로 차원을 증가시켜줌\n",
        "# 기존엔 reshape 으로 하던걸 텐서에서는 K.expand_dims를 사용\n",
        "#\n",
        "\n",
        "repeat_d_layer = RepeatVectorLayer(max_encoder_seq_length, 2)\n",
        "# 입력을 n 번 반복합니다.\n",
        "repeat_d = repeat_d_layer(decoder_outputs)\n",
        "\n",
        "repeat_e_layer = RepeatVectorLayer(max_decoder_seq_length, 1)\n",
        "# 입력을 n 번 반복합니다.\n",
        "repeat_e = repeat_e_layer(encoder_outputs)\n",
        "\n",
        "concat_for_score_layer = layers.Concatenate(axis=-1)\n",
        "#layers.Concatenate는 입력 목록을 연결하는 계층입니다.\n",
        "# 연결 축을 제외하고 모두 동일한 모양의 텐서 목록을 입력으로 사용하고 \n",
        "# 모든 입력의 연결 인 단일 텐서를 반환합니다.\n",
        "concat_for_score = concat_for_score_layer([repeat_d, repeat_e])\n",
        "\n",
        "dense1_t_score_layer = layers.Dense(latent_dim // 2, activation='tanh')\n",
        "# Dense 클래스 객체를 TimeDistributed wrapper를 사용하여 3차원 텐서 입력을 받을 수 있게 확장\n",
        "dense1_score_layer = layers.TimeDistributed(dense1_t_score_layer)\n",
        "dense1_score = dense1_score_layer(concat_for_score)\n",
        "\n",
        "\n",
        "dense2_t_score_layer = layers.Dense(1)\n",
        "# Dense 클래스 객체를 TimeDistributed wrapper를 사용하여 3차원 텐서 입력을 받을 수 있게 확장\n",
        "dense2_score_layer = layers.TimeDistributed(dense2_t_score_layer)\n",
        "dense2_score = dense2_score_layer(dense1_score)\n",
        "dense2_score = layers.Reshape((max_decoder_seq_length, max_encoder_seq_length))(dense2_score)\n",
        "\n",
        "# soft max 설정\n",
        "softmax_score_layer = layers.Softmax(axis=-1)\n",
        "softmax_score = softmax_score_layer(dense2_score)\n",
        "\n",
        "# 입력을 n 번 반복합니다 RepeatVectorLayer\n",
        "repeat_score_layer = RepeatVectorLayer(latent_dim, 2)\n",
        "repeat_score = repeat_score_layer(softmax_score)\n",
        "\n",
        "# layers.Permute 주어진 패턴에 따라 입력 치수를 변경합니다.\n",
        "permute_e = layers.Permute((2, 1))(encoder_outputs)\n",
        "repeat_e_layer = RepeatVectorLayer(max_decoder_seq_length, 1)\n",
        "repeat_e = repeat_e_layer(permute_e)\n",
        "\n",
        "attended_mat_layer = layers.Multiply() # 행렬곱\n",
        "attended_mat = attended_mat_layer([repeat_score, repeat_e])\n",
        "\n",
        "context_layer = layers.Lambda(lambda x: K.sum(x, axis=-1),\n",
        "                             lambda x: tuple(x[:-1]))\n",
        "context = context_layer(attended_mat)\n",
        "\n",
        "concat_context_layer = layers.Concatenate(axis=-1)\n",
        "'''입력 목록을 연결하는 계층입니다.\n",
        "연결 축을 제외하고 모두 동일한 모양의 텐서 목록을 입력으로 \n",
        "사용하고 모든 입력의 연결 인 단일 텐서를 반환합니다.'''\n",
        "concat_context = concat_context_layer([context, decoder_outputs])\n",
        "\n",
        "attention_dense_output_layer = layers.Dense(latent_dim, activation='tanh')\n",
        "attention_output_layer = layers.TimeDistributed(attention_dense_output_layer)\n",
        "attention_output = attention_output_layer(concat_context)\n",
        "\n",
        "decoder_dense = layers.Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(attention_output)\n",
        "model = models.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# # 모델 생성\n",
        "\n",
        "# che = 'atten_GRU_weight_g.h5'\n",
        "# point = ModelCheckpoint(filepath=che , monitor='val_loss', verbose=1, save_best_only=True)\n",
        "# early_stopping = EarlyStopping(monitor='val_loss', patience=100)\n",
        "# # Run training\n",
        "\n",
        "# model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "#                     batch_size=batch_size,\n",
        "#                     epochs=epochs,\n",
        "#                     validation_split=0.2,\n",
        "#                     verbose=1,callbacks=[point,early_stopping])\n",
        "# # Save model\n",
        "# model.save('atten_GRU_weight.h5')\n",
        "\n",
        "\n",
        "# # \n",
        "# from keras.models import model_from_json\n",
        "# json_file = open(\"model.json\", \"r\")\n",
        "# loaded_model_json = json_file.read()\n",
        "# json_file.close()\n",
        "# loaded_model = model_from_json(loaded_model_json)\n",
        "\n",
        "model.load_weights(\"wh.h5\")\n",
        "print(\"Loaded model from disk\")\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "#model = load_model('/content/atten_GRU_weight_drop0.25.h5')\n",
        "\n",
        "# Next: inference mode (sampling).\n",
        "# Here's the drill:\n",
        "# 1) encode input and retrieve initial decoder state\n",
        "# 2) run one step of decoder with this initial state\n",
        "# and a \"start of sequence\" token as target.\n",
        "# Output will be the next target token\n",
        "# 3) Repeat with the current target token and current states\n",
        "\n",
        "# # 어텐션 검증\n",
        "# test_data_num = 0\n",
        "# test_max_len = 0\n",
        "# for i, s in enumerate(input_texts):\n",
        "#   if len(s) > test_max_len:\n",
        "#     test_max_len = len(s)\n",
        "#     test_data_num = i\n",
        "\n",
        "# test_enc_input = encoder_input_data[test_data_num].reshape(\n",
        "#     (1, max_encoder_seq_length, num_encoder_tokens))\n",
        "# test_dec_input = decoder_input_data[test_data_num].reshape(\n",
        "#     (1, max_decoder_seq_length, num_decoder_tokens))\n",
        "\n",
        "# attention_layer = softmax_score_layer\n",
        "# func = K.function([encoder_inputs, decoder_inputs] + [K.learning_phase()], [attention_layer.output])\n",
        "# score_values = func([test_enc_input, test_dec_input, 1.0])[0]\n",
        "# score_values = score_values.reshape((max_decoder_seq_length, max_encoder_seq_length))\n",
        "\n",
        "# score_values = score_values[:len(target_texts[test_data_num])-1, :len(input_texts[test_data_num])]\n",
        "\n",
        "# fig = plt.figure()\n",
        "# ax = fig.add_subplot(111)\n",
        "# cax = ax.matshow(score_values, interpolation='nearest')\n",
        "# fig.colorbar(cax)\n",
        "\n",
        "# test_enc_names = []\n",
        "# for vec in test_enc_input[0]:\n",
        "#   sampled_token_index = np.argmax(vec)\n",
        "#   sampled_char = reverse_input_char_index[sampled_token_index]\n",
        "#   test_enc_names.append(sampled_char)\n",
        "# test_dec_names = []\n",
        "# for vec in test_dec_input[0]:\n",
        "#   sampled_token_index = np.argmax(vec)\n",
        "#   sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "#   test_dec_names.append(sampled_char)\n",
        "\n",
        "# print(test_dec_names[1:len(target_texts[test_data_num])])\n",
        "\n",
        "# ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "# ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "# ax.set_yticklabels(['']+test_dec_names[1:-1] + ['<END>'])\n",
        "# ax.set_xticklabels(['']+test_enc_names)\n",
        "\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# 모델추론단계\n",
        "encoder_model = models.Model(encoder_inputs, [encoder_outputs, state_h])\n",
        "encoder_outputs_input = layers.Input(shape=(max_encoder_seq_length, latent_dim))\n",
        "\n",
        "decoder_inputs = layers.Input(shape=(1, num_decoder_tokens))\n",
        "decoder_state_input_h = layers.Input(shape=(latent_dim,))\n",
        "decoder_outputs, decoder_h = decoder(decoder_inputs, initial_state=decoder_state_input_h)\n",
        "\n",
        "repeat_d_layer = RepeatVectorLayer(max_encoder_seq_length, 2)\n",
        "repeat_d = repeat_d_layer(decoder_outputs)\n",
        "\n",
        "repeat_e_layer = RepeatVectorLayer(1, axis=1)\n",
        "repeat_e = repeat_e_layer(encoder_outputs_input)\n",
        "\n",
        "concat_for_score_layer = layers.Concatenate(axis=-1)\n",
        "concat_for_score = concat_for_score_layer([repeat_d, repeat_e])\n",
        "\n",
        "dense1_score_layer = layers.TimeDistributed(dense1_t_score_layer)\n",
        "dense1_score = dense1_score_layer(concat_for_score)\n",
        "\n",
        "dense2_score_layer = layers.TimeDistributed(dense2_t_score_layer)\n",
        "dense2_score = dense2_score_layer(dense1_score)\n",
        "dense2_score = layers.Reshape((1, max_encoder_seq_length))(dense2_score)\n",
        "\n",
        "softmax_score_layer = layers.Softmax(axis=-1)\n",
        "softmax_score = softmax_score_layer(dense2_score)\n",
        "\n",
        "repeat_score_layer = RepeatVectorLayer(latent_dim, 2)\n",
        "repeat_score = repeat_score_layer(softmax_score)\n",
        "\n",
        "permute_e = layers.Permute((2, 1))(encoder_outputs_input)\n",
        "repeat_e_layer = RepeatVectorLayer(1, axis=1)\n",
        "repeat_e = repeat_e_layer(permute_e)\n",
        "\n",
        "attended_mat_layer = layers.Multiply()\n",
        "attended_mat = attended_mat_layer([repeat_score, repeat_e])\n",
        "\n",
        "context_layer = layers.Lambda(lambda x: K.sum(x, axis=-1),\n",
        "                             lambda x: tuple(x[:-1]))\n",
        "context = context_layer(attended_mat)\n",
        "\n",
        "concat_context_layer = layers.Concatenate(axis=-1)\n",
        "concat_context = concat_context_layer([context, decoder_outputs])\n",
        "\n",
        "attention_output_layer = layers.TimeDistributed(attention_dense_output_layer)\n",
        "attention_output = attention_output_layer(concat_context)\n",
        "\n",
        "decoder_att_outputs = decoder_dense(attention_output)\n",
        "\n",
        "decoder_model = models.Model([decoder_inputs, decoder_state_input_h, encoder_outputs_input],\n",
        "                            [decoder_outputs, decoder_h, decoder_att_outputs])\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples: 1175\n",
            "Number of unique input tokens: 780\n",
            "Number of unique output tokens: 719\n",
            "Max sequence length for inputs: 165\n",
            "Max sequence length for outputs: 183\n",
            "Loaded model from disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6r1N_PrxC9N",
        "colab_type": "code",
        "outputId": "f7910e41-424e-4877-867b-0f94cda116f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "  # 입력 문장을 인코딩\n",
        "  enc_outputs, states_value = encoder_model.predict(input_seq)\n",
        " \n",
        "  # 디코더의 입력으로 쓸 단일 문자\n",
        "  target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "  # 첫 입력은 시작 문자인 '\\t'로 설정\n",
        "  target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        " \n",
        "  # 문장 생성\n",
        "  stop_condition = False\n",
        "  decoded_sentence = ''\n",
        "  while not stop_condition:\n",
        "    # 이전의 출력, 상태를 디코더에 넣어서 새로운 출력, 상태를 얻음\n",
        "    # 이전 문자와 상태로 다음 문자와 상태를 얻는다고 보면 됨.\n",
        "    dec_outputs, h, output_tokens = decoder_model.predict(\n",
        "        [target_seq, states_value, enc_outputs])\n",
        " \n",
        "    # 사전을 사용해서 원 핫 인코딩 출력을 실제 문자로 변환\n",
        "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "    sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "    decoded_sentence += sampled_char\n",
        " \n",
        "    # 종료 문자가 나왔거나 문장 길이가 한계를 넘으면 종료\n",
        "    if (sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length):\n",
        "      stop_condition = True\n",
        " \n",
        "    # 디코더의 다음 입력으로 쓸 데이터 갱신\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    target_seq[0, 0, sampled_token_index] = 1.\n",
        "    \n",
        "    states_value = h\n",
        " \n",
        "  return decoded_sentence\n",
        "result= []\n",
        "for seq_index in range(1,200,1):\n",
        "  \n",
        "  input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
        "  decoded_sentence = decode_sequence(input_seq)\n",
        "  print('\"{}\" -> \"{}\"'.format(input_texts[seq_index], decoded_sentence.strip()))\n",
        "  result.append(decoded_sentence.strip())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"강생이\" -> \"강아지\"\n",
            "\"부각허다\" -> \"부글부글하다\"\n",
            "\"강알\" -> \"강아지\"\n",
            "\"부끄다\" -> \"부끄러지다\"\n",
            "\"개끔\" -> \"거품\"\n",
            "\"분시몰랑\" -> \"정황도 모르고\"\n",
            "\"개작개작\" -> \"개가 뒤는 모습\"\n",
            "\"삐암데기\" -> \"뺨\"\n",
            "\"검질\" -> \"잡초\"\n",
            "\"속슴허라\" -> \"말하지말라\"\n",
            "\"게미융허다\" -> \"희미하다\"\n",
            "\"솜쫄르멍\" -> \"숨막히는\"\n",
            "\"게작헌\" -> \"입이 큰\"\n",
            "\"쉰달이\" -> \"유산식품\"\n",
            "\"고라불켜\" -> \"고자질한다\"\n",
            "\"심토맥이\" -> \"마음 씀씀이\"\n",
            "\"곡기다\" -> \"숨막히다\"\n",
            "\"영\" -> \"이렇게\"\n",
            "\"골다\" -> \"차례로바꾸다\"\n",
            "\"왁왁허다\" -> \"캄캄하다\"\n",
            "\"곱지다\" -> \"숨기다\"\n",
            "\"요망지다\" -> \"똑똑하다\"\n",
            "\"과랑과랑\" -> \"쨍쨍한 햇볕속에 일 하는다.\"\n",
            "\"우영밭\" -> \"텃밭\"\n",
            "\"괸당\" -> \"친척\"\n",
            "\"웃뜨리\" -> \"산간마을\"\n",
            "\"굽\" -> \"밑바닥\"\n",
            "\"재짝재짝\" -> \"걷는 모습\"\n",
            "\"기시리다\" -> \"그을리다\"\n",
            "\"제라헌\" -> \"제대로 된\"\n",
            "\"꽝\" -> \"뼈\"\n",
            "\"조그물다\" -> \"다물다\"\n",
            "\"촐래\" -> \"반찬\"\n",
            "\"주레사니\" -> \"보리피리\"\n",
            "\"니치름\" -> \"상복치\"\n",
            "\"촘아가라\" -> \"어이없다\"\n",
            "\"데껴불켜\" -> \"던져버린다\"\n",
            "\"칭원허다\" -> \"원통하다\"\n",
            "\"두렁청허다\" -> \"얼떨떨하다\"\n",
            "\"콥데사니\" -> \"마늘\"\n",
            "\"들러켬쩌\" -> \"날뛰다\"\n",
            "\"코시롱\" -> \"향기\"\n",
            "\"허다\" -> \"다섯\"\n",
            "\"듬삭하다\" -> \"푸짐하다\"\n",
            "\"탈\" -> \"딸기\"\n",
            "\"멜라지켜\" -> \"찌그러지다\"\n",
            "\"통시\" -> \"화장실\"\n",
            "\"메기독딱\" -> \"음도 없음\"\n",
            "\"트멍\" -> \"틈새\"\n",
            "\"모소완\" -> \"무서워\"\n",
            "\"하근디\" -> \"이곳저곳\"\n",
            "\"몬뜨글락\" -> \"홀랑벗은모습\"\n",
            "\"하다\" -> \"재주가다\"\n",
            "\"몬직당\" -> \"만지다가\"\n",
            "\"몬짝\" -> \"전부\"\n",
            "\"허당말타\" -> \"바다가 안 할 것이냐\"\n",
            "\"몰명허다\" -> \"하다\"\n",
            "\"헌저글라\" -> \"빨리빨리\"\n",
            "\"몽케다\" -> \"늦장부리다\"\n",
            "\"호썰\" -> \"조금\"\n",
            "\"무리다\" -> \"맛이가다\"\n",
            "\"호야\" -> \"목\"\n",
            "\"무사\" -> \"왜 그러지요?\"\n",
            "\"흐랑허다\" -> \"어지런한다\"\n",
            "\"배롱허다\" -> \"어둠속에 빛이 희미하다\"\n",
            "\"허운데기\" -> \"얼굴\"\n",
            "\"배지금허다\" -> \"호미하다\"\n",
            "\"쪼광\" -> \"모습\"\n",
            "\"물옷\" -> \"상복\"\n",
            "\"센 바당\" -> \"바다에 가면 바다\"\n",
            "\"앞바르\" -> \"가까운 바다\"\n",
            "\"난바르\" -> \"바가지\"\n",
            "\"산목 졸르면 비온다\" -> \"한라산의 중턱에 띠같은 구름이 끼면 비가 온다\"\n",
            "\"돌 갓쓰민 우친다\" -> \"달무리지\"\n",
            "\"곰새기 들럭 켬져\" -> \"돌고래가 날뛴다\"\n",
            "\"토끼썼다\" -> \"황사현상\"\n",
            "\"큰눈\" -> \"물안경\"\n",
            "\"소살\" -> \"사납게소리\"\n",
            "\"머정좋다\" -> \"해산물을 많이 잡았다\"\n",
            "\"제수가 좋다\" -> \"해산물을 많이 잡았다\"\n",
            "\"궐 면했다\" -> \"겨우 한두개 잡았다\"\n",
            "\"마당 바랏져\" -> \"바다가 잔잔하다\"\n",
            "\"큰누\" -> \"산더미 같은 파도\"\n",
            "\"작은누\" -> \"작은 파도\"\n",
            "\"절 치대긴다\" -> \"파도치는 친다\"\n",
            "\"하르방 \" -> \"할아버지\"\n",
            "\"할망 \" -> \"할머니\"\n",
            "\"아방 \" -> \"아버지\"\n",
            "\"어멍 \" -> \"어머니\"\n",
            "\"비바리 \" -> \"처녀\"\n",
            "\"괸당 \" -> \"친척\"\n",
            "\"걸바시 \" -> \"거지\"\n",
            "\"넹바리 \" -> \"색시\"\n",
            "\"다슴아돌 \" -> \"의붓아들\"\n",
            "\"말젯놈 \" -> \"세번째자식\"\n",
            "\"소나이 \" -> \"사나이\"\n",
            "\"성님 \" -> \"형님\"\n",
            "\"작산 거 \" -> \"어른이된\"\n",
            "\"좀녀 \" -> \"해녀\"\n",
            "\"촐람생이 \" -> \"경솔한사람\"\n",
            "\"홀아방 \" -> \"홀아비\"\n",
            "\"가달 \" -> \"다리\"\n",
            "\"꼴랑지 \" -> \"꼬리\"\n",
            "\"구뚱배기 \" -> \"귀쪽뺨\"\n",
            "\"꽝 \" -> \"뼈\"\n",
            "\"굴레 \" -> \"입\"\n",
            "\"대망생이 \" -> \"망아지\"\n",
            "\"등땡이 \" -> \"등어리\"\n",
            "\"또꼬망 \" -> \"똥구멍\"\n",
            "\"모감지 \" -> \"감물들인옷\"\n",
            "\"베 봉탱이 \" -> \"배 불뚝이\"\n",
            "\"베아지 볼라불라\" -> \"배 밟아버린다\"\n",
            "\"상판이 \" -> \"얼굴\"\n",
            "\"야게기 \" -> \"목\"\n",
            "\"야굴탁 \" -> \"턱\"\n",
            "\"임댕이 \" -> \"이마\"\n",
            "\"정겡이 \" -> \"종아리\"\n",
            "\"저껭이 \" -> \"겨드랑이\"\n",
            "\"조금태기 \" -> \"간지롭게\"\n",
            "\"좀짐팽이 \" -> \"종아리\"\n",
            "\"허운데기 \" -> \"얼굴\"\n",
            "\"허벅다리 \" -> \"넓적다리\"\n",
            "\"놋 \" -> \"얼굴\"\n",
            "\"간수메 \" -> \"통조림\"\n",
            "\"개역 \" -> \"미숫가루\"\n",
            "\"것 \" -> \"음식을\"\n",
            "\"괴기 \" -> \"돼지\"\n",
            "\"바당괴기 \" -> \"바닷고기\"\n",
            "\"돗괴기 \" -> \"돼지고기\"\n",
            "\"쇠괴기 \" -> \"쇠고기\"\n",
            "\"도괴기 \" -> \"돼지\"\n",
            "\"곤떡 \" -> \"쌀로만든하얀떡\"\n",
            "\"곤밥 \" -> \"흰쌀밥\"\n",
            "\"놈삐 \" -> \"무우\"\n",
            "\"대사니김치 \" -> \"마늘장아찌\"\n",
            "\"마농 \" -> \"마늘\"\n",
            "\"조배기 \" -> \"메밀수제비\"\n",
            "\"촐래 \" -> \"반찬\"\n",
            "\"촘지금 \" -> \"참기름\"\n",
            "\"짐치 \" -> \"김치\"\n",
            "\"촙쏠 \" -> \"찹쌀\"\n",
            "\"조팝 \" -> \"조밥\"\n",
            "\"갈옷 \" -> \"감물들인옷\"\n",
            "\"갈 중이 \" -> \"감물들인 고\"\n",
            "\"강알터진 바지 \" -> \"개구멍\"\n",
            "\"게와 \" -> \"호주머니\"\n",
            "\"단취 \" -> \"단추\"\n",
            "\"밀랑 페랭이 \" -> \"밀짚 모자\"\n",
            "\"보선 \" -> \"버선\"\n",
            "\"소중이 \" -> \"속옷\"\n",
            "\"신착 \" -> \"신짝\"\n",
            "\"찍신 \" -> \"짚신\"\n",
            "\"좀뱅이 \" -> \"잠방이\"\n",
            "\"등지게 \" -> \"감물들인베옷\"\n",
            "\"고장중이 \" -> \"고쟁이\"\n",
            "\"도폭 \" -> \"우두머니\"\n",
            "\"두루막 \" -> \"두루마기\"\n",
            "\"베불레기 \" -> \"아기옷\"\n",
            "\"우장 \" -> \"비온날\"\n",
            "\"저구리 \" -> \"고리\"\n",
            "\"지성귀 \" -> \"기저귀\"\n",
            "\"지서귀 \" -> \"기저귀\"\n",
            "\"쪼께 \" -> \"조끼\"\n",
            "\"치메 \" -> \"치마\"\n",
            "\"건대 \" -> \"건대\"\n",
            "\"사모관대 \" -> \"사모관대\"\n",
            "\"시미옷 \" -> \"상복\"\n",
            "\"제복 \" -> \"상제옷\"\n",
            "\"망근 \" -> \"망건\"\n",
            "\"방립 \" -> \"상갓닮은거\"\n",
            "\"벙것 \" -> \"벙거지\"\n",
            "\"상갓 \" -> \"삿갓\"\n",
            "\"탕근 \" -> \"탕건\"\n",
            "\"풍뎅이 \" -> \"남자모자\"\n",
            "\"휘양 \" -> \"여자모자\"\n",
            "\"낭저 \" -> \"빈네꽂은머리\"\n",
            "\"달리 \" -> \"땋은놈의머리\"\n",
            "\"빈네 \" -> \"비녀\"\n",
            "\"상퉁이 \" -> \"상투\"\n",
            "\"얼레기 \" -> \"머리빗\"\n",
            "\"얼레빗 \" -> \"머리빗\"\n",
            "\"쪽도리 \" -> \"족두리\"\n",
            "\"쳉빗 \" -> \"참빗\"\n",
            "\"과지 \" -> \"무당입는옷\"\n",
            "\"남신 \" -> \"나막신\"\n",
            "\"송락 \" -> \"무당들 쓰는 모자\"\n",
            "\"요령 \" -> \"방울\"\n",
            "\"복치메 \" -> \"상복치마\"\n",
            "\"구덕 \" -> \"바구니\"\n",
            "\"고량착 \" -> \"대로만든채롱\"\n",
            "\"낭푼이 \" -> \"양푼\"\n",
            "\"그릇대배기 \" -> \"물긷는그릇\"\n",
            "\"물구루마 \" -> \"마차\"\n",
            "\"바농 \" -> \"바늘\"\n",
            "\"박새기 \" -> \"바가지\"\n",
            "\"숟구락 \" -> \"수저\"\n",
            "\"제끄락 \" -> \"젓갈\"\n",
            "\"주멩기 \" -> \"주머니\"\n",
            "\"허벅 \" -> \"물을길어나르는통\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyMczuG7S42e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a6c37f0c-9794-4182-999c-549909fff189"
      },
      "source": [
        "result"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['강아지',\n",
              " '부글부글하다',\n",
              " '강아지',\n",
              " '부끄러지다',\n",
              " '거품',\n",
              " '정황도 모르고',\n",
              " '개가 뒤는 모습',\n",
              " '뺨',\n",
              " '잡초',\n",
              " '말하지말라',\n",
              " '희미하다',\n",
              " '숨막히는',\n",
              " '입이 큰',\n",
              " '유산식품',\n",
              " '고자질한다',\n",
              " '마음 씀씀이',\n",
              " '숨막히다',\n",
              " '이렇게',\n",
              " '차례로바꾸다',\n",
              " '캄캄하다',\n",
              " '숨기다',\n",
              " '똑똑하다',\n",
              " '쨍쨍한 햇볕속에 일 하는다.',\n",
              " '텃밭',\n",
              " '친척',\n",
              " '산간마을',\n",
              " '밑바닥',\n",
              " '걷는 모습',\n",
              " '그을리다',\n",
              " '제대로 된',\n",
              " '뼈',\n",
              " '다물다',\n",
              " '반찬',\n",
              " '보리피리',\n",
              " '상복치',\n",
              " '어이없다',\n",
              " '던져버린다',\n",
              " '원통하다',\n",
              " '얼떨떨하다',\n",
              " '마늘',\n",
              " '날뛰다',\n",
              " '향기',\n",
              " '다섯',\n",
              " '푸짐하다',\n",
              " '딸기',\n",
              " '찌그러지다',\n",
              " '화장실',\n",
              " '음도 없음',\n",
              " '틈새',\n",
              " '무서워',\n",
              " '이곳저곳',\n",
              " '홀랑벗은모습',\n",
              " '재주가다',\n",
              " '만지다가',\n",
              " '전부',\n",
              " '바다가 안 할 것이냐',\n",
              " '하다',\n",
              " '빨리빨리',\n",
              " '늦장부리다',\n",
              " '조금',\n",
              " '맛이가다',\n",
              " '목',\n",
              " '왜 그러지요?',\n",
              " '어지런한다',\n",
              " '어둠속에 빛이 희미하다',\n",
              " '얼굴',\n",
              " '호미하다',\n",
              " '모습',\n",
              " '상복',\n",
              " '바다에 가면 바다',\n",
              " '가까운 바다',\n",
              " '바가지',\n",
              " '한라산의 중턱에 띠같은 구름이 끼면 비가 온다',\n",
              " '달무리지',\n",
              " '돌고래가 날뛴다',\n",
              " '황사현상',\n",
              " '물안경',\n",
              " '사납게소리',\n",
              " '해산물을 많이 잡았다',\n",
              " '해산물을 많이 잡았다',\n",
              " '겨우 한두개 잡았다',\n",
              " '바다가 잔잔하다',\n",
              " '산더미 같은 파도',\n",
              " '작은 파도',\n",
              " '파도치는 친다',\n",
              " '할아버지',\n",
              " '할머니',\n",
              " '아버지',\n",
              " '어머니',\n",
              " '처녀',\n",
              " '친척',\n",
              " '거지',\n",
              " '색시',\n",
              " '의붓아들',\n",
              " '세번째자식',\n",
              " '사나이',\n",
              " '형님',\n",
              " '어른이된',\n",
              " '해녀',\n",
              " '경솔한사람',\n",
              " '홀아비',\n",
              " '다리',\n",
              " '꼬리',\n",
              " '귀쪽뺨',\n",
              " '뼈',\n",
              " '입',\n",
              " '망아지',\n",
              " '등어리',\n",
              " '똥구멍',\n",
              " '감물들인옷',\n",
              " '배 불뚝이',\n",
              " '배 밟아버린다',\n",
              " '얼굴',\n",
              " '목',\n",
              " '턱',\n",
              " '이마',\n",
              " '종아리',\n",
              " '겨드랑이',\n",
              " '간지롭게',\n",
              " '종아리',\n",
              " '얼굴',\n",
              " '넓적다리',\n",
              " '얼굴',\n",
              " '통조림',\n",
              " '미숫가루',\n",
              " '음식을',\n",
              " '돼지',\n",
              " '바닷고기',\n",
              " '돼지고기',\n",
              " '쇠고기',\n",
              " '돼지',\n",
              " '쌀로만든하얀떡',\n",
              " '흰쌀밥',\n",
              " '무우',\n",
              " '마늘장아찌',\n",
              " '마늘',\n",
              " '메밀수제비',\n",
              " '반찬',\n",
              " '참기름',\n",
              " '김치',\n",
              " '찹쌀',\n",
              " '조밥',\n",
              " '감물들인옷',\n",
              " '감물들인 고',\n",
              " '개구멍',\n",
              " '호주머니',\n",
              " '단추',\n",
              " '밀짚 모자',\n",
              " '버선',\n",
              " '속옷',\n",
              " '신짝',\n",
              " '짚신',\n",
              " '잠방이',\n",
              " '감물들인베옷',\n",
              " '고쟁이',\n",
              " '우두머니',\n",
              " '두루마기',\n",
              " '아기옷',\n",
              " '비온날',\n",
              " '고리',\n",
              " '기저귀',\n",
              " '기저귀',\n",
              " '조끼',\n",
              " '치마',\n",
              " '건대',\n",
              " '사모관대',\n",
              " '상복',\n",
              " '상제옷',\n",
              " '망건',\n",
              " '상갓닮은거',\n",
              " '벙거지',\n",
              " '삿갓',\n",
              " '탕건',\n",
              " '남자모자',\n",
              " '여자모자',\n",
              " '빈네꽂은머리',\n",
              " '땋은놈의머리',\n",
              " '비녀',\n",
              " '상투',\n",
              " '머리빗',\n",
              " '머리빗',\n",
              " '족두리',\n",
              " '참빗',\n",
              " '무당입는옷',\n",
              " '나막신',\n",
              " '무당들 쓰는 모자',\n",
              " '방울',\n",
              " '상복치마',\n",
              " '바구니',\n",
              " '대로만든채롱',\n",
              " '양푼',\n",
              " '물긷는그릇',\n",
              " '마차',\n",
              " '바늘',\n",
              " '바가지',\n",
              " '수저',\n",
              " '젓갈',\n",
              " '주머니',\n",
              " '물을길어나르는통']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgzt9KB6L9ay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_texts2 = []\n",
        "\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    _, target_text2 = line.split('\\t')\n",
        "    target_text2 = target_text2\n",
        "    target_texts2.append(target_text2)\n",
        "    # for char in target_text2:\n",
        "    #     if char not in target_characters:\n",
        "    #         target_characters.add(char)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgS-PZa6M1Yz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "987672f7-5fb5-4ad9-82a9-cb514e4acd53"
      },
      "source": [
        "tar = target_texts2[1:200]\n",
        "tar"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['강아지',\n",
              " '부글부글하다',\n",
              " '가랭이',\n",
              " '부풀어 오르다',\n",
              " '거품',\n",
              " '정황도 모르고',\n",
              " '밥을 추하게 먹는 모습',\n",
              " '뺨',\n",
              " '잡초',\n",
              " '말하지말라',\n",
              " '희미하다',\n",
              " '숨막히는',\n",
              " '입이 큰',\n",
              " '유산식품',\n",
              " '고자질한다',\n",
              " '마음 씀씀이',\n",
              " '숨막히다',\n",
              " '이렇게',\n",
              " '잘게부수다',\n",
              " '캄캄하다',\n",
              " '숨기다',\n",
              " '똑똑하다',\n",
              " '햇살이 눈부시게 비추는모습',\n",
              " '텃밭',\n",
              " '친족',\n",
              " '산간마을',\n",
              " '밑바닥',\n",
              " '걷는 모습',\n",
              " '그을리다',\n",
              " '제대로 된',\n",
              " '뼈',\n",
              " '다물다',\n",
              " '반찬',\n",
              " '보리피리',\n",
              " '침',\n",
              " '어이없다',\n",
              " '던져버린다',\n",
              " '원통하다',\n",
              " '얼떨떨하다',\n",
              " '마늘',\n",
              " '날뛰다',\n",
              " '향기',\n",
              " '하다',\n",
              " '푸짐하다',\n",
              " '딸기',\n",
              " '찌그러지다',\n",
              " '돼지우리가 있는 변소',\n",
              " '아무것도 없음',\n",
              " '틈새',\n",
              " '무서워',\n",
              " '이곳저곳',\n",
              " '홀랑벗은모습',\n",
              " '많다',\n",
              " '만지다가',\n",
              " '전부',\n",
              " '하다가 안 할 것이냐',\n",
              " '멍청하다',\n",
              " '빨리가자',\n",
              " '늦장부리다',\n",
              " '조금',\n",
              " '맛이가다',\n",
              " '등',\n",
              " '왜?',\n",
              " '늘어지다',\n",
              " '어둠속에 빛이 희미하다',\n",
              " '얼굴',\n",
              " '고소하다',\n",
              " '모습',\n",
              " '잠수복',\n",
              " '거칠고 노한 바다',\n",
              " '가까운 바다',\n",
              " '먼 바다',\n",
              " '한라산의 중턱에 띠같은 구름이 끼면 비가 온다',\n",
              " '달무리지면 비가 온다',\n",
              " '돌고래가 날뛴다',\n",
              " '황사현상',\n",
              " '물안경',\n",
              " '작살',\n",
              " '해산물을 많이 잡았다',\n",
              " '해산물을 많이 잡았다',\n",
              " '겨우 한두개 잡았다',\n",
              " '바다가 잔잔하다',\n",
              " '산더미 같은 파도',\n",
              " '작은 파도',\n",
              " '물결이 친다',\n",
              " ' 할아버지',\n",
              " ' 할머니',\n",
              " '아버지',\n",
              " ' 어머니',\n",
              " ' 처녀',\n",
              " ' 친척',\n",
              " ' 거지',\n",
              " ' 색시',\n",
              " ' 의붓아들',\n",
              " ' 세번째자식',\n",
              " ' 사나이',\n",
              " ' 형님',\n",
              " ' 어른이된 사람',\n",
              " ' 해녀',\n",
              " ' 경솔한사람',\n",
              " ' 홀아비 ',\n",
              " ' 다리',\n",
              " ' 꼬리',\n",
              " ' 귀쪽뺨',\n",
              " ' 뼈',\n",
              " ' 입',\n",
              " ' 머리',\n",
              " ' 등어리',\n",
              " ' 똥구멍',\n",
              " ' 멱살',\n",
              " ' 배 불뚝이',\n",
              " ' 배 밟아버린다',\n",
              " ' 얼굴',\n",
              " ' 목',\n",
              " ' 턱',\n",
              " ' 이마',\n",
              " ' 종아리',\n",
              " ' 겨드랑이',\n",
              " '간지롭게',\n",
              " ' 종아리',\n",
              " '머리카락',\n",
              " ' 넓적다리',\n",
              " ' 얼굴',\n",
              " ' 통조림',\n",
              " ' 미숫가루',\n",
              " ' 동물먹이',\n",
              " ' 고기',\n",
              " ' 바닷고기',\n",
              " ' 돼지고기',\n",
              " ' 쇠고기',\n",
              " ' 돼지고기',\n",
              " ' 쌀로만든하얀떡',\n",
              " ' 흰쌀밥',\n",
              " ' 무우',\n",
              " ' 마늘장아찌',\n",
              " ' 마늘',\n",
              " ' 메밀수제비',\n",
              " ' 반찬',\n",
              " ' 참기름',\n",
              " ' 김치',\n",
              " ' 찹쌀',\n",
              " ' 조밥',\n",
              " ' 감물들인옷',\n",
              " ' 감물들인 고의',\n",
              " ' 개구멍 바지',\n",
              " ' 호주머니',\n",
              " ' 단추',\n",
              " ' 밀짚 모자',\n",
              " ' 버선',\n",
              " ' 속옷',\n",
              " ' 신짝',\n",
              " ' 짚신',\n",
              " ' 잠방이',\n",
              " ' 감물들인베옷',\n",
              " ' 고쟁이',\n",
              " ' 우두머니',\n",
              " ' 두루마기',\n",
              " ' 아기옷',\n",
              " ' 비온날',\n",
              " ' 저고리 ',\n",
              " ' 기저귀',\n",
              " ' 기저귀',\n",
              " ' 조끼',\n",
              " ' 치마',\n",
              " ' 건대',\n",
              " ' 사모관대',\n",
              " ' 손자용상복',\n",
              " ' 상제옷',\n",
              " ' 망건',\n",
              " ' 상갓닮은거',\n",
              " ' 벙거지',\n",
              " ' 삿갓',\n",
              " ' 탕건',\n",
              " ' 남자모자',\n",
              " ' 여자모자',\n",
              " ' 빈네꽂은머리',\n",
              " ' 땋은놈의머리',\n",
              " ' 비녀',\n",
              " ' 상투',\n",
              " ' 머리빗',\n",
              " ' 머리빗',\n",
              " ' 족두리',\n",
              " ' 참빗',\n",
              " ' 무당입는옷',\n",
              " ' 나막신',\n",
              " ' 무당들 쓰는 모자',\n",
              " ' 방울',\n",
              " ' 상복치마',\n",
              " ' 바구니',\n",
              " ' 대로만든채롱',\n",
              " ' 양푼',\n",
              " ' 물긷는그릇',\n",
              " ' 마차',\n",
              " ' 바늘',\n",
              " ' 바가지',\n",
              " ' 수저',\n",
              " ' 젓갈',\n",
              " ' 주머니',\n",
              " ' 물을길어나르는통']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o00dOI1cNvtp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "b2307b49-a85c-4145-db87-f7db14197e20"
      },
      "source": [
        "def dap(result, tar):\n",
        "  import nltk.translate.bleu_score as bleu\n",
        "  candidate = []\n",
        "  references = []\n",
        "  dap = []\n",
        "  for i in range(0,len(result),1):\n",
        "    re = []\n",
        "    candidate = result[i]\n",
        "    re.append(tar[i])\n",
        "    dap.append(bleu.sentence_bleu(list(map(lambda ref: ref.split(), re)),candidate.split()))\n",
        "  return sum(dap)/len(result)\n",
        "# 성능테스트\n",
        "dap(result,tar)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8506021942442353"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vPrh_JmU9Pw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wlKdkp5U9R1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiIkSSrQqZ1V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "ac9b4dea-2258-40dd-8b09-1652f6cd3508"
      },
      "source": [
        "import nltk.translate.bleu_score as bleu\n",
        "\n",
        "# 예측값\n",
        "candidate = '강아지'\n",
        "\n",
        "# 정답값\n",
        "references = ['강아지']\n",
        "\n",
        "print(bleu.sentence_bleu(list(map(lambda ref: ref.split(), references)),candidate.split()))\n",
        "# NLTK 패키지 구현되어져 있는 코드로 계산한 BLEU 점수"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIbMIwFgPzjl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FT2z0YhxqZ6z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99rjfa2MUyey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1M2iUgkUyiQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgQrN1x3Uynq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtdlR3pUUyl2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnxNBal0UykL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02wcJRr1Uyg3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAkmMBdQFU0e",
        "colab_type": "code",
        "outputId": "f7de6ac1-e536-4934-f459-0ba1304aef3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        }
      },
      "source": [
        "batch_size = 32  # Batch size for training.\n",
        "epochs = 100  # Number of epochs to train for.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "num_samples = 10000  # Number of samples to train on.\n",
        "# Path to the data txt file on disk.\n",
        "data_path = '/content/test.txt'\n",
        "\n",
        "\n",
        "def datasets(data_path = '/content/test.txt'):\n",
        "    input_texts = []\n",
        "    target_texts = []\n",
        "    input_characters = set()\n",
        "    target_characters = set()\n",
        "    with open(data_path, 'r', encoding='utf-8') as f:\n",
        "        lines = f.read().split('\\n')\n",
        "\n",
        "    for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "        input_text, target_text = line.split('\\t')\n",
        "        # We use \"tab\" as the \"start sequence\" character\n",
        "        # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "        target_text = '\\t' + target_text + '\\n'\n",
        "        input_texts.append(input_text)\n",
        "        target_texts.append(target_text)\n",
        "        for char in input_text:\n",
        "            if char not in input_characters:\n",
        "                input_characters.add(char)\n",
        "        for char in target_text:\n",
        "            if char not in target_characters:\n",
        "                target_characters.add(char)\n",
        "    # 전처리\n",
        "    input_characters = sorted(list(input_characters))\n",
        "    target_characters = sorted(list(target_characters))\n",
        "    num_encoder_tokens = len(input_characters)\n",
        "    num_decoder_tokens = len(target_characters)\n",
        "    max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "    max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "    print('Number of samples:', len(input_texts))\n",
        "    print('Number of unique input tokens:', num_encoder_tokens)\n",
        "    print('Number of unique output tokens:', num_decoder_tokens)\n",
        "    print('Max sequence length for inputs:', max_encoder_seq_length)\n",
        "    print('Max sequence length for outputs:', max_decoder_seq_length)\n",
        "\n",
        "\n",
        "    # 문자 -> 숫자 변환용 사전\n",
        "    input_token_index = dict(\n",
        "        [(char, i) for i, char in enumerate(input_characters)])\n",
        "    target_token_index = dict(\n",
        "        [(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "    # 학습에 사용할 데이터를 담을 3차원 배열\n",
        "    encoder_input_data = np.zeros(\n",
        "        (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
        "        dtype='float32')\n",
        "    decoder_input_data = np.zeros(\n",
        "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "        dtype='float32')\n",
        "    decoder_target_data = np.zeros(\n",
        "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "        dtype='float32')\n",
        "\n",
        "    # 문장을 문자 단위로 원 핫 인코딩하면서 학습용 데이터를 만듬\n",
        "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "        for t, char in enumerate(input_text):\n",
        "            encoder_input_data[i, t, input_token_index[char]] = 1.\n",
        "        for t, char in enumerate(target_text):\n",
        "            decoder_input_data[i, t, target_token_index[char]] = 1.\n",
        "            if t > 0:\n",
        "                decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
        "\n",
        "    # 숫자 -> 문자 변환용 사전\n",
        "    reverse_input_char_index = dict(\n",
        "        (i, char) for char, i in input_token_index.items())\n",
        "    reverse_target_char_index = dict(\n",
        "        (i, char) for char, i in target_token_index.items())\n",
        "    \n",
        "\n",
        "    return encoder_input_data, decoder_target_data\n",
        "encoder_input,decoder_target = datasets(data_path)\n",
        "encoder_input_data[1:2]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples: 24\n",
            "Number of unique input tokens: 56\n",
            "Number of unique output tokens: 64\n",
            "Max sequence length for inputs: 7\n",
            "Max sequence length for outputs: 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0.]]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqSfrZdN6XlV",
        "colab_type": "code",
        "outputId": "1394bbd1-a2dc-4e46-e2a3-c4cbce0b136d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "encoder_model.predict(encoder_input_data[1:2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-f08c2f61f8c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_input_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m         \u001b[0;31m# Case 2: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1380\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    139\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_5 to have shape (165, 780) but got array with shape (7, 56)"
          ]
        }
      ]
    }
  ]
}