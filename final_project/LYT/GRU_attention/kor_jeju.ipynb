{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kor_jeju.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIMvHhr7XYBE",
        "colab_type": "code",
        "outputId": "ff1066c5-10b9-426f-924d-7d790a80c900",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "items = [1, 2, 3, 4, 5]\n",
        "squared = list(map(lambda x: x**3, items))\n",
        "squared"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 8, 27, 64, 125]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEIky_zshPjR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://neurowhai.tistory.com/292"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa4IReo6Cc7a",
        "colab_type": "code",
        "outputId": "686f61e0-9abf-499a-c8d6-73dd43f1dbed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "import tensorflow as tf\n",
        "device_lib.list_local_devices()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 3876883829019089860, name: \"/device:XLA_CPU:0\"\n",
              " device_type: \"XLA_CPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 5933003659533636849\n",
              " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n",
              " device_type: \"XLA_GPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 12620358854082576940\n",
              " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 15956161332\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 11978132078730237417\n",
              " physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNlo6VGXFllH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xGdGylMFawA",
        "colab_type": "code",
        "outputId": "134ad03e-6683-4020-e051-bf9dc7893d4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_Q2fuUMCOaO",
        "colab_type": "code",
        "outputId": "18077331-4472-48a1-8d2c-8cc460c2ff04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras import layers, models\n",
        "from __future__ import print_function\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Bidirectional, Dropout, Embedding\n",
        "import numpy as np\n",
        "from keras import datasets\n",
        "from keras import backend as K\n",
        "from keras.utils import plot_model\n",
        "import matplotlib\n",
        "from matplotlib import ticker\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "batch_size = 16  # Batch size for training.\n",
        "epochs = 100  # Number of epochs to train for.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "num_samples = 10000  # Number of samples to train on.\n",
        "# Path to the data txt file on disk.\n",
        "data_path = '/content/dataset.txt'\n",
        "\n",
        "# Vectorize the data.\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text = line.split('\\t')\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    target_text = '\\t' + target_text + '\\n'\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "# 전처리\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "print('Number of samples:', len(input_texts))\n",
        "print('Number of unique input tokens:', num_encoder_tokens)\n",
        "print('Number of unique output tokens:', num_decoder_tokens)\n",
        "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
        "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
        "\n",
        "\n",
        "# 문자 -> 숫자 변환용 사전\n",
        "input_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "# 학습에 사용할 데이터를 담을 3차원 배열\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "\n",
        "# 문장을 문자 단위로 원 핫 인코딩하면서 학습용 데이터를 만듬\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
        "    for t, char in enumerate(target_text):\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
        "        if t > 0:\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
        "\n",
        "# 숫자 -> 문자 변환용 사전\n",
        "reverse_input_char_index = dict(\n",
        "    (i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict(\n",
        "    (i, char) for char, i in target_token_index.items())\n",
        "\n",
        "def RepeatVectorLayer(rep, axis):\n",
        "  return layers.Lambda(lambda x: K.repeat_elements(K.expand_dims(x, axis), rep, axis),\n",
        "                      lambda x: tuple((x[0],) + x[1:axis] + (rep,) + x[axis:]))\n",
        "\n",
        "\n",
        "# 인코더 생성\n",
        "encoder_inputs = layers.Input(shape=(max_encoder_seq_length, num_encoder_tokens))\n",
        "# dropout 전\n",
        "#encoder = layers.GRU(latent_dim, return_sequences=True, return_state=True)\n",
        "encoder = layers.GRU(latent_dim,dropout=0.25,recurrent_dropout=0.25, return_sequences=True, return_state=True)\n",
        "encoder_outputs, state_h = encoder(encoder_inputs)\n",
        "\n",
        "\n",
        "# 디코더 생성\n",
        "decoder_inputs = layers.Input(shape=(max_decoder_seq_length, num_decoder_tokens))\n",
        "# dropout 전\n",
        "#decoder = layers.GRU(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder = layers.GRU(latent_dim,dropout=0.25,recurrent_dropout=0.25, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _ = decoder(decoder_inputs, initial_state=state_h)\n",
        "\n",
        "# embedding test\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# attention 생성\n",
        "'''\n",
        "어텐션의 기본 아이디어는 디코더에서 출력 단어를 예측하는 매 시점(time step)마다, \n",
        "인코더에서의 전체 입력 문장을 다시 한 번 참고한다는 점입니다. \n",
        "단, 전체 입력 문장을 전부 다 동일한 비율로 참고하는 것이 아니라, \n",
        "해당 시점에서 예측해야할 단어와 연관이 있는 입력 단어 부분을 좀 더 \n",
        "집중(attention)해서 보게 됩니다.\n",
        "'''\n",
        "\n",
        "repeat_d_layer = RepeatVectorLayer(max_encoder_seq_length, 2)\n",
        "# 입력을 n 번 반복합니다.\n",
        "repeat_d = repeat_d_layer(decoder_outputs)\n",
        "\n",
        "repeat_e_layer = RepeatVectorLayer(max_decoder_seq_length, 1)\n",
        "# 입력을 n 번 반복합니다.\n",
        "repeat_e = repeat_e_layer(encoder_outputs)\n",
        "\n",
        "concat_for_score_layer = layers.Concatenate(axis=-1)\n",
        "#layers.Concatenate는 입력 목록을 연결하는 계층입니다.\n",
        "# 연결 축을 제외하고 모두 동일한 모양의 텐서 목록을 입력으로 사용하고 \n",
        "# 모든 입력의 연결 인 단일 텐서를 반환합니다.\n",
        "concat_for_score = concat_for_score_layer([repeat_d, repeat_e])\n",
        "\n",
        "dense1_t_score_layer = layers.Dense(latent_dim // 2, activation='tanh')\n",
        "# Dense 클래스 객체를 TimeDistributed wrapper를 사용하여 3차원 텐서 입력을 받을 수 있게 확장\n",
        "dense1_score_layer = layers.TimeDistributed(dense1_t_score_layer)\n",
        "dense1_score = dense1_score_layer(concat_for_score)\n",
        "\n",
        "\n",
        "dense2_t_score_layer = layers.Dense(1)\n",
        "# Dense 클래스 객체를 TimeDistributed wrapper를 사용하여 3차원 텐서 입력을 받을 수 있게 확장\n",
        "dense2_score_layer = layers.TimeDistributed(dense2_t_score_layer)\n",
        "dense2_score = dense2_score_layer(dense1_score)\n",
        "dense2_score = layers.Reshape((max_decoder_seq_length, max_encoder_seq_length))(dense2_score)\n",
        "\n",
        "# soft max 설정\n",
        "softmax_score_layer = layers.Softmax(axis=-1)\n",
        "softmax_score = softmax_score_layer(dense2_score)\n",
        "\n",
        "# 입력을 n 번 반복합니다 RepeatVectorLayer\n",
        "repeat_score_layer = RepeatVectorLayer(latent_dim, 2)\n",
        "repeat_score = repeat_score_layer(softmax_score)\n",
        "\n",
        "# layers.Permute 주어진 패턴에 따라 입력 치수를 변경합니다.\n",
        "permute_e = layers.Permute((2, 1))(encoder_outputs)\n",
        "repeat_e_layer = RepeatVectorLayer(max_decoder_seq_length, 1)\n",
        "repeat_e = repeat_e_layer(permute_e)\n",
        "\n",
        "attended_mat_layer = layers.Multiply() # 행렬곱\n",
        "attended_mat = attended_mat_layer([repeat_score, repeat_e])\n",
        "\n",
        "context_layer = layers.Lambda(lambda x: K.sum(x, axis=-1),\n",
        "                             lambda x: tuple(x[:-1]))\n",
        "context = context_layer(attended_mat)\n",
        "\n",
        "concat_context_layer = layers.Concatenate(axis=-1)\n",
        "'''입력 목록을 연결하는 계층입니다.\n",
        "연결 축을 제외하고 모두 동일한 모양의 텐서 목록을 입력으로 \n",
        "사용하고 모든 입력의 연결 인 단일 텐서를 반환합니다.'''\n",
        "concat_context = concat_context_layer([context, decoder_outputs])\n",
        "\n",
        "attention_dense_output_layer = layers.Dense(latent_dim, activation='tanh')\n",
        "attention_output_layer = layers.TimeDistributed(attention_dense_output_layer)\n",
        "attention_output = attention_output_layer(concat_context)\n",
        "\n",
        "decoder_dense = layers.Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(attention_output)\n",
        "\n",
        "\n",
        "# 모델 생성\n",
        "model = models.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "che = 'total.h5'\n",
        "point = ModelCheckpoint(filepath=che , monitor='val_loss', verbose=1, save_best_only=True)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=100)\n",
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "history = model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_split=0.2,\n",
        "                    verbose=1,callbacks=[point,early_stopping])\n",
        "# Save model\n",
        "model_json = model.to_json()\n",
        "with open(\"model_dropX.json\", \"w\") as json_file : \n",
        "    json_file.write(model_json)\n",
        "model.save_weights(\"wh_re_drop.h5\")\n",
        "#model.save('atten_GRU_weight2.h5')\n",
        "# \n",
        "\n",
        "\n",
        "# Next: inference mode (sampling).\n",
        "# Here's the drill:\n",
        "# 1) 입력을 인코딩하고 초기 디코더 상태 검색\n",
        "# 2) 이 초기 상태로 디코더 한 단계 실행\n",
        "# \"시퀀스 시작\" 토큰을 대상으로 한다.\n",
        "# 출력이 다음 대상 토큰임\n",
        "# 3) 현재 대상 토큰 및 현재 상태로 반복\n",
        "\n",
        "# 샘플링 모델 정의\n",
        "encoder_model = models.Model(encoder_inputs, [encoder_outputs, state_h])\n",
        "encoder_outputs_input = layers.Input(shape=(max_encoder_seq_length, latent_dim))\n",
        "\n",
        "decoder_inputs = layers.Input(shape=(1, num_decoder_tokens))\n",
        "decoder_state_input_h = layers.Input(shape=(latent_dim,))\n",
        "decoder_outputs, decoder_h = decoder(decoder_inputs, initial_state=decoder_state_input_h)\n",
        "\n",
        "repeat_d_layer = RepeatVectorLayer(max_encoder_seq_length, 2)\n",
        "repeat_d = repeat_d_layer(decoder_outputs)\n",
        "\n",
        "repeat_e_layer = RepeatVectorLayer(1, axis=1)\n",
        "repeat_e = repeat_e_layer(encoder_outputs_input)\n",
        "\n",
        "concat_for_score_layer = layers.Concatenate(axis=-1)\n",
        "concat_for_score = concat_for_score_layer([repeat_d, repeat_e])\n",
        "\n",
        "dense1_score_layer = layers.TimeDistributed(dense1_t_score_layer)\n",
        "dense1_score = dense1_score_layer(concat_for_score)\n",
        "\n",
        "dense2_score_layer = layers.TimeDistributed(dense2_t_score_layer)\n",
        "dense2_score = dense2_score_layer(dense1_score)\n",
        "dense2_score = layers.Reshape((1, max_encoder_seq_length))(dense2_score)\n",
        "\n",
        "softmax_score_layer = layers.Softmax(axis=-1)\n",
        "softmax_score = softmax_score_layer(dense2_score)\n",
        "\n",
        "repeat_score_layer = RepeatVectorLayer(latent_dim, 2)\n",
        "repeat_score = repeat_score_layer(softmax_score)\n",
        "\n",
        "permute_e = layers.Permute((2, 1))(encoder_outputs_input)\n",
        "repeat_e_layer = RepeatVectorLayer(1, axis=1)\n",
        "repeat_e = repeat_e_layer(permute_e)\n",
        "\n",
        "attended_mat_layer = layers.Multiply()\n",
        "attended_mat = attended_mat_layer([repeat_score, repeat_e])\n",
        "\n",
        "context_layer = layers.Lambda(lambda x: K.sum(x, axis=-1),\n",
        "                             lambda x: tuple(x[:-1]))\n",
        "context = context_layer(attended_mat)\n",
        "\n",
        "concat_context_layer = layers.Concatenate(axis=-1)\n",
        "concat_context = concat_context_layer([context, decoder_outputs])\n",
        "\n",
        "attention_output_layer = layers.TimeDistributed(attention_dense_output_layer)\n",
        "attention_output = attention_output_layer(concat_context)\n",
        "\n",
        "decoder_att_outputs = decoder_dense(attention_output)\n",
        "\n",
        "decoder_model = models.Model([decoder_inputs, decoder_state_input_h, encoder_outputs_input],\n",
        "                            [decoder_outputs, decoder_h, decoder_att_outputs])\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "  # 입력 문장을 인코딩\n",
        "  enc_outputs, states_value = encoder_model.predict(input_seq)\n",
        " \n",
        "  # 디코더의 입력으로 쓸 단일 문자\n",
        "  target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "  # 첫 입력은 시작 문자인 '\\t'로 설정\n",
        "  target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        " \n",
        "  # 문장 생성\n",
        "  stop_condition = False\n",
        "  decoded_sentence = ''\n",
        "  while not stop_condition:\n",
        "    # 이전의 출력, 상태를 디코더에 넣어서 새로운 출력, 상태를 얻음\n",
        "    # 이전 문자와 상태로 다음 문자와 상태를 얻는다고 보면 됨.\n",
        "    dec_outputs, h, output_tokens = decoder_model.predict(\n",
        "        [target_seq, states_value, enc_outputs])\n",
        " \n",
        "    # 사전을 사용해서 원 핫 인코딩 출력을 실제 문자로 변환\n",
        "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "    sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "    decoded_sentence += sampled_char\n",
        " \n",
        "    # 종료 문자가 나왔거나 문장 길이가 한계를 넘으면 종료\n",
        "    if (sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length):\n",
        "      stop_condition = True\n",
        " \n",
        "    # 디코더의 다음 입력으로 쓸 데이터 갱신\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    target_seq[0, 0, sampled_token_index] = 1.\n",
        "    \n",
        "    states_value = h\n",
        " \n",
        "  return decoded_sentence\n",
        "\n",
        "for seq_index in range(30):\n",
        "  input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
        "  decoded_sentence = decode_sequence(input_seq)\n",
        "  print('\"{}\" -> \"{}\"'.format(input_texts[seq_index], decoded_sentence.strip()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Number of samples: 1175\n",
            "Number of unique input tokens: 780\n",
            "Number of unique output tokens: 719\n",
            "Max sequence length for inputs: 165\n",
            "Max sequence length for outputs: 183\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 165, 780)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, 183, 719)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "gru_1 (GRU)                     [(None, 165, 256), ( 796416      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "gru_2 (GRU)                     [(None, 183, 256), ( 749568      input_2[0][0]                    \n",
            "                                                                 gru_1[0][1]                      \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 183, 165, 256 0           gru_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 183, 165, 256 0           gru_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 183, 165, 512 0           lambda_1[0][0]                   \n",
            "                                                                 lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_1 (TimeDistrib (None, 183, 165, 128 65664       concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_2 (TimeDistrib (None, 183, 165, 1)  129         time_distributed_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 183, 165)     0           time_distributed_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "softmax_1 (Softmax)             (None, 183, 165)     0           reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 256, 165)     0           gru_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 183, 256, 165 0           softmax_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_4 (Lambda)               (None, 183, 256, 165 0           permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_1 (Multiply)           (None, 183, 256, 165 0           lambda_3[0][0]                   \n",
            "                                                                 lambda_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_5 (Lambda)               (None, 183, 256)     0           multiply_1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 183, 512)     0           lambda_5[0][0]                   \n",
            "                                                                 gru_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_3 (TimeDistrib (None, 183, 256)     131328      concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 183, 719)     184783      time_distributed_3[0][0]         \n",
            "==================================================================================================\n",
            "Total params: 1,927,888\n",
            "Trainable params: 1,927,888\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 940 samples, validate on 235 samples\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "940/940 [==============================] - 41s 44ms/step - loss: 0.3242 - acc: 0.0125 - val_loss: 0.5468 - val_acc: 0.0267\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.54680, saving model to total.h5\n",
            "Epoch 2/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.2919 - acc: 0.0148 - val_loss: 0.5120 - val_acc: 0.0287\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.54680 to 0.51196, saving model to total.h5\n",
            "Epoch 3/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.2776 - acc: 0.0161 - val_loss: 0.4874 - val_acc: 0.0327\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.51196 to 0.48736, saving model to total.h5\n",
            "Epoch 4/100\n",
            "940/940 [==============================] - 39s 41ms/step - loss: 0.2695 - acc: 0.0171 - val_loss: 0.4731 - val_acc: 0.0354\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.48736 to 0.47306, saving model to total.h5\n",
            "Epoch 5/100\n",
            "940/940 [==============================] - 39s 42ms/step - loss: 0.2626 - acc: 0.0181 - val_loss: 0.4613 - val_acc: 0.0371\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.47306 to 0.46134, saving model to total.h5\n",
            "Epoch 6/100\n",
            "940/940 [==============================] - 39s 41ms/step - loss: 0.2557 - acc: 0.0189 - val_loss: 0.4490 - val_acc: 0.0396\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.46134 to 0.44897, saving model to total.h5\n",
            "Epoch 7/100\n",
            "940/940 [==============================] - 39s 41ms/step - loss: 0.2486 - acc: 0.0200 - val_loss: 0.4390 - val_acc: 0.0396\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.44897 to 0.43899, saving model to total.h5\n",
            "Epoch 8/100\n",
            "940/940 [==============================] - 39s 41ms/step - loss: 0.2435 - acc: 0.0209 - val_loss: 0.4251 - val_acc: 0.0424\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.43899 to 0.42513, saving model to total.h5\n",
            "Epoch 9/100\n",
            "940/940 [==============================] - 39s 41ms/step - loss: 0.2363 - acc: 0.0217 - val_loss: 0.4280 - val_acc: 0.0408\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.42513\n",
            "Epoch 10/100\n",
            "940/940 [==============================] - 39s 41ms/step - loss: 0.2315 - acc: 0.0221 - val_loss: 0.4182 - val_acc: 0.0438\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.42513 to 0.41818, saving model to total.h5\n",
            "Epoch 11/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.2260 - acc: 0.0229 - val_loss: 0.4121 - val_acc: 0.0459\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.41818 to 0.41208, saving model to total.h5\n",
            "Epoch 12/100\n",
            "940/940 [==============================] - 39s 41ms/step - loss: 0.2218 - acc: 0.0235 - val_loss: 0.4143 - val_acc: 0.0442\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.41208\n",
            "Epoch 13/100\n",
            "940/940 [==============================] - 39s 42ms/step - loss: 0.2164 - acc: 0.0244 - val_loss: 0.4066 - val_acc: 0.0466\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.41208 to 0.40658, saving model to total.h5\n",
            "Epoch 14/100\n",
            "940/940 [==============================] - 38s 40ms/step - loss: 0.2117 - acc: 0.0252 - val_loss: 0.4042 - val_acc: 0.0460\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.40658 to 0.40424, saving model to total.h5\n",
            "Epoch 15/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.2080 - acc: 0.0254 - val_loss: 0.3992 - val_acc: 0.0471\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.40424 to 0.39917, saving model to total.h5\n",
            "Epoch 16/100\n",
            "940/940 [==============================] - 39s 41ms/step - loss: 0.2035 - acc: 0.0262 - val_loss: 0.3944 - val_acc: 0.0477\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.39917 to 0.39437, saving model to total.h5\n",
            "Epoch 17/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.1992 - acc: 0.0266 - val_loss: 0.3925 - val_acc: 0.0477\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.39437 to 0.39245, saving model to total.h5\n",
            "Epoch 18/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.1948 - acc: 0.0273 - val_loss: 0.3934 - val_acc: 0.0480\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.39245\n",
            "Epoch 19/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.1909 - acc: 0.0279 - val_loss: 0.3840 - val_acc: 0.0507\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.39245 to 0.38401, saving model to total.h5\n",
            "Epoch 20/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.1871 - acc: 0.0284 - val_loss: 0.3829 - val_acc: 0.0509\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.38401 to 0.38287, saving model to total.h5\n",
            "Epoch 21/100\n",
            "940/940 [==============================] - 39s 41ms/step - loss: 0.1838 - acc: 0.0289 - val_loss: 0.3881 - val_acc: 0.0507\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.38287\n",
            "Epoch 22/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.1788 - acc: 0.0298 - val_loss: 0.3819 - val_acc: 0.0513\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.38287 to 0.38190, saving model to total.h5\n",
            "Epoch 23/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.1746 - acc: 0.0301 - val_loss: 0.3780 - val_acc: 0.0518\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.38190 to 0.37805, saving model to total.h5\n",
            "Epoch 24/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.1716 - acc: 0.0305 - val_loss: 0.3776 - val_acc: 0.0525\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.37805 to 0.37757, saving model to total.h5\n",
            "Epoch 25/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.1677 - acc: 0.0313 - val_loss: 0.3757 - val_acc: 0.0517\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.37757 to 0.37573, saving model to total.h5\n",
            "Epoch 26/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.1658 - acc: 0.0314 - val_loss: 0.3718 - val_acc: 0.0531\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.37573 to 0.37185, saving model to total.h5\n",
            "Epoch 27/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.1609 - acc: 0.0322 - val_loss: 0.3753 - val_acc: 0.0527\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.37185\n",
            "Epoch 28/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.1564 - acc: 0.0329 - val_loss: 0.3760 - val_acc: 0.0538\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.37185\n",
            "Epoch 29/100\n",
            "940/940 [==============================] - 39s 41ms/step - loss: 0.1548 - acc: 0.0332 - val_loss: 0.3751 - val_acc: 0.0532\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.37185\n",
            "Epoch 30/100\n",
            "940/940 [==============================] - 39s 41ms/step - loss: 0.1518 - acc: 0.0338 - val_loss: 0.3759 - val_acc: 0.0536\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.37185\n",
            "Epoch 31/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.1481 - acc: 0.0342 - val_loss: 0.3675 - val_acc: 0.0542\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.37185 to 0.36750, saving model to total.h5\n",
            "Epoch 32/100\n",
            "940/940 [==============================] - 39s 41ms/step - loss: 0.1458 - acc: 0.0344 - val_loss: 0.3756 - val_acc: 0.0542\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.36750\n",
            "Epoch 33/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.1429 - acc: 0.0351 - val_loss: 0.3646 - val_acc: 0.0552\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.36750 to 0.36464, saving model to total.h5\n",
            "Epoch 34/100\n",
            "940/940 [==============================] - 39s 41ms/step - loss: 0.1389 - acc: 0.0358 - val_loss: 0.3702 - val_acc: 0.0546\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.36464\n",
            "Epoch 35/100\n",
            "940/940 [==============================] - 39s 41ms/step - loss: 0.1362 - acc: 0.0362 - val_loss: 0.3665 - val_acc: 0.0547\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.36464\n",
            "Epoch 36/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.1325 - acc: 0.0369 - val_loss: 0.3670 - val_acc: 0.0558\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.36464\n",
            "Epoch 37/100\n",
            "940/940 [==============================] - 39s 41ms/step - loss: 0.1294 - acc: 0.0373 - val_loss: 0.3721 - val_acc: 0.0548\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.36464\n",
            "Epoch 38/100\n",
            "940/940 [==============================] - 39s 41ms/step - loss: 0.1274 - acc: 0.0378 - val_loss: 0.3720 - val_acc: 0.0534\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.36464\n",
            "Epoch 39/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.1239 - acc: 0.0382 - val_loss: 0.3759 - val_acc: 0.0526\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.36464\n",
            "Epoch 40/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.1224 - acc: 0.0384 - val_loss: 0.3610 - val_acc: 0.0560\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.36464 to 0.36095, saving model to total.h5\n",
            "Epoch 41/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.1176 - acc: 0.0396 - val_loss: 0.3683 - val_acc: 0.0557\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.36095\n",
            "Epoch 42/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.1162 - acc: 0.0398 - val_loss: 0.3696 - val_acc: 0.0545\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.36095\n",
            "Epoch 43/100\n",
            "940/940 [==============================] - 39s 41ms/step - loss: 0.1134 - acc: 0.0403 - val_loss: 0.3659 - val_acc: 0.0566\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.36095\n",
            "Epoch 44/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.1116 - acc: 0.0404 - val_loss: 0.3694 - val_acc: 0.0549\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.36095\n",
            "Epoch 45/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.1098 - acc: 0.0407 - val_loss: 0.3687 - val_acc: 0.0552\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.36095\n",
            "Epoch 46/100\n",
            "940/940 [==============================] - 39s 41ms/step - loss: 0.1065 - acc: 0.0415 - val_loss: 0.3695 - val_acc: 0.0553\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.36095\n",
            "Epoch 47/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.1057 - acc: 0.0415 - val_loss: 0.3651 - val_acc: 0.0568\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.36095\n",
            "Epoch 48/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.1011 - acc: 0.0427 - val_loss: 0.3621 - val_acc: 0.0576\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.36095\n",
            "Epoch 49/100\n",
            "940/940 [==============================] - 39s 41ms/step - loss: 0.1011 - acc: 0.0427 - val_loss: 0.3680 - val_acc: 0.0566\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.36095\n",
            "Epoch 50/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.0967 - acc: 0.0436 - val_loss: 0.3658 - val_acc: 0.0561\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.36095\n",
            "Epoch 51/100\n",
            "940/940 [==============================] - 39s 41ms/step - loss: 0.0958 - acc: 0.0437 - val_loss: 0.3623 - val_acc: 0.0576\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.36095\n",
            "Epoch 52/100\n",
            "940/940 [==============================] - 39s 41ms/step - loss: 0.0949 - acc: 0.0437 - val_loss: 0.3713 - val_acc: 0.0565\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.36095\n",
            "Epoch 53/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.0917 - acc: 0.0445 - val_loss: 0.3650 - val_acc: 0.0575\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.36095\n",
            "Epoch 54/100\n",
            "940/940 [==============================] - 39s 42ms/step - loss: 0.0903 - acc: 0.0449 - val_loss: 0.3753 - val_acc: 0.0557\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.36095\n",
            "Epoch 55/100\n",
            "940/940 [==============================] - 39s 41ms/step - loss: 0.0859 - acc: 0.0459 - val_loss: 0.3781 - val_acc: 0.0570\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.36095\n",
            "Epoch 56/100\n",
            "940/940 [==============================] - 39s 41ms/step - loss: 0.0857 - acc: 0.0458 - val_loss: 0.3697 - val_acc: 0.0562\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.36095\n",
            "Epoch 57/100\n",
            "940/940 [==============================] - 39s 41ms/step - loss: 0.0847 - acc: 0.0462 - val_loss: 0.3677 - val_acc: 0.0576\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.36095\n",
            "Epoch 58/100\n",
            "940/940 [==============================] - 39s 41ms/step - loss: 0.0816 - acc: 0.0468 - val_loss: 0.3658 - val_acc: 0.0571\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.36095\n",
            "Epoch 59/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.0800 - acc: 0.0471 - val_loss: 0.3726 - val_acc: 0.0574\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.36095\n",
            "Epoch 60/100\n",
            "940/940 [==============================] - 39s 41ms/step - loss: 0.0792 - acc: 0.0473 - val_loss: 0.3731 - val_acc: 0.0574\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.36095\n",
            "Epoch 61/100\n",
            "940/940 [==============================] - 39s 41ms/step - loss: 0.0763 - acc: 0.0482 - val_loss: 0.3724 - val_acc: 0.0579\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.36095\n",
            "Epoch 62/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.0748 - acc: 0.0482 - val_loss: 0.3764 - val_acc: 0.0573\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.36095\n",
            "Epoch 63/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.0728 - acc: 0.0487 - val_loss: 0.3789 - val_acc: 0.0573\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.36095\n",
            "Epoch 64/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.0717 - acc: 0.0490 - val_loss: 0.3698 - val_acc: 0.0584\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.36095\n",
            "Epoch 65/100\n",
            "940/940 [==============================] - 39s 41ms/step - loss: 0.0692 - acc: 0.0496 - val_loss: 0.3745 - val_acc: 0.0576\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.36095\n",
            "Epoch 66/100\n",
            "940/940 [==============================] - 39s 41ms/step - loss: 0.0695 - acc: 0.0496 - val_loss: 0.3797 - val_acc: 0.0571\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.36095\n",
            "Epoch 67/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.0703 - acc: 0.0489 - val_loss: 0.3812 - val_acc: 0.0566\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.36095\n",
            "Epoch 68/100\n",
            "940/940 [==============================] - 39s 41ms/step - loss: 0.0670 - acc: 0.0501 - val_loss: 0.3843 - val_acc: 0.0574\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.36095\n",
            "Epoch 69/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.0644 - acc: 0.0506 - val_loss: 0.3873 - val_acc: 0.0566\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.36095\n",
            "Epoch 70/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.0639 - acc: 0.0506 - val_loss: 0.3825 - val_acc: 0.0579\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.36095\n",
            "Epoch 71/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.0632 - acc: 0.0511 - val_loss: 0.3808 - val_acc: 0.0568\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.36095\n",
            "Epoch 72/100\n",
            "940/940 [==============================] - 39s 41ms/step - loss: 0.0599 - acc: 0.0519 - val_loss: 0.3825 - val_acc: 0.0579\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.36095\n",
            "Epoch 73/100\n",
            "940/940 [==============================] - 38s 40ms/step - loss: 0.0598 - acc: 0.0521 - val_loss: 0.3806 - val_acc: 0.0580\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.36095\n",
            "Epoch 74/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.0588 - acc: 0.0519 - val_loss: 0.3820 - val_acc: 0.0584\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.36095\n",
            "Epoch 75/100\n",
            "940/940 [==============================] - 38s 40ms/step - loss: 0.0587 - acc: 0.0518 - val_loss: 0.3783 - val_acc: 0.0581\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.36095\n",
            "Epoch 76/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.0574 - acc: 0.0525 - val_loss: 0.3883 - val_acc: 0.0584\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.36095\n",
            "Epoch 77/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.0554 - acc: 0.0527 - val_loss: 0.3828 - val_acc: 0.0581\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.36095\n",
            "Epoch 78/100\n",
            "940/940 [==============================] - 38s 40ms/step - loss: 0.0545 - acc: 0.0530 - val_loss: 0.3888 - val_acc: 0.0574\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.36095\n",
            "Epoch 79/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.0541 - acc: 0.0534 - val_loss: 0.3823 - val_acc: 0.0585\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.36095\n",
            "Epoch 80/100\n",
            "940/940 [==============================] - 39s 41ms/step - loss: 0.0534 - acc: 0.0534 - val_loss: 0.4014 - val_acc: 0.0566\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.36095\n",
            "Epoch 81/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.0504 - acc: 0.0540 - val_loss: 0.3821 - val_acc: 0.0587\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.36095\n",
            "Epoch 82/100\n",
            "940/940 [==============================] - 39s 42ms/step - loss: 0.0491 - acc: 0.0545 - val_loss: 0.3902 - val_acc: 0.0579\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.36095\n",
            "Epoch 83/100\n",
            "940/940 [==============================] - 39s 42ms/step - loss: 0.0504 - acc: 0.0539 - val_loss: 0.3861 - val_acc: 0.0584\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.36095\n",
            "Epoch 84/100\n",
            "940/940 [==============================] - 39s 41ms/step - loss: 0.0484 - acc: 0.0545 - val_loss: 0.3839 - val_acc: 0.0588\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.36095\n",
            "Epoch 85/100\n",
            "940/940 [==============================] - 39s 41ms/step - loss: 0.0482 - acc: 0.0547 - val_loss: 0.3991 - val_acc: 0.0576\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.36095\n",
            "Epoch 86/100\n",
            "940/940 [==============================] - 39s 41ms/step - loss: 0.0484 - acc: 0.0541 - val_loss: 0.3895 - val_acc: 0.0581\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.36095\n",
            "Epoch 87/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.0455 - acc: 0.0552 - val_loss: 0.3941 - val_acc: 0.0574\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.36095\n",
            "Epoch 88/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.0452 - acc: 0.0550 - val_loss: 0.3957 - val_acc: 0.0577\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.36095\n",
            "Epoch 89/100\n",
            "940/940 [==============================] - 38s 40ms/step - loss: 0.0445 - acc: 0.0556 - val_loss: 0.3888 - val_acc: 0.0579\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.36095\n",
            "Epoch 90/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.0429 - acc: 0.0556 - val_loss: 0.3941 - val_acc: 0.0586\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.36095\n",
            "Epoch 91/100\n",
            "940/940 [==============================] - 39s 41ms/step - loss: 0.0434 - acc: 0.0555 - val_loss: 0.3893 - val_acc: 0.0582\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.36095\n",
            "Epoch 92/100\n",
            "940/940 [==============================] - 38s 40ms/step - loss: 0.0417 - acc: 0.0562 - val_loss: 0.4014 - val_acc: 0.0574\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.36095\n",
            "Epoch 93/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.0423 - acc: 0.0558 - val_loss: 0.3981 - val_acc: 0.0584\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.36095\n",
            "Epoch 94/100\n",
            "940/940 [==============================] - 39s 41ms/step - loss: 0.0398 - acc: 0.0563 - val_loss: 0.3995 - val_acc: 0.0574\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.36095\n",
            "Epoch 95/100\n",
            "940/940 [==============================] - 38s 40ms/step - loss: 0.0399 - acc: 0.0565 - val_loss: 0.3982 - val_acc: 0.0585\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.36095\n",
            "Epoch 96/100\n",
            "940/940 [==============================] - 38s 40ms/step - loss: 0.0398 - acc: 0.0566 - val_loss: 0.4000 - val_acc: 0.0581\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.36095\n",
            "Epoch 97/100\n",
            "940/940 [==============================] - 38s 40ms/step - loss: 0.0393 - acc: 0.0567 - val_loss: 0.3975 - val_acc: 0.0586\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.36095\n",
            "Epoch 98/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.0380 - acc: 0.0571 - val_loss: 0.3988 - val_acc: 0.0589\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.36095\n",
            "Epoch 99/100\n",
            "940/940 [==============================] - 38s 41ms/step - loss: 0.0368 - acc: 0.0573 - val_loss: 0.4051 - val_acc: 0.0576\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.36095\n",
            "Epoch 100/100\n",
            "940/940 [==============================] - 38s 40ms/step - loss: 0.0388 - acc: 0.0570 - val_loss: 0.3977 - val_acc: 0.0585\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.36095\n",
            "\"﻿버래기\" -> \"넘치도록\"\n",
            "\"강생이\" -> \"강아지\"\n",
            "\"부각허다\" -> \"부글부글하다\"\n",
            "\"강알\" -> \"아래가 없어지\"\n",
            "\"부끄다\" -> \"부풀어 오르다\"\n",
            "\"개끔\" -> \"거품\"\n",
            "\"분시몰랑\" -> \"정황도 모르고\"\n",
            "\"개작개작\" -> \"개구니\"\n",
            "\"삐암데기\" -> \"뺨\"\n",
            "\"검질\" -> \"잡초\"\n",
            "\"속슴허라\" -> \"말하지말라\"\n",
            "\"게미융허다\" -> \"희미하다\"\n",
            "\"솜쫄르멍\" -> \"숨막히는\"\n",
            "\"게작헌\" -> \"입이 큰\"\n",
            "\"쉰달이\" -> \"유산식품\"\n",
            "\"고라불켜\" -> \"고자질한다\"\n",
            "\"심토맥이\" -> \"마음 씀씀이\"\n",
            "\"곡기다\" -> \"고르게되다\"\n",
            "\"영\" -> \"이렇게\"\n",
            "\"골다\" -> \"차례로바꾸다\"\n",
            "\"왁왁허다\" -> \"캄캄하다\"\n",
            "\"곱지다\" -> \"숨기다\"\n",
            "\"요망지다\" -> \"똑똑하다\"\n",
            "\"과랑과랑\" -> \"햇살이 눈부시게 비추는모습\"\n",
            "\"우영밭\" -> \"밭\"\n",
            "\"괸당\" -> \"친척\"\n",
            "\"웃뜨리\" -> \"산간마을\"\n",
            "\"굽\" -> \"밑바닥\"\n",
            "\"재짝재짝\" -> \"걷는 모습\"\n",
            "\"기시리다\" -> \"그을리다\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_bfHvo3Wacs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = []\n",
        "for seq_index in range(1,len(input_texts),1):\n",
        "  \n",
        "  input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
        "  decoded_sentence = decode_sequence(input_seq)\n",
        "  #print('\"{}\" -> \"{}\"'.format(input_texts[seq_index], decoded_sentence.strip()))\n",
        "  result.append(decoded_sentence.strip())\n",
        "\n",
        "target_texts2 = []\n",
        "\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    _, target_text2 = line.split('\\t')\n",
        "    target_text2 = target_text2\n",
        "    target_texts2.append(target_text2)\n",
        "    # for char in target_text2:\n",
        "    #     if char not in target_characters:\n",
        "    #         target_characters.add(char)\n",
        "tar = target_texts2[1:len(input_texts)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_K8NWquzi4n",
        "colab_type": "code",
        "outputId": "3200e0cb-fb91-4420-ac5a-e274ed7963e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "source": [
        "def dap(result, tar):\n",
        "  import nltk.translate.bleu_score as bleu\n",
        "  candidate = []\n",
        "  references = []\n",
        "  dap = []\n",
        "  dd = []\n",
        "  for i in range(0,len(result)):\n",
        "    re = []\n",
        "    candidate = result[i]\n",
        "    re.append(tar[i])\n",
        "    dap.append(bleu.sentence_bleu(list(map(lambda ref: ref.split(), re)),candidate.split()))\n",
        "    if i % 10 == 0:\n",
        "      dd.append(bleu.sentence_bleu(list(map(lambda ref: ref.split(), re)),candidate.split()))\n",
        "      #print(\"예측값:\",candidate,'//',\"실제값:\",re,\"Bleu 점수:\",dd)\n",
        "      dd=[]\n",
        "  return sum(dap)/len(result)\n",
        "# 성능테스트\n",
        "print('전체점수 : ', dap(result,tar))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "전체점수 :  0.7839549046126238\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svBej46F9y7K",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEpF0Jdpc43g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBTIcYitc48D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O1zyyyvc455",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwMP_X3T-ZBe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpBMqB1y-ZFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebx0ZMlTvJMJ",
        "colab_type": "text"
      },
      "source": [
        "# 검증"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD_XvYHxfVn7",
        "colab_type": "code",
        "outputId": "82c1a862-8786-43e0-8592-31e6d266bba8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        }
      },
      "source": [
        "# 손실 그래프\n",
        "plt.plot(history.history['loss'], 'y', label='train loss')\n",
        "plt.plot(history.history['val_loss'], 'r', label='val loss')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 정확도 그래프\n",
        "# plt.plot(history.history['acc'], 'y', label='train acc')\n",
        "# plt.plot(history.history['val_acc'], 'r', label='val acc')\n",
        "# plt.legend(loc='upper left')\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD6CAYAAACoCZCsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZ5Sx8GZVq6T",
        "colab_type": "text"
      },
      "source": [
        "모델불러오고 시작"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odtxgzYXVqey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sp7puwJOTU8e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}