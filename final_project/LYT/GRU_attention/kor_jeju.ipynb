{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kor_jeju.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEIky_zshPjR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://neurowhai.tistory.com/292"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa4IReo6Cc7a",
        "colab_type": "code",
        "outputId": "686f61e0-9abf-499a-c8d6-73dd43f1dbed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "import tensorflow as tf\n",
        "device_lib.list_local_devices()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 3876883829019089860, name: \"/device:XLA_CPU:0\"\n",
              " device_type: \"XLA_CPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 5933003659533636849\n",
              " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n",
              " device_type: \"XLA_GPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 12620358854082576940\n",
              " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 15956161332\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 11978132078730237417\n",
              " physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNlo6VGXFllH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xGdGylMFawA",
        "colab_type": "code",
        "outputId": "134ad03e-6683-4020-e051-bf9dc7893d4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_Q2fuUMCOaO",
        "colab_type": "code",
        "outputId": "4a98634e-0006-4e87-9d45-3ab0515f800f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras import layers, models\n",
        "from __future__ import print_function\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Bidirectional, Dropout, Embedding\n",
        "import numpy as np\n",
        "from keras import datasets\n",
        "from keras import backend as K\n",
        "from keras.utils import plot_model\n",
        "import matplotlib\n",
        "from matplotlib import ticker\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "batch_size = 32  # Batch size for training.\n",
        "epochs = 100  # Number of epochs to train for.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "num_samples = 10000  # Number of samples to train on.\n",
        "# Path to the data txt file on disk.\n",
        "data_path = '/content/dataset.txt'\n",
        "\n",
        "# Vectorize the data.\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text = line.split('\\t')\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    target_text = '\\t' + target_text + '\\n'\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "# 전처리\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "print('Number of samples:', len(input_texts))\n",
        "print('Number of unique input tokens:', num_encoder_tokens)\n",
        "print('Number of unique output tokens:', num_decoder_tokens)\n",
        "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
        "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
        "\n",
        "\n",
        "# 문자 -> 숫자 변환용 사전\n",
        "input_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "# 학습에 사용할 데이터를 담을 3차원 배열\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "\n",
        "# 문장을 문자 단위로 원 핫 인코딩하면서 학습용 데이터를 만듬\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
        "    for t, char in enumerate(target_text):\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
        "        if t > 0:\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
        "\n",
        "# 숫자 -> 문자 변환용 사전\n",
        "reverse_input_char_index = dict(\n",
        "    (i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict(\n",
        "    (i, char) for char, i in target_token_index.items())\n",
        "\n",
        "def RepeatVectorLayer(rep, axis):\n",
        "  return layers.Lambda(lambda x: K.repeat_elements(K.expand_dims(x, axis), rep, axis),\n",
        "                      lambda x: tuple((x[0],) + x[1:axis] + (rep,) + x[axis:]))\n",
        "\n",
        "\n",
        "# 인코더 생성\n",
        "encoder_inputs = layers.Input(shape=(max_encoder_seq_length, num_encoder_tokens))\n",
        "# dropout 전\n",
        "# encoder = layers.GRU(latent_dim, return_sequences=True, return_state=True)\n",
        "\n",
        "encoder = layers.GRU(latent_dim,dropout=0.25,recurrent_dropout=0.25, return_sequences=True, return_state=True)\n",
        "encoder_outputs, state_h = encoder(encoder_inputs)\n",
        "\n",
        "\n",
        "# 디코더 생성\n",
        "decoder_inputs = layers.Input(shape=(max_decoder_seq_length, num_decoder_tokens))\n",
        "# dropout 전\n",
        "# decoder = layers.GRU(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder = layers.GRU(latent_dim,dropout=0.25,recurrent_dropout=0.25, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _ = decoder(decoder_inputs, initial_state=state_h)\n",
        "\n",
        "# embedding test\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# attention 생성\n",
        "'''\n",
        "어텐션의 기본 아이디어는 디코더에서 출력 단어를 예측하는 매 시점(time step)마다, \n",
        "인코더에서의 전체 입력 문장을 다시 한 번 참고한다는 점입니다. \n",
        "단, 전체 입력 문장을 전부 다 동일한 비율로 참고하는 것이 아니라, \n",
        "해당 시점에서 예측해야할 단어와 연관이 있는 입력 단어 부분을 좀 더 \n",
        "집중(attention)해서 보게 됩니다.\n",
        "'''\n",
        "\n",
        "repeat_d_layer = RepeatVectorLayer(max_encoder_seq_length, 2)\n",
        "# 입력을 n 번 반복합니다.\n",
        "repeat_d = repeat_d_layer(decoder_outputs)\n",
        "\n",
        "repeat_e_layer = RepeatVectorLayer(max_decoder_seq_length, 1)\n",
        "# 입력을 n 번 반복합니다.\n",
        "repeat_e = repeat_e_layer(encoder_outputs)\n",
        "\n",
        "concat_for_score_layer = layers.Concatenate(axis=-1)\n",
        "#layers.Concatenate는 입력 목록을 연결하는 계층입니다.\n",
        "# 연결 축을 제외하고 모두 동일한 모양의 텐서 목록을 입력으로 사용하고 \n",
        "# 모든 입력의 연결 인 단일 텐서를 반환합니다.\n",
        "concat_for_score = concat_for_score_layer([repeat_d, repeat_e])\n",
        "\n",
        "dense1_t_score_layer = layers.Dense(latent_dim // 2, activation='tanh')\n",
        "# Dense 클래스 객체를 TimeDistributed wrapper를 사용하여 3차원 텐서 입력을 받을 수 있게 확장\n",
        "dense1_score_layer = layers.TimeDistributed(dense1_t_score_layer)\n",
        "dense1_score = dense1_score_layer(concat_for_score)\n",
        "\n",
        "\n",
        "dense2_t_score_layer = layers.Dense(1)\n",
        "# Dense 클래스 객체를 TimeDistributed wrapper를 사용하여 3차원 텐서 입력을 받을 수 있게 확장\n",
        "dense2_score_layer = layers.TimeDistributed(dense2_t_score_layer)\n",
        "dense2_score = dense2_score_layer(dense1_score)\n",
        "dense2_score = layers.Reshape((max_decoder_seq_length, max_encoder_seq_length))(dense2_score)\n",
        "\n",
        "# soft max 설정\n",
        "softmax_score_layer = layers.Softmax(axis=-1)\n",
        "softmax_score = softmax_score_layer(dense2_score)\n",
        "\n",
        "# 입력을 n 번 반복합니다 RepeatVectorLayer\n",
        "repeat_score_layer = RepeatVectorLayer(latent_dim, 2)\n",
        "repeat_score = repeat_score_layer(softmax_score)\n",
        "\n",
        "# layers.Permute 주어진 패턴에 따라 입력 치수를 변경합니다.\n",
        "permute_e = layers.Permute((2, 1))(encoder_outputs)\n",
        "repeat_e_layer = RepeatVectorLayer(max_decoder_seq_length, 1)\n",
        "repeat_e = repeat_e_layer(permute_e)\n",
        "\n",
        "attended_mat_layer = layers.Multiply() # 행렬곱\n",
        "attended_mat = attended_mat_layer([repeat_score, repeat_e])\n",
        "\n",
        "context_layer = layers.Lambda(lambda x: K.sum(x, axis=-1),\n",
        "                             lambda x: tuple(x[:-1]))\n",
        "context = context_layer(attended_mat)\n",
        "\n",
        "concat_context_layer = layers.Concatenate(axis=-1)\n",
        "'''입력 목록을 연결하는 계층입니다.\n",
        "연결 축을 제외하고 모두 동일한 모양의 텐서 목록을 입력으로 \n",
        "사용하고 모든 입력의 연결 인 단일 텐서를 반환합니다.'''\n",
        "concat_context = concat_context_layer([context, decoder_outputs])\n",
        "\n",
        "attention_dense_output_layer = layers.Dense(latent_dim, activation='tanh')\n",
        "attention_output_layer = layers.TimeDistributed(attention_dense_output_layer)\n",
        "attention_output = attention_output_layer(concat_context)\n",
        "\n",
        "decoder_dense = layers.Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(attention_output)\n",
        "\n",
        "\n",
        "# 모델 생성\n",
        "model = models.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "che = 'total.h5'\n",
        "point = ModelCheckpoint(filepath=che , monitor='val_loss', verbose=1, save_best_only=True)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=100)\n",
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "history = model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_split=0.2,\n",
        "                    verbose=1,callbacks=[point,early_stopping])\n",
        "# Save model\n",
        "model_json = model.to_json()\n",
        "with open(\"model_re.json\", \"w\") as json_file : \n",
        "    json_file.write(model_json)\n",
        "model.save_weights(\"wh_re.h5\")\n",
        "#model.save('atten_GRU_weight2.h5')\n",
        "# \n",
        "\n",
        "\n",
        "# Next: inference mode (sampling).\n",
        "# Here's the drill:\n",
        "# 1) 입력을 인코딩하고 초기 디코더 상태 검색\n",
        "# 2) 이 초기 상태로 디코더 한 단계 실행\n",
        "# \"시퀀스 시작\" 토큰을 대상으로 한다.\n",
        "# 출력이 다음 대상 토큰임\n",
        "# 3) 현재 대상 토큰 및 현재 상태로 반복\n",
        "\n",
        "# 샘플링 모델 정의\n",
        "encoder_model = models.Model(encoder_inputs, [encoder_outputs, state_h])\n",
        "encoder_outputs_input = layers.Input(shape=(max_encoder_seq_length, latent_dim))\n",
        "\n",
        "decoder_inputs = layers.Input(shape=(1, num_decoder_tokens))\n",
        "decoder_state_input_h = layers.Input(shape=(latent_dim,))\n",
        "decoder_outputs, decoder_h = decoder(decoder_inputs, initial_state=decoder_state_input_h)\n",
        "\n",
        "repeat_d_layer = RepeatVectorLayer(max_encoder_seq_length, 2)\n",
        "repeat_d = repeat_d_layer(decoder_outputs)\n",
        "\n",
        "repeat_e_layer = RepeatVectorLayer(1, axis=1)\n",
        "repeat_e = repeat_e_layer(encoder_outputs_input)\n",
        "\n",
        "concat_for_score_layer = layers.Concatenate(axis=-1)\n",
        "concat_for_score = concat_for_score_layer([repeat_d, repeat_e])\n",
        "\n",
        "dense1_score_layer = layers.TimeDistributed(dense1_t_score_layer)\n",
        "dense1_score = dense1_score_layer(concat_for_score)\n",
        "\n",
        "dense2_score_layer = layers.TimeDistributed(dense2_t_score_layer)\n",
        "dense2_score = dense2_score_layer(dense1_score)\n",
        "dense2_score = layers.Reshape((1, max_encoder_seq_length))(dense2_score)\n",
        "\n",
        "softmax_score_layer = layers.Softmax(axis=-1)\n",
        "softmax_score = softmax_score_layer(dense2_score)\n",
        "\n",
        "repeat_score_layer = RepeatVectorLayer(latent_dim, 2)\n",
        "repeat_score = repeat_score_layer(softmax_score)\n",
        "\n",
        "permute_e = layers.Permute((2, 1))(encoder_outputs_input)\n",
        "repeat_e_layer = RepeatVectorLayer(1, axis=1)\n",
        "repeat_e = repeat_e_layer(permute_e)\n",
        "\n",
        "attended_mat_layer = layers.Multiply()\n",
        "attended_mat = attended_mat_layer([repeat_score, repeat_e])\n",
        "\n",
        "context_layer = layers.Lambda(lambda x: K.sum(x, axis=-1),\n",
        "                             lambda x: tuple(x[:-1]))\n",
        "context = context_layer(attended_mat)\n",
        "\n",
        "concat_context_layer = layers.Concatenate(axis=-1)\n",
        "concat_context = concat_context_layer([context, decoder_outputs])\n",
        "\n",
        "attention_output_layer = layers.TimeDistributed(attention_dense_output_layer)\n",
        "attention_output = attention_output_layer(concat_context)\n",
        "\n",
        "decoder_att_outputs = decoder_dense(attention_output)\n",
        "\n",
        "decoder_model = models.Model([decoder_inputs, decoder_state_input_h, encoder_outputs_input],\n",
        "                            [decoder_outputs, decoder_h, decoder_att_outputs])\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "  # 입력 문장을 인코딩\n",
        "  enc_outputs, states_value = encoder_model.predict(input_seq)\n",
        " \n",
        "  # 디코더의 입력으로 쓸 단일 문자\n",
        "  target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "  # 첫 입력은 시작 문자인 '\\t'로 설정\n",
        "  target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        " \n",
        "  # 문장 생성\n",
        "  stop_condition = False\n",
        "  decoded_sentence = ''\n",
        "  while not stop_condition:\n",
        "    # 이전의 출력, 상태를 디코더에 넣어서 새로운 출력, 상태를 얻음\n",
        "    # 이전 문자와 상태로 다음 문자와 상태를 얻는다고 보면 됨.\n",
        "    dec_outputs, h, output_tokens = decoder_model.predict(\n",
        "        [target_seq, states_value, enc_outputs])\n",
        " \n",
        "    # 사전을 사용해서 원 핫 인코딩 출력을 실제 문자로 변환\n",
        "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "    sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "    decoded_sentence += sampled_char\n",
        " \n",
        "    # 종료 문자가 나왔거나 문장 길이가 한계를 넘으면 종료\n",
        "    if (sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length):\n",
        "      stop_condition = True\n",
        " \n",
        "    # 디코더의 다음 입력으로 쓸 데이터 갱신\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    target_seq[0, 0, sampled_token_index] = 1.\n",
        "    \n",
        "    states_value = h\n",
        " \n",
        "  return decoded_sentence\n",
        "\n",
        "for seq_index in range(30):\n",
        "  input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
        "  decoded_sentence = decode_sequence(input_seq)\n",
        "  print('\"{}\" -> \"{}\"'.format(input_texts[seq_index], decoded_sentence.strip()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Number of samples: 1175\n",
            "Number of unique input tokens: 717\n",
            "Number of unique output tokens: 781\n",
            "Max sequence length for inputs: 181\n",
            "Max sequence length for outputs: 167\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 181, 717)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, 167, 781)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "gru_1 (GRU)                     [(None, 181, 256), ( 748032      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "gru_2 (GRU)                     [(None, 167, 256), ( 797184      input_2[0][0]                    \n",
            "                                                                 gru_1[0][1]                      \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 167, 181, 256 0           gru_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 167, 181, 256 0           gru_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 167, 181, 512 0           lambda_1[0][0]                   \n",
            "                                                                 lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_1 (TimeDistrib (None, 167, 181, 128 65664       concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_2 (TimeDistrib (None, 167, 181, 1)  129         time_distributed_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 167, 181)     0           time_distributed_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "softmax_1 (Softmax)             (None, 167, 181)     0           reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 256, 181)     0           gru_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 167, 256, 181 0           softmax_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_4 (Lambda)               (None, 167, 256, 181 0           permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_1 (Multiply)           (None, 167, 256, 181 0           lambda_3[0][0]                   \n",
            "                                                                 lambda_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_5 (Lambda)               (None, 167, 256)     0           multiply_1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 167, 512)     0           lambda_5[0][0]                   \n",
            "                                                                 gru_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_3 (TimeDistrib (None, 167, 256)     131328      concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 167, 781)     200717      time_distributed_3[0][0]         \n",
            "==================================================================================================\n",
            "Total params: 1,943,054\n",
            "Trainable params: 1,943,054\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 940 samples, validate on 235 samples\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "940/940 [==============================] - 38s 40ms/step - loss: 0.3454 - acc: 0.0129 - val_loss: 0.5686 - val_acc: 0.0273\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.56863, saving model to total.h5\n",
            "Epoch 2/100\n",
            "940/940 [==============================] - 27s 28ms/step - loss: 0.3076 - acc: 0.0145 - val_loss: 0.5612 - val_acc: 0.0253\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.56863 to 0.56117, saving model to total.h5\n",
            "Epoch 3/100\n",
            "940/940 [==============================] - 27s 29ms/step - loss: 0.2983 - acc: 0.0159 - val_loss: 0.5327 - val_acc: 0.0312\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.56117 to 0.53267, saving model to total.h5\n",
            "Epoch 4/100\n",
            "940/940 [==============================] - 27s 28ms/step - loss: 0.2893 - acc: 0.0168 - val_loss: 0.5199 - val_acc: 0.0317\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.53267 to 0.51994, saving model to total.h5\n",
            "Epoch 5/100\n",
            "940/940 [==============================] - 27s 28ms/step - loss: 0.2828 - acc: 0.0170 - val_loss: 0.5104 - val_acc: 0.0319\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.51994 to 0.51035, saving model to total.h5\n",
            "Epoch 6/100\n",
            "940/940 [==============================] - 27s 28ms/step - loss: 0.2783 - acc: 0.0175 - val_loss: 0.5053 - val_acc: 0.0322\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.51035 to 0.50529, saving model to total.h5\n",
            "Epoch 7/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.2741 - acc: 0.0179 - val_loss: 0.5141 - val_acc: 0.0311\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.50529\n",
            "Epoch 8/100\n",
            "940/940 [==============================] - 27s 29ms/step - loss: 0.2710 - acc: 0.0181 - val_loss: 0.5018 - val_acc: 0.0343\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.50529 to 0.50184, saving model to total.h5\n",
            "Epoch 9/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.2666 - acc: 0.0186 - val_loss: 0.4872 - val_acc: 0.0349\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.50184 to 0.48724, saving model to total.h5\n",
            "Epoch 10/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.2633 - acc: 0.0189 - val_loss: 0.4896 - val_acc: 0.0355\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.48724\n",
            "Epoch 11/100\n",
            "940/940 [==============================] - 27s 29ms/step - loss: 0.2605 - acc: 0.0194 - val_loss: 0.4885 - val_acc: 0.0347\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.48724\n",
            "Epoch 12/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.2563 - acc: 0.0198 - val_loss: 0.4818 - val_acc: 0.0352\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.48724 to 0.48184, saving model to total.h5\n",
            "Epoch 13/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.2544 - acc: 0.0199 - val_loss: 0.4897 - val_acc: 0.0325\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.48184\n",
            "Epoch 14/100\n",
            "940/940 [==============================] - 27s 29ms/step - loss: 0.2507 - acc: 0.0203 - val_loss: 0.4779 - val_acc: 0.0352\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.48184 to 0.47792, saving model to total.h5\n",
            "Epoch 15/100\n",
            "940/940 [==============================] - 27s 28ms/step - loss: 0.2457 - acc: 0.0212 - val_loss: 0.4729 - val_acc: 0.0359\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.47792 to 0.47293, saving model to total.h5\n",
            "Epoch 16/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.2415 - acc: 0.0215 - val_loss: 0.4683 - val_acc: 0.0373\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.47293 to 0.46831, saving model to total.h5\n",
            "Epoch 17/100\n",
            "940/940 [==============================] - 27s 29ms/step - loss: 0.2393 - acc: 0.0221 - val_loss: 0.4716 - val_acc: 0.0361\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.46831\n",
            "Epoch 18/100\n",
            "940/940 [==============================] - 27s 28ms/step - loss: 0.2347 - acc: 0.0223 - val_loss: 0.4671 - val_acc: 0.0381\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.46831 to 0.46714, saving model to total.h5\n",
            "Epoch 19/100\n",
            "940/940 [==============================] - 27s 28ms/step - loss: 0.2320 - acc: 0.0227 - val_loss: 0.4628 - val_acc: 0.0382\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.46714 to 0.46281, saving model to total.h5\n",
            "Epoch 20/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.2275 - acc: 0.0232 - val_loss: 0.4642 - val_acc: 0.0395\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.46281\n",
            "Epoch 21/100\n",
            "940/940 [==============================] - 27s 29ms/step - loss: 0.2238 - acc: 0.0235 - val_loss: 0.4593 - val_acc: 0.0390\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.46281 to 0.45933, saving model to total.h5\n",
            "Epoch 22/100\n",
            "940/940 [==============================] - 27s 29ms/step - loss: 0.2199 - acc: 0.0242 - val_loss: 0.4612 - val_acc: 0.0367\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.45933\n",
            "Epoch 23/100\n",
            "940/940 [==============================] - 27s 29ms/step - loss: 0.2170 - acc: 0.0246 - val_loss: 0.4585 - val_acc: 0.0392\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.45933 to 0.45852, saving model to total.h5\n",
            "Epoch 24/100\n",
            "940/940 [==============================] - 27s 29ms/step - loss: 0.2125 - acc: 0.0252 - val_loss: 0.4481 - val_acc: 0.0410\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.45852 to 0.44805, saving model to total.h5\n",
            "Epoch 25/100\n",
            "940/940 [==============================] - 27s 29ms/step - loss: 0.2099 - acc: 0.0253 - val_loss: 0.4538 - val_acc: 0.0391\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.44805\n",
            "Epoch 26/100\n",
            "940/940 [==============================] - 27s 29ms/step - loss: 0.2056 - acc: 0.0261 - val_loss: 0.4516 - val_acc: 0.0412\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.44805\n",
            "Epoch 27/100\n",
            "940/940 [==============================] - 27s 29ms/step - loss: 0.2028 - acc: 0.0262 - val_loss: 0.4483 - val_acc: 0.0417\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.44805\n",
            "Epoch 28/100\n",
            "940/940 [==============================] - 27s 28ms/step - loss: 0.1996 - acc: 0.0264 - val_loss: 0.4499 - val_acc: 0.0411\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.44805\n",
            "Epoch 29/100\n",
            "940/940 [==============================] - 27s 29ms/step - loss: 0.1953 - acc: 0.0273 - val_loss: 0.4465 - val_acc: 0.0415\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.44805 to 0.44653, saving model to total.h5\n",
            "Epoch 30/100\n",
            "940/940 [==============================] - 27s 28ms/step - loss: 0.1932 - acc: 0.0274 - val_loss: 0.4500 - val_acc: 0.0402\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.44653\n",
            "Epoch 31/100\n",
            "940/940 [==============================] - 27s 29ms/step - loss: 0.1895 - acc: 0.0278 - val_loss: 0.4499 - val_acc: 0.0418\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.44653\n",
            "Epoch 32/100\n",
            "940/940 [==============================] - 27s 28ms/step - loss: 0.1860 - acc: 0.0285 - val_loss: 0.4524 - val_acc: 0.0403\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.44653\n",
            "Epoch 33/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.1836 - acc: 0.0289 - val_loss: 0.4408 - val_acc: 0.0432\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.44653 to 0.44081, saving model to total.h5\n",
            "Epoch 34/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.1804 - acc: 0.0289 - val_loss: 0.4494 - val_acc: 0.0416\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.44081\n",
            "Epoch 35/100\n",
            "940/940 [==============================] - 27s 29ms/step - loss: 0.1764 - acc: 0.0295 - val_loss: 0.4407 - val_acc: 0.0438\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.44081 to 0.44073, saving model to total.h5\n",
            "Epoch 36/100\n",
            "940/940 [==============================] - 27s 28ms/step - loss: 0.1727 - acc: 0.0302 - val_loss: 0.4463 - val_acc: 0.0420\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.44073\n",
            "Epoch 37/100\n",
            "940/940 [==============================] - 27s 29ms/step - loss: 0.1703 - acc: 0.0306 - val_loss: 0.4397 - val_acc: 0.0442\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.44073 to 0.43971, saving model to total.h5\n",
            "Epoch 38/100\n",
            "940/940 [==============================] - 27s 29ms/step - loss: 0.1680 - acc: 0.0312 - val_loss: 0.4474 - val_acc: 0.0427\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.43971\n",
            "Epoch 39/100\n",
            "940/940 [==============================] - 27s 29ms/step - loss: 0.1648 - acc: 0.0314 - val_loss: 0.4442 - val_acc: 0.0441\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.43971\n",
            "Epoch 40/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.1618 - acc: 0.0319 - val_loss: 0.4405 - val_acc: 0.0438\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.43971\n",
            "Epoch 41/100\n",
            "940/940 [==============================] - 27s 28ms/step - loss: 0.1601 - acc: 0.0320 - val_loss: 0.4515 - val_acc: 0.0418\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.43971\n",
            "Epoch 42/100\n",
            "940/940 [==============================] - 27s 28ms/step - loss: 0.1564 - acc: 0.0325 - val_loss: 0.4539 - val_acc: 0.0433\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.43971\n",
            "Epoch 43/100\n",
            "940/940 [==============================] - 27s 28ms/step - loss: 0.1559 - acc: 0.0326 - val_loss: 0.4550 - val_acc: 0.0432\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.43971\n",
            "Epoch 44/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.1503 - acc: 0.0333 - val_loss: 0.4463 - val_acc: 0.0446\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.43971\n",
            "Epoch 45/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.1473 - acc: 0.0341 - val_loss: 0.4539 - val_acc: 0.0432\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.43971\n",
            "Epoch 46/100\n",
            "940/940 [==============================] - 27s 28ms/step - loss: 0.1455 - acc: 0.0344 - val_loss: 0.4515 - val_acc: 0.0427\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.43971\n",
            "Epoch 47/100\n",
            "940/940 [==============================] - 27s 28ms/step - loss: 0.1416 - acc: 0.0349 - val_loss: 0.4466 - val_acc: 0.0445\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.43971\n",
            "Epoch 48/100\n",
            "940/940 [==============================] - 27s 29ms/step - loss: 0.1414 - acc: 0.0349 - val_loss: 0.4584 - val_acc: 0.0430\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.43971\n",
            "Epoch 49/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.1381 - acc: 0.0355 - val_loss: 0.4540 - val_acc: 0.0441\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.43971\n",
            "Epoch 50/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.1365 - acc: 0.0357 - val_loss: 0.4503 - val_acc: 0.0445\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.43971\n",
            "Epoch 51/100\n",
            "940/940 [==============================] - 27s 28ms/step - loss: 0.1328 - acc: 0.0363 - val_loss: 0.4558 - val_acc: 0.0435\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.43971\n",
            "Epoch 52/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.1300 - acc: 0.0372 - val_loss: 0.4561 - val_acc: 0.0450\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.43971\n",
            "Epoch 53/100\n",
            "940/940 [==============================] - 27s 28ms/step - loss: 0.1266 - acc: 0.0375 - val_loss: 0.4550 - val_acc: 0.0454\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.43971\n",
            "Epoch 54/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.1258 - acc: 0.0374 - val_loss: 0.4577 - val_acc: 0.0445\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.43971\n",
            "Epoch 55/100\n",
            "940/940 [==============================] - 27s 29ms/step - loss: 0.1236 - acc: 0.0379 - val_loss: 0.4603 - val_acc: 0.0453\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.43971\n",
            "Epoch 56/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.1213 - acc: 0.0385 - val_loss: 0.4595 - val_acc: 0.0444\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.43971\n",
            "Epoch 57/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.1172 - acc: 0.0394 - val_loss: 0.4617 - val_acc: 0.0465\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.43971\n",
            "Epoch 58/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.1178 - acc: 0.0391 - val_loss: 0.4546 - val_acc: 0.0458\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.43971\n",
            "Epoch 59/100\n",
            "940/940 [==============================] - 27s 28ms/step - loss: 0.1147 - acc: 0.0399 - val_loss: 0.4558 - val_acc: 0.0456\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.43971\n",
            "Epoch 60/100\n",
            "940/940 [==============================] - 27s 28ms/step - loss: 0.1129 - acc: 0.0401 - val_loss: 0.4650 - val_acc: 0.0446\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.43971\n",
            "Epoch 61/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.1110 - acc: 0.0402 - val_loss: 0.4655 - val_acc: 0.0447\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.43971\n",
            "Epoch 62/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.1082 - acc: 0.0413 - val_loss: 0.4655 - val_acc: 0.0454\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.43971\n",
            "Epoch 63/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.1042 - acc: 0.0419 - val_loss: 0.4643 - val_acc: 0.0458\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.43971\n",
            "Epoch 64/100\n",
            "940/940 [==============================] - 27s 28ms/step - loss: 0.1030 - acc: 0.0423 - val_loss: 0.4656 - val_acc: 0.0453\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.43971\n",
            "Epoch 65/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.1023 - acc: 0.0423 - val_loss: 0.4712 - val_acc: 0.0447\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.43971\n",
            "Epoch 66/100\n",
            "940/940 [==============================] - 27s 29ms/step - loss: 0.0980 - acc: 0.0431 - val_loss: 0.4719 - val_acc: 0.0458\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.43971\n",
            "Epoch 67/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.0983 - acc: 0.0431 - val_loss: 0.4676 - val_acc: 0.0460\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.43971\n",
            "Epoch 68/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.0962 - acc: 0.0439 - val_loss: 0.4724 - val_acc: 0.0451\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.43971\n",
            "Epoch 69/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.0956 - acc: 0.0438 - val_loss: 0.4746 - val_acc: 0.0455\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.43971\n",
            "Epoch 70/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.0935 - acc: 0.0439 - val_loss: 0.4803 - val_acc: 0.0457\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.43971\n",
            "Epoch 71/100\n",
            "940/940 [==============================] - 27s 28ms/step - loss: 0.0907 - acc: 0.0450 - val_loss: 0.4758 - val_acc: 0.0454\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.43971\n",
            "Epoch 72/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.0899 - acc: 0.0446 - val_loss: 0.4729 - val_acc: 0.0465\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.43971\n",
            "Epoch 73/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.0858 - acc: 0.0456 - val_loss: 0.4801 - val_acc: 0.0453\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.43971\n",
            "Epoch 74/100\n",
            "940/940 [==============================] - 27s 29ms/step - loss: 0.0869 - acc: 0.0453 - val_loss: 0.4758 - val_acc: 0.0468\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.43971\n",
            "Epoch 75/100\n",
            "940/940 [==============================] - 27s 28ms/step - loss: 0.0852 - acc: 0.0459 - val_loss: 0.4884 - val_acc: 0.0449\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.43971\n",
            "Epoch 76/100\n",
            "940/940 [==============================] - 27s 29ms/step - loss: 0.0826 - acc: 0.0462 - val_loss: 0.4777 - val_acc: 0.0465\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.43971\n",
            "Epoch 77/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.0811 - acc: 0.0469 - val_loss: 0.4835 - val_acc: 0.0466\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.43971\n",
            "Epoch 78/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.0784 - acc: 0.0471 - val_loss: 0.4871 - val_acc: 0.0451\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.43971\n",
            "Epoch 79/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.0779 - acc: 0.0476 - val_loss: 0.4874 - val_acc: 0.0448\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.43971\n",
            "Epoch 80/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.0757 - acc: 0.0480 - val_loss: 0.4867 - val_acc: 0.0452\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.43971\n",
            "Epoch 81/100\n",
            "940/940 [==============================] - 27s 28ms/step - loss: 0.0761 - acc: 0.0478 - val_loss: 0.4905 - val_acc: 0.0462\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.43971\n",
            "Epoch 82/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.0713 - acc: 0.0494 - val_loss: 0.4926 - val_acc: 0.0449\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.43971\n",
            "Epoch 83/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.0710 - acc: 0.0492 - val_loss: 0.4932 - val_acc: 0.0454\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.43971\n",
            "Epoch 84/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.0721 - acc: 0.0488 - val_loss: 0.4960 - val_acc: 0.0440\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.43971\n",
            "Epoch 85/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.0688 - acc: 0.0496 - val_loss: 0.4928 - val_acc: 0.0467\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.43971\n",
            "Epoch 86/100\n",
            "940/940 [==============================] - 27s 29ms/step - loss: 0.0686 - acc: 0.0496 - val_loss: 0.4938 - val_acc: 0.0461\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.43971\n",
            "Epoch 87/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.0660 - acc: 0.0500 - val_loss: 0.4883 - val_acc: 0.0469\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.43971\n",
            "Epoch 88/100\n",
            "940/940 [==============================] - 27s 28ms/step - loss: 0.0654 - acc: 0.0508 - val_loss: 0.4994 - val_acc: 0.0439\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.43971\n",
            "Epoch 89/100\n",
            "940/940 [==============================] - 27s 28ms/step - loss: 0.0636 - acc: 0.0506 - val_loss: 0.4955 - val_acc: 0.0455\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.43971\n",
            "Epoch 90/100\n",
            "940/940 [==============================] - 27s 28ms/step - loss: 0.0630 - acc: 0.0510 - val_loss: 0.4984 - val_acc: 0.0459\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.43971\n",
            "Epoch 91/100\n",
            "940/940 [==============================] - 27s 29ms/step - loss: 0.0634 - acc: 0.0506 - val_loss: 0.5008 - val_acc: 0.0457\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.43971\n",
            "Epoch 92/100\n",
            "940/940 [==============================] - 27s 29ms/step - loss: 0.0606 - acc: 0.0517 - val_loss: 0.5052 - val_acc: 0.0445\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.43971\n",
            "Epoch 93/100\n",
            "940/940 [==============================] - 27s 29ms/step - loss: 0.0598 - acc: 0.0517 - val_loss: 0.5104 - val_acc: 0.0440\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.43971\n",
            "Epoch 94/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.0593 - acc: 0.0519 - val_loss: 0.5062 - val_acc: 0.0452\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.43971\n",
            "Epoch 95/100\n",
            "940/940 [==============================] - 27s 28ms/step - loss: 0.0589 - acc: 0.0518 - val_loss: 0.5055 - val_acc: 0.0471\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.43971\n",
            "Epoch 96/100\n",
            "940/940 [==============================] - 27s 29ms/step - loss: 0.0563 - acc: 0.0523 - val_loss: 0.5082 - val_acc: 0.0463\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.43971\n",
            "Epoch 97/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.0563 - acc: 0.0526 - val_loss: 0.5092 - val_acc: 0.0452\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.43971\n",
            "Epoch 98/100\n",
            "940/940 [==============================] - 26s 28ms/step - loss: 0.0558 - acc: 0.0524 - val_loss: 0.4996 - val_acc: 0.0461\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.43971\n",
            "Epoch 99/100\n",
            "940/940 [==============================] - 27s 28ms/step - loss: 0.0530 - acc: 0.0534 - val_loss: 0.5074 - val_acc: 0.0464\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.43971\n",
            "Epoch 100/100\n",
            "940/940 [==============================] - 27s 28ms/step - loss: 0.0546 - acc: 0.0533 - val_loss: 0.5042 - val_acc: 0.0477\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.43971\n",
            "\"넘치도록\" -> \"버래기\"\n",
            "\"강아지\" -> \"강생이\"\n",
            "\"부글부글하다\" -> \"부각허다\"\n",
            "\"가랭이\" -> \"강알\"\n",
            "\"부풀어 오르다\" -> \"부끄다\"\n",
            "\"거품\" -> \"개끔\"\n",
            "\"정황도 모르고\" -> \"분시몰랑\"\n",
            "\"밥을 추하게 먹는 모습\" -> \"개작작작\"\n",
            "\"뺨\" -> \"삐암데기\"\n",
            "\"잡초\" -> \"검질\"\n",
            "\"말하지말라\" -> \"속슴허다\"\n",
            "\"희미하다\" -> \"게미융허다\"\n",
            "\"숨막히는\" -> \"고끼다\"\n",
            "\"입이 큰\" -> \"게작헌\"\n",
            "\"유산식품\" -> \"쉰달이\"\n",
            "\"고자질한다\" -> \"고라불켜\"\n",
            "\"마음 씀씀이\" -> \"심토맥이\"\n",
            "\"숨막히다\" -> \"고끼다\"\n",
            "\"이렇게\" -> \"영 골아도 속솜\"\n",
            "\"잘게부수다\" -> \"골다\"\n",
            "\"캄캄하다\" -> \"왁왁허다\"\n",
            "\"숨기다\" -> \"곱지다\"\n",
            "\"똑똑하다\" -> \"요망지다\"\n",
            "\"햇살이 눈부시게 비추는모습\" -> \"과랑과랑\"\n",
            "\"텃밭\" -> \"우영밭\"\n",
            "\"친족\" -> \"괸당\"\n",
            "\"산간마을\" -> \"웃뜨리\"\n",
            "\"밑바닥\" -> \"굽\"\n",
            "\"걷는 모습\" -> \"재짝재짝\"\n",
            "\"그을리다\" -> \"기시리다\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_bfHvo3Wacs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UOfWaGnVkzL",
        "colab_type": "code",
        "outputId": "38908922-c0e5-458d-95f1-23804de421eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        }
      },
      "source": [
        "for seq_index in range(30): # 입력 문장의 인덱스\n",
        "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print(35 * \"-\")\n",
        "    print('입력 문장:', lines.src[seq_index])\n",
        "    print('정답 문장:', lines.tar[seq_index][1:len(lines.tar[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
        "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-dd7ae2adb195>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# 입력 문장의 인덱스\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0minput_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mseq_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdecoded_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m35\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'입력 문장:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'encoder_input' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ1JXMOjhmRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svBej46F9y7K",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQXq3BIJzmzJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_inputs = layers.Input(shape=(max_encoder_seq_length, num_encoder_tokens))\n",
        "x = Embedding(num_encoder_tokens, latent_dim)(encoder_inputs)\n",
        "x,encoder = layers.GRU(latent_dim,dropout=0.25,recurrent_dropout=0.2, return_sequences=True, return_state=True)(x)\n",
        "encoder_outputs, state_h = encoder(encoder_inputs)\n",
        "\n",
        "\n",
        "# 디코더 생성\n",
        "decoder_inputs = layers.Input(shape=(max_decoder_seq_length, num_decoder_tokens))\n",
        "x = Embedding(num_decoder_tokens, latent_dim)(decoder_inputs)\n",
        "x,decoder = layers.GRU(latent_dim,dropout=0.25,recurrent_dropout=0.2, return_sequences=True, return_state=True)(x)\n",
        "decoder_outputs, _ = decoder(decoder_inputs, initial_state=state_h)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbypv0zO-m0i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 임베딩 사용방법\n",
        "# 참고용\n",
        "# 입력 시퀀스 정의와 처리\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "x = Embedding(num_encoder_tokens, latent_dim)(encoder_inputs)\n",
        "x, state_h, state_c = LSTM(latent_dim,\n",
        "                           return_state=True)(x)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# `encoder_states`를 초기 상태로 사용해 decoder를 설정\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "x = Embedding(num_decoder_tokens, latent_dim)(decoder_inputs)\n",
        "x = LSTM(latent_dim, return_sequences=True)(x, initial_state=encoder_states)\n",
        "decoder_outputs = Dense(num_decoder_tokens, activation='softmax')(x)\n",
        "\n",
        "# `encoder_input_data`와 `decoder_input_data`를 `decoder_target_data`로 반환하도록 모델을 정의\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# 컴파일 & 학습 실행\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "# `decoder_target_data`는  `decoder_input_data` 같은 정수 시퀀스보단 one-hot 인코딩 형식이 되어야 함.\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwMP_X3T-ZBe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpBMqB1y-ZFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebx0ZMlTvJMJ",
        "colab_type": "text"
      },
      "source": [
        "# 검증"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD_XvYHxfVn7",
        "colab_type": "code",
        "outputId": "82c1a862-8786-43e0-8592-31e6d266bba8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        }
      },
      "source": [
        "  # 손실 그래프\n",
        "  plt.plot(history.history['loss'], 'y', label='train loss')\n",
        "  plt.plot(history.history['val_loss'], 'r', label='val loss')\n",
        "  plt.legend(loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "  # 정확도 그래프\n",
        "  plt.plot(history.history['acc'], 'y', label='train acc')\n",
        "  plt.plot(history.history['val_acc'], 'r', label='val acc')\n",
        "  plt.legend(loc='upper left')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD6CAYAAACoCZCsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZ5Sx8GZVq6T",
        "colab_type": "text"
      },
      "source": [
        "모델불러오고 시작"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odtxgzYXVqey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sp7puwJOTU8e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUL-DOIIU6zS",
        "colab_type": "code",
        "outputId": "9957facb-cc2b-4af7-869e-25d9f58f6d5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "from keras import layers, models\n",
        "from __future__ import print_function\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Bidirectional\n",
        "import numpy as np\n",
        "from keras import datasets\n",
        "from keras import backend as K\n",
        "from keras.utils import plot_model\n",
        "import matplotlib\n",
        "from matplotlib import ticker\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "batch_size = 32  # Batch size for training.\n",
        "epochs = 100  # Number of epochs to train for.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "num_samples = 10000  # Number of samples to train on.\n",
        "# Path to the data txt file on disk.\n",
        "data_path = '/content/test.txt'\n",
        "\n",
        "# Vectorize the data.\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text = line.split('\\t')\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    target_text = '\\t' + target_text + '\\n'\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "# 전처리\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "print('Number of samples:', len(input_texts))\n",
        "print('Number of unique input tokens:', num_encoder_tokens)\n",
        "print('Number of unique output tokens:', num_decoder_tokens)\n",
        "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
        "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
        "\n",
        "\n",
        "# 문자 -> 숫자 변환용 사전\n",
        "input_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "# 학습에 사용할 데이터를 담을 3차원 배열\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "\n",
        "# 문장을 문자 단위로 원 핫 인코딩하면서 학습용 데이터를 만듬\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
        "    for t, char in enumerate(target_text):\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
        "        if t > 0:\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
        "\n",
        "# 숫자 -> 문자 변환용 사전\n",
        "reverse_input_char_index = dict(\n",
        "    (i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict(\n",
        "    (i, char) for char, i in target_token_index.items())\n",
        "\n",
        "def RepeatVectorLayer(rep, axis):\n",
        "  return layers.Lambda(lambda x: K.repeat_elements(K.expand_dims(x, axis), rep, axis),\n",
        "                      lambda x: tuple((x[0],) + x[1:axis] + (rep,) + x[axis:]))\n",
        "\n",
        "# 인코더 생성\n",
        "encoder_inputs = layers.Input(shape=(max_encoder_seq_length, num_encoder_tokens))\n",
        "encoder = layers.GRU(latent_dim, return_sequences=True, return_state=True)\n",
        "encoder_outputs, state_h = encoder(encoder_inputs)\n",
        "\n",
        "# 디코더 생성\n",
        "decoder_inputs = layers.Input(shape=(max_decoder_seq_length, num_decoder_tokens))\n",
        "decoder = layers.GRU(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _ = decoder(decoder_inputs, initial_state=state_h)\n",
        "\n",
        "# attention 생성\n",
        "\n",
        "repeat_d_layer = RepeatVectorLayer(max_encoder_seq_length, 2)\n",
        "repeat_d = repeat_d_layer(decoder_outputs)\n",
        "\n",
        "repeat_e_layer = RepeatVectorLayer(max_decoder_seq_length, 1)\n",
        "repeat_e = repeat_e_layer(encoder_outputs)\n",
        "\n",
        "concat_for_score_layer = layers.Concatenate(axis=-1)\n",
        "concat_for_score = concat_for_score_layer([repeat_d, repeat_e])\n",
        "\n",
        "dense1_t_score_layer = layers.Dense(latent_dim // 2, activation='tanh')\n",
        "dense1_score_layer = layers.TimeDistributed(dense1_t_score_layer)\n",
        "dense1_score = dense1_score_layer(concat_for_score)\n",
        "dense2_t_score_layer = layers.Dense(1)\n",
        "dense2_score_layer = layers.TimeDistributed(dense2_t_score_layer)\n",
        "dense2_score = dense2_score_layer(dense1_score)\n",
        "dense2_score = layers.Reshape((max_decoder_seq_length, max_encoder_seq_length))(dense2_score)\n",
        "\n",
        "softmax_score_layer = layers.Softmax(axis=-1)\n",
        "softmax_score = softmax_score_layer(dense2_score)\n",
        "\n",
        "repeat_score_layer = RepeatVectorLayer(latent_dim, 2)\n",
        "repeat_score = repeat_score_layer(softmax_score)\n",
        "\n",
        "permute_e = layers.Permute((2, 1))(encoder_outputs)\n",
        "repeat_e_layer = RepeatVectorLayer(max_decoder_seq_length, 1)\n",
        "repeat_e = repeat_e_layer(permute_e)\n",
        "\n",
        "attended_mat_layer = layers.Multiply()\n",
        "attended_mat = attended_mat_layer([repeat_score, repeat_e])\n",
        "\n",
        "context_layer = layers.Lambda(lambda x: K.sum(x, axis=-1),\n",
        "                             lambda x: tuple(x[:-1]))\n",
        "context = context_layer(attended_mat)\n",
        "\n",
        "concat_context_layer = layers.Concatenate(axis=-1)\n",
        "concat_context = concat_context_layer([context, decoder_outputs])\n",
        "\n",
        "attention_dense_output_layer = layers.Dense(latent_dim, activation='tanh')\n",
        "attention_output_layer = layers.TimeDistributed(attention_dense_output_layer)\n",
        "attention_output = attention_output_layer(concat_context)\n",
        "\n",
        "decoder_dense = layers.Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(attention_output)\n",
        "################### model\n",
        "from keras.models import load_model\n",
        "history = load_model('/content/atten_GRU_weight_g.h5')\n",
        "###################\n",
        "encoder_model = models.Model(encoder_inputs, [encoder_outputs, state_h])\n",
        "encoder_outputs_input = layers.Input(shape=(max_encoder_seq_length, latent_dim))\n",
        "\n",
        "decoder_inputs = layers.Input(shape=(1, num_decoder_tokens))\n",
        "decoder_state_input_h = layers.Input(shape=(latent_dim,))\n",
        "decoder_outputs, decoder_h = decoder(decoder_inputs, initial_state=decoder_state_input_h)\n",
        "\n",
        "repeat_d_layer = RepeatVectorLayer(max_encoder_seq_length, 2)\n",
        "repeat_d = repeat_d_layer(decoder_outputs)\n",
        "\n",
        "repeat_e_layer = RepeatVectorLayer(1, axis=1)\n",
        "repeat_e = repeat_e_layer(encoder_outputs_input)\n",
        "\n",
        "concat_for_score_layer = layers.Concatenate(axis=-1)\n",
        "concat_for_score = concat_for_score_layer([repeat_d, repeat_e])\n",
        "\n",
        "dense1_score_layer = layers.TimeDistributed(dense1_t_score_layer)\n",
        "dense1_score = dense1_score_layer(concat_for_score)\n",
        "\n",
        "dense2_score_layer = layers.TimeDistributed(dense2_t_score_layer)\n",
        "dense2_score = dense2_score_layer(dense1_score)\n",
        "dense2_score = layers.Reshape((1, max_encoder_seq_length))(dense2_score)\n",
        "\n",
        "softmax_score_layer = layers.Softmax(axis=-1)\n",
        "softmax_score = softmax_score_layer(dense2_score)\n",
        "\n",
        "repeat_score_layer = RepeatVectorLayer(latent_dim, 2)\n",
        "repeat_score = repeat_score_layer(softmax_score)\n",
        "\n",
        "permute_e = layers.Permute((2, 1))(encoder_outputs_input)\n",
        "repeat_e_layer = RepeatVectorLayer(1, axis=1)\n",
        "repeat_e = repeat_e_layer(permute_e)\n",
        "\n",
        "attended_mat_layer = layers.Multiply()\n",
        "attended_mat = attended_mat_layer([repeat_score, repeat_e])\n",
        "\n",
        "context_layer = layers.Lambda(lambda x: K.sum(x, axis=-1),\n",
        "                             lambda x: tuple(x[:-1]))\n",
        "context = context_layer(attended_mat)\n",
        "\n",
        "concat_context_layer = layers.Concatenate(axis=-1)\n",
        "concat_context = concat_context_layer([context, decoder_outputs])\n",
        "\n",
        "attention_output_layer = layers.TimeDistributed(attention_dense_output_layer)\n",
        "attention_output = attention_output_layer(concat_context)\n",
        "\n",
        "decoder_att_outputs = decoder_dense(attention_output)\n",
        "\n",
        "decoder_model = models.Model([decoder_inputs, decoder_state_input_h, encoder_outputs_input],\n",
        "                            [decoder_outputs, decoder_h, decoder_att_outputs])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples: 1203\n",
            "Number of unique input tokens: 787\n",
            "Number of unique output tokens: 749\n",
            "Max sequence length for inputs: 165\n",
            "Max sequence length for outputs: 183\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYQDRTk3yz4r",
        "colab_type": "code",
        "outputId": "8db2093c-ec63-4123-eced-0bfa79afac2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "enc_outputs, states_value = encoder_model.predict(encoder_input_data[2:3])\n",
        "print(enc_outputs)\n",
        "print()\n",
        "print(states_value)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[ 0.0845568  -0.04223287  0.01504657 ... -0.01675112  0.01217361\n",
            "   -0.07913809]\n",
            "  [-0.01012469 -0.04554577  0.07890618 ... -0.00401536 -0.00055613\n",
            "   -0.10650092]\n",
            "  [-0.06501827  0.05378567  0.2214259  ... -0.02666245 -0.05511926\n",
            "   -0.14034012]\n",
            "  ...\n",
            "  [ 0.09179988  0.02620946 -0.1362757  ...  0.20895988 -0.19421019\n",
            "   -0.08369055]\n",
            "  [ 0.09182352  0.02621309 -0.13627124 ...  0.20896643 -0.19420826\n",
            "   -0.08371452]\n",
            "  [ 0.09184714  0.02621309 -0.1362681  ...  0.20896728 -0.19420315\n",
            "   -0.08373799]]]\n",
            "\n",
            "[[ 9.18471441e-02  2.62130871e-02 -1.36268094e-01 -6.08358458e-02\n",
            "  -7.28430599e-02  1.69694096e-01 -1.23323165e-02  7.65081495e-02\n",
            "   3.10718752e-02 -1.84719614e-03  1.00338481e-01  3.34279910e-02\n",
            "   3.96817364e-02 -1.35956854e-01 -1.39421657e-01 -3.15561518e-02\n",
            "  -5.76024204e-02 -6.05241433e-02  4.81434584e-01 -1.32362813e-01\n",
            "   3.27997003e-03 -8.01443495e-03 -1.85289495e-02 -1.41692251e-01\n",
            "  -1.06842995e-01  5.45921910e-04  3.87596935e-01  6.03721961e-02\n",
            "   8.07228014e-02  3.64684537e-02  6.18462488e-02 -3.01507413e-01\n",
            "   6.04353547e-02 -7.76714385e-02  8.05897266e-03  1.32232606e-01\n",
            "  -2.29691058e-01 -4.98210862e-02 -2.38389224e-02  1.56470120e-01\n",
            "   6.38743164e-03  3.33776698e-03  2.15708166e-01 -4.40563895e-02\n",
            "   1.10457890e-01  2.38315105e-01 -6.23930581e-02  5.62002882e-02\n",
            "   7.72832111e-02 -5.39433174e-02  3.63828987e-02 -9.58689675e-02\n",
            "   1.84071869e-01 -1.66121677e-01 -3.39908972e-02 -2.15133131e-01\n",
            "   7.14138970e-02 -3.09865996e-02  8.90992209e-02  1.21591933e-01\n",
            "  -8.37342814e-02  1.47961497e-01  6.88679591e-02 -1.00608014e-01\n",
            "  -5.67902171e-04 -6.49102440e-05 -9.97563638e-03  3.03351164e-01\n",
            "   1.66438043e-01 -2.06658304e-01 -5.07169962e-02  8.32277834e-02\n",
            "  -8.44014995e-03  3.11356708e-02  2.02999890e-01  3.80575687e-01\n",
            "   2.49356270e-01 -1.85453385e-01 -1.56539418e-02  1.61208510e-02\n",
            "  -1.16894118e-01 -1.70623348e-03 -1.91666603e-01  2.00133383e-01\n",
            "  -1.40919417e-01 -3.46512422e-02 -2.42267526e-03  1.05558652e-02\n",
            "   7.01953471e-02 -1.26980543e-01  1.37789950e-01  3.16646285e-02\n",
            "   6.61833510e-02  8.30543041e-02  3.50542888e-02 -4.35679965e-02\n",
            "   4.84781377e-02  8.51513594e-02  2.07637280e-01  1.66170165e-01\n",
            "  -2.58929674e-02  5.60409687e-02  3.92889827e-02  1.90010052e-02\n",
            "  -5.54189011e-02  2.54782364e-02 -4.24576737e-02  3.20782326e-02\n",
            "   5.26398756e-02  5.21192327e-03  7.30864587e-04 -1.02668539e-01\n",
            "  -7.57500231e-02  4.47970033e-02 -1.00827888e-01 -4.71055880e-02\n",
            "  -2.28769388e-02  4.21066850e-01 -1.52236193e-01  5.47476485e-02\n",
            "   1.60006918e-02 -6.64427280e-02  7.25652948e-02  3.81450504e-02\n",
            "  -4.35017515e-03  2.17028093e-02  1.02061760e-02 -1.68222889e-01\n",
            "  -1.52572691e-01 -2.58231796e-02  3.30447331e-02 -7.59621263e-02\n",
            "  -1.37626305e-01 -4.94327247e-02 -2.87606232e-02 -1.19385913e-01\n",
            "   1.11554191e-02  3.02720964e-01 -5.11564091e-02  7.41369370e-03\n",
            "  -1.62335008e-01  4.87262011e-02 -5.91877177e-02  4.73358892e-02\n",
            "  -4.67050970e-02 -1.11307308e-01 -1.66128114e-01 -8.15205723e-02\n",
            "  -1.14731282e-01  1.62780270e-01 -1.95625350e-01  9.88608599e-02\n",
            "   3.91380116e-02 -3.30329202e-02  7.03968480e-03 -2.11861625e-01\n",
            "  -1.73475891e-01 -4.60696109e-02  4.97804210e-03 -2.86249518e-01\n",
            "  -2.32184887e-01 -3.17064933e-02 -6.89055622e-02  1.84144378e-01\n",
            "   9.26086679e-02  1.44372853e-02 -1.06695279e-01 -5.99997267e-02\n",
            "   6.59822971e-02 -1.35351326e-02  6.56801909e-02 -8.86300951e-02\n",
            "  -7.00306445e-02  6.67676851e-02 -6.01751767e-02  1.09740719e-01\n",
            "   6.46574562e-03  3.28110754e-01 -1.63401157e-01 -1.12513311e-01\n",
            "   1.28691763e-01  9.86647755e-02  4.48322073e-02 -2.18776595e-02\n",
            "   9.25635174e-03 -1.37545094e-01  1.59236267e-01 -1.90956488e-01\n",
            "  -2.20924735e-01 -3.53857577e-02  2.75073886e-01 -2.45391399e-01\n",
            "  -1.01602763e-01 -4.96193953e-02  9.15436149e-02 -1.08572736e-01\n",
            "   8.92296582e-02  8.36705789e-02 -3.86312485e-01  2.38839090e-02\n",
            "   2.87341196e-02 -6.95853680e-02 -1.90594792e-01  1.01653501e-01\n",
            "   1.12030365e-01 -7.86008313e-02  6.46999106e-02 -1.73732951e-01\n",
            "   1.18991658e-01 -3.19677554e-02  9.33487043e-02  6.57879785e-02\n",
            "  -7.43617415e-02 -5.83736598e-02 -3.25249061e-02 -1.73416764e-01\n",
            "   2.19107389e-01  1.95692778e-01 -7.86744505e-02  1.26054004e-01\n",
            "  -7.62614012e-02 -2.13176742e-01 -2.38743141e-01  1.62854835e-01\n",
            "   1.29453288e-02  4.04490232e-02 -1.42958462e-01 -1.33307904e-01\n",
            "  -3.30790989e-02 -4.91285957e-02 -1.44636026e-02  2.60626078e-01\n",
            "   1.63212493e-01 -2.19912782e-01  2.57921014e-02  8.73038024e-02\n",
            "  -7.52206966e-02 -1.60397604e-01  1.05496019e-01  9.22601819e-02\n",
            "  -1.28820330e-01  1.82020152e-03  5.71692586e-02 -4.98069031e-03\n",
            "  -3.05308439e-02 -1.23219583e-02  2.07211673e-02  3.78815502e-01\n",
            "  -1.34695843e-01  8.47664550e-02  7.40507394e-02 -3.68028395e-02\n",
            "  -8.72248188e-02  2.08967283e-01 -1.94203153e-01 -8.37379918e-02]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDe7X2LKUonT",
        "colab_type": "code",
        "outputId": "7fa8a8b9-014a-4e45-fac7-63b4066a6f5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# def decode_sequence(input_seq):\n",
        "#   # 입력 문장을 인코딩\n",
        "#   enc_outputs, states_value = encoder_model.predict(input_seq)\n",
        " \n",
        "#   # 디코더의 입력으로 쓸 단일 문자\n",
        "#   target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "#   # 첫 입력은 시작 문자인 '\\t'로 설정\n",
        "#   target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        " \n",
        "#   # 문장 생성\n",
        "#   stop_condition = False\n",
        "#   decoded_sentence = ''\n",
        "#   while not stop_condition:\n",
        "#     # 이전의 출력, 상태를 디코더에 넣어서 새로운 출력, 상태를 얻음\n",
        "#     # 이전 문자와 상태로 다음 문자와 상태를 얻는다고 보면 됨.\n",
        "#     dec_outputs, h, output_tokens = decoder_model.predict(\n",
        "#         [target_seq, states_value, enc_outputs])\n",
        " \n",
        "#     # 사전을 사용해서 원 핫 인코딩 출력을 실제 문자로 변환\n",
        "#     sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "#     sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "#     decoded_sentence += sampled_char\n",
        " \n",
        "#     # 종료 문자가 나왔거나 문장 길이가 한계를 넘으면 종료\n",
        "#     if (sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length):\n",
        "#       stop_condition = True\n",
        " \n",
        "#     # 디코더의 다음 입력으로 쓸 데이터 갱신\n",
        "#     target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "#     target_seq[0, 0, sampled_token_index] = 1.\n",
        "    \n",
        "#     states_value = h\n",
        " \n",
        "#   return decoded_sentence\n",
        "\n",
        "for seq_index in range(10,1300,20):\n",
        "  input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
        "  decoded_sentence = decode_sequence(input_seq)\n",
        "  print('\"{}\" -> \"{}\"'.format(input_texts[seq_index], decoded_sentence.strip()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"Wait!\" -> \"기다려!\"\n",
            "\"Listen.\" -> \"들어.\"\n",
            "\"Help me.\" -> \"도와줘.\"\n",
            "\"Cool off!\" -> \"진정해!\"\n",
            "\"Stand up!\" -> \"일어서!\"\n",
            "\"What fun!\" -> \"재밌잖아!\"\n",
            "\"Grab this.\" -> \"저걸 움켜쥐어.\"\n",
            "\"Of course.\" -> \"물론이죠.\"\n",
            "\"They left.\" -> \"그들이 떠났어.\"\n",
            "\"Wonderful!\" -> \"멋져!\"\n",
            "\"Ignore that.\" -> \"저걸 움켜쥐어.\"\n",
            "\"Remember it.\" -> \"기억해.\"\n",
            "\"Tom escaped.\" -> \"톰이 빠져나왔어.\"\n",
            "\"We survived!\" -> \"우리 안전해!\"\n",
            "\"Examine this.\" -> \"이걸 조사해봐.\"\n",
            "\"It'll be hot.\" -> \"더워질거야.\"\n",
            "\"Nobody knows.\" -> \"아무도 몰라.\"\n",
            "\"Tom chuckled.\" -> \"톰이 싱긋 웃었어.\"\n",
            "\"We overslept.\" -> \"우린 늦잠잤어.\"\n",
            "\"I am homesick.\" -> \"나는 영화를 봤어.\"\n",
            "\"Please listen.\" -> \"제발 진정해.\"\n",
            "\"That's not it.\" -> \"저거 내 고양이야.\"\n",
            "\"Everybody knew.\" -> \"모두가 알고 있었어.\"\n",
            "\"Keep listening.\" -> \"계속 웃어.\"\n",
            "\"Ticket, please.\" -> \"티켓 부탁해.\"\n",
            "\"Don't lie to me.\" -> \"저희에게 거짓말 하지 마세요.\"\n",
            "\"I caught a cold.\" -> \"감기에 걸렸어.\"\n",
            "\"My cat is black.\" -> \"내 고양이는 검은색 고양이야.\"\n",
            "\"Tom volunteered.\" -> \"톰이 자웠했어.\"\n",
            "\"Everyone changes.\" -> \"누구나 바뀌어.\"\n",
            "\"I'm dead serious.\" -> \"난 절대 농담하는게 아냐.\"\n",
            "\"That's very kind.\" -> \"참 친절하구나.\"\n",
            "\"What do you like?\" -> \"뭐가 좋아?\"\n",
            "\"Everyone screamed.\" -> \"모두들 기다렸어.\"\n",
            "\"I saw Tom waiting.\" -> \"톰이 기다리는 걸 봤어.\"\n",
            "\"I'm waiting for a reply.\" -> \"나는 프랑스어 수업을 듣고 있어.\"\n",
            "\"Stay a while and listen.\" -> \"가만히 좀 들어.\"\n",
            "\"Tom is a lovable person.\" -> \"톰은 아직도 꽤 어려.\"\n",
            "\"What does it consist of?\" -> \"뭘로 구성되어 있어?\"\n",
            "\"Do you want me to answer?\" -> \"내가 대답하길 원해?\"\n",
            "\"I have to say I envy you.\" -> \"네가 부럽다고 말해야할 것 같아.\"\n",
            "\"I've gained weight again.\" -> \"또 살이 쪄버렸어.\"\n",
            "\"The boys have gone north.\" -> \"그 사람들은 막 바로 먹고 있어.\"\n",
            "\"Tom is passed out in bed.\" -> \"톰은 메리의 감정을 상하게 했다.\"\n",
            "\"Would you like some more?\" -> \"더 먹을래?\"\n",
            "\"French bread is delicious.\" -> \"프랑스 빵은 맛있습니다.\"\n",
            "\"I haven't kissed Mary yet.\" -> \"아직 메리랑 키스 못 해봤어.\"\n",
            "\"I'm interested in science.\" -> \"난 과학에 관심 있어.\"\n",
            "\"One of my cats is missing.\" -> \"내 고양이 중 한 마리가 안 보여.\"\n",
            "\"Tom likes butter too much.\" -> \"톰은 색소폰을 연주할 수 있어.\"\n",
            "\"Where do you want to live?\" -> \"이 먹고 싶어?\"\n",
            "\"Have you ever been in jail?\" -> \"너 해고당한 적 있어?\"\n",
            "\"I tried to act standoffish.\" -> \"난 톰한테 프랑스어를 가르칠 수 있어.\"\n",
            "\"It is already nine o'clock.\" -> \"이건 부정확한 것 같아.\"\n",
            "\"The medicine tastes bitter.\" -> \"그는 새로운 사람들을 녹였어.\"\n",
            "\"Tom is eating potato chips.\" -> \"톰은 메리의 감정을 상하게 했다.\"\n",
            "\"Tom's birthday's coming up.\" -> \"톰은 메리의 감정을 상하게 했다.\"\n",
            "\"Would you mind if I helped?\" -> \"너 좋아해?\"\n",
            "\"Humans originated in Africa.\" -> \"그는 계획을 실행했다.\"\n",
            "\"I think this is Tom's house.\" -> \"이건 부정확한 것 같아.\"\n",
            "\"I heard that Tom attempted suicide.\" -> \"톰이 먹는 걸 들었어.\"\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-44dc2e67cba4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0minput_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_input_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mseq_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mdecoded_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\"{}\" -> \"{}\"'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded_sentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-ffdd7b5df63a>\u001b[0m in \u001b[0;36mdecode_sequence\u001b[0;34m(input_seq)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m   \u001b[0;31m# 입력 문장을 인코딩\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m   \u001b[0menc_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m   \u001b[0;31m# 디코더의 입력으로 쓸 단일 문자\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQ4PFBTpSr0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrKRwrR3Tppv",
        "colab_type": "text"
      },
      "source": [
        "테스트셋 만들기."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yegVgDMzfbdN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path = '/content/test.txt'\n",
        "\n",
        "# Vectorize the data.\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text = line.split('\\t')\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    target_text = '\\t' + target_text + '\\n'\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOCte51Zf5Yu",
        "colab_type": "code",
        "outputId": "dd118f9b-ebec-45ff-fb7c-e670d9aecd0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "input_texts\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\ufeff버래기',\n",
              " '바당에 괴기 사레 마씀',\n",
              " '무사 누게 왔수과',\n",
              " '서울서 족은 아방네 완 마씀',\n",
              " '게민 멩심허영 갔당 옵서',\n",
              " '고랑은 몰라 마씀',\n",
              " '제주도에 왕 봐사 알아짐니다',\n",
              " '돌도 많고 보롬도 많고 비바리도 많고',\n",
              " '유채꽃도 곱드락 호게 피었수다']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwmKB6pkSsn7",
        "colab_type": "code",
        "outputId": "d8474768-f6d4-4461-d617-41a26aa2a663",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "# 테스트 데이터 셋 만들기\n",
        "\n",
        "from keras import layers, models\n",
        "from __future__ import print_function\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Bidirectional\n",
        "import numpy as np\n",
        "from keras import datasets\n",
        "from keras import backend as K\n",
        "from keras.utils import plot_model\n",
        "from matplotlib import ticker\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "data_path = '/content/test.txt'\n",
        "\n",
        "# Vectorize the data.\n",
        "input_texts = []\n",
        "\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text = line.split('\\t')\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    target_text = '\\t' + target_text + '\\n'\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "# 전처리\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "print('Number of samples:', len(input_texts))\n",
        "print('Number of unique input tokens:', num_encoder_tokens)\n",
        "print('Number of unique output tokens:', num_decoder_tokens)\n",
        "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
        "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
        "\n",
        "\n",
        "# 문자 -> 숫자 변환용 사전\n",
        "input_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "# 학습에 사용할 데이터를 담을 3차원 배열\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "\n",
        "# 문장을 문자 단위로 원 핫 인코딩하면서 학습용 데이터를 만듬\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
        "    for t, char in enumerate(target_text):\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
        "        if t > 0:\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
        "\n",
        "# 숫자 -> 문자 변환용 사전\n",
        "reverse_input_char_index = dict(\n",
        "    (i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict(\n",
        "    (i, char) for char, i in target_token_index.items())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples: 9\n",
            "Number of unique input tokens: 62\n",
            "Number of unique output tokens: 66\n",
            "Max sequence length for inputs: 20\n",
            "Max sequence length for outputs: 22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3OWr5U-e1dV",
        "colab_type": "code",
        "outputId": "e75b7990-01c6-405e-d778-c798b2c546b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        }
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "  # 입력 문장을 인코딩\n",
        "  enc_outputs, states_value = encoder_model.predict(input_seq)\n",
        " \n",
        "  # 디코더의 입력으로 쓸 단일 문자\n",
        "  target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "  # 첫 입력은 시작 문자인 '\\t'로 설정\n",
        "  target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        " \n",
        "  # 문장 생성\n",
        "  stop_condition = False\n",
        "  decoded_sentence = ''\n",
        "  while not stop_condition:\n",
        "    # 이전의 출력, 상태를 디코더에 넣어서 새로운 출력, 상태를 얻음\n",
        "    # 이전 문자와 상태로 다음 문자와 상태를 얻는다고 보면 됨.\n",
        "    dec_outputs, h, output_tokens = decoder_model.predict(\n",
        "        [target_seq, states_value, enc_outputs])\n",
        " \n",
        "    # 사전을 사용해서 원 핫 인코딩 출력을 실제 문자로 변환\n",
        "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "    sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "    decoded_sentence += sampled_char\n",
        " \n",
        "    # 종료 문자가 나왔거나 문장 길이가 한계를 넘으면 종료\n",
        "    if (sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length):\n",
        "      stop_condition = True\n",
        " \n",
        "    # 디코더의 다음 입력으로 쓸 데이터 갱신\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    target_seq[0, 0, sampled_token_index] = 1.\n",
        "    \n",
        "    states_value = h\n",
        " \n",
        "  return decoded_sentence\n",
        "\n",
        "for seq_index in range(9):\n",
        "  input_seq = encoder_input_data_test[seq_index: seq_index + 1]\n",
        "  decoded_sentence = decode_sequence(input_seq)\n",
        "  print('\"{}\" -> \"{}\"'.format(input_texts[seq_index], decoded_sentence.strip()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-07cfbd166981>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0minput_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_input_data_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mseq_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m   \u001b[0mdecoded_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\"{}\" -> \"{}\"'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded_sentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-07cfbd166981>\u001b[0m in \u001b[0;36mdecode_sequence\u001b[0;34m(input_seq)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;31m# 입력 문장을 인코딩\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0menc_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m# 디코더의 입력으로 쓸 단일 문자\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m         \u001b[0;31m# Case 2: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1380\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    139\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_1 to have shape (165, 787) but got array with shape (20, 62)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRr-uZL4e1hz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP-PbxIwe1kp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCh33atHwvI2",
        "colab_type": "code",
        "outputId": "5591393b-adab-4a2a-e69c-1f7553a29e8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "encoder_model.predict(encoder_input_data[1:2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[ 0.0000000e+00, -0.0000000e+00, -0.0000000e+00,  0.0000000e+00,\n",
              "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  1.6329135e-01,\n",
              "          6.4644217e-04, -0.0000000e+00, -9.2373818e-02,  2.9097532e-28,\n",
              "         -0.0000000e+00, -8.5620570e-01,  0.0000000e+00,  0.0000000e+00,\n",
              "         -9.6190804e-01,  0.0000000e+00,  1.4762883e-31,  0.0000000e+00,\n",
              "         -2.5769413e-02, -1.5891892e-01,  0.0000000e+00, -0.0000000e+00,\n",
              "          0.0000000e+00,  0.0000000e+00, -4.8811927e-01, -5.2259541e-01,\n",
              "          2.4740072e-01,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
              "         -4.8581851e-03, -1.8517128e-01, -0.0000000e+00, -4.0916356e-01,\n",
              "          0.0000000e+00, -9.0105736e-01, -0.0000000e+00,  5.5774748e-03,\n",
              "         -2.9330635e-01, -9.8345417e-01,  0.0000000e+00,  0.0000000e+00,\n",
              "         -0.0000000e+00,  0.0000000e+00, -0.0000000e+00,  3.7912405e-01,\n",
              "         -8.5560732e-02, -0.0000000e+00,  0.0000000e+00, -0.0000000e+00,\n",
              "         -2.9002559e-01, -0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
              "         -0.0000000e+00,  7.6037529e-03,  0.0000000e+00, -0.0000000e+00,\n",
              "          9.0489155e-01,  2.0254247e-01,  0.0000000e+00,  4.6030561e-23,\n",
              "          0.0000000e+00,  0.0000000e+00, -7.6174669e-02, -0.0000000e+00,\n",
              "          0.0000000e+00,  0.0000000e+00, -0.0000000e+00,  2.5714195e-01,\n",
              "          8.8205129e-02, -0.0000000e+00,  0.0000000e+00, -0.0000000e+00,\n",
              "         -0.0000000e+00,  0.0000000e+00, -4.2958260e-02, -4.8764575e-01,\n",
              "         -0.0000000e+00,  0.0000000e+00,  4.7477731e-01,  8.6879867e-01,\n",
              "          1.1599462e-02,  0.0000000e+00,  9.0954005e-04, -1.2454973e-01,\n",
              "          0.0000000e+00,  0.0000000e+00,  9.9968106e-01, -9.1520578e-02,\n",
              "          7.9044974e-01, -2.3096931e-01,  0.0000000e+00, -0.0000000e+00,\n",
              "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00, -0.0000000e+00,\n",
              "          0.0000000e+00,  4.7353655e-01,  4.7984362e-02, -6.8209851e-01,\n",
              "          0.0000000e+00, -7.3482305e-01, -8.5555112e-31, -5.8489289e-02,\n",
              "          0.0000000e+00,  1.3068425e-05, -0.0000000e+00, -5.5376284e-02,\n",
              "         -0.0000000e+00,  0.0000000e+00, -0.0000000e+00, -9.4534010e-02,\n",
              "         -0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
              "          0.0000000e+00, -9.4223726e-01, -4.8242757e-01, -0.0000000e+00,\n",
              "          1.2427284e-01,  0.0000000e+00,  6.2701292e-02, -9.9980128e-01,\n",
              "         -1.8277472e-01, -5.0021201e-02,  0.0000000e+00,  6.6289696e-04,\n",
              "          0.0000000e+00, -3.8783801e-01, -0.0000000e+00, -0.0000000e+00,\n",
              "         -0.0000000e+00, -1.9770602e-02,  0.0000000e+00, -9.9869603e-01,\n",
              "         -0.0000000e+00, -0.0000000e+00,  0.0000000e+00, -2.7131078e-01,\n",
              "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  1.6902500e-01,\n",
              "          0.0000000e+00, -1.5806250e-02,  0.0000000e+00,  0.0000000e+00,\n",
              "          5.6324024e-37,  0.0000000e+00, -0.0000000e+00,  0.0000000e+00,\n",
              "         -0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
              "         -0.0000000e+00, -0.0000000e+00,  0.0000000e+00, -0.0000000e+00,\n",
              "          2.4540097e-02,  7.3298591e-01, -1.1825219e-01, -0.0000000e+00,\n",
              "         -4.2891189e-01, -1.8483192e-02, -0.0000000e+00,  0.0000000e+00,\n",
              "          0.0000000e+00, -0.0000000e+00, -7.6095667e-04,  0.0000000e+00,\n",
              "          3.3993727e-01, -1.1197328e-01, -3.5808769e-01, -0.0000000e+00,\n",
              "         -0.0000000e+00,  5.7189941e-01,  0.0000000e+00,  0.0000000e+00,\n",
              "          1.0680524e-01, -0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
              "          0.0000000e+00, -6.4338589e-01,  0.0000000e+00,  0.0000000e+00,\n",
              "         -9.7646445e-01,  0.0000000e+00,  0.0000000e+00, -0.0000000e+00,\n",
              "         -4.5837779e-03,  0.0000000e+00, -0.0000000e+00,  0.0000000e+00,\n",
              "          0.0000000e+00, -7.2626758e-01,  1.0000000e+00,  0.0000000e+00,\n",
              "         -0.0000000e+00,  0.0000000e+00,  1.8077916e-01, -0.0000000e+00,\n",
              "         -0.0000000e+00,  3.1599915e-01, -7.0703638e-01,  0.0000000e+00,\n",
              "          0.0000000e+00, -3.6475569e-01,  0.0000000e+00,  0.0000000e+00,\n",
              "         -0.0000000e+00,  2.0247820e-01,  9.1961674e-02, -1.0000000e+00,\n",
              "         -0.0000000e+00, -0.0000000e+00, -0.0000000e+00, -4.7337539e-03,\n",
              "         -0.0000000e+00, -0.0000000e+00,  3.1825322e-01,  0.0000000e+00,\n",
              "          3.3561704e-03,  0.0000000e+00,  9.5703237e-02, -9.9436027e-01,\n",
              "         -0.0000000e+00, -0.0000000e+00,  0.0000000e+00, -6.5144464e-05,\n",
              "         -0.0000000e+00,  1.5679682e-02,  0.0000000e+00, -0.0000000e+00,\n",
              "         -0.0000000e+00,  4.9511697e-02,  8.6975589e-02,  2.4072388e-01,\n",
              "          1.7965989e-30,  0.0000000e+00, -0.0000000e+00,  2.3980862e-02,\n",
              "         -0.0000000e+00, -0.0000000e+00,  0.0000000e+00, -0.0000000e+00,\n",
              "          0.0000000e+00,  0.0000000e+00, -5.6163829e-02, -9.9846846e-01]],\n",
              "       dtype=float32),\n",
              " array([[ 0.00000000e+00, -0.00000000e+00, -2.98531342e+01,\n",
              "          0.00000000e+00,  0.00000000e+00,  5.01047707e+01,\n",
              "          1.34669174e+02,  2.75115460e-01,  7.64443817e+01,\n",
              "         -1.37504721e+00, -4.31306332e-01,  5.30959675e-28,\n",
              "         -9.65302825e-01, -1.76891494e+00,  0.00000000e+00,\n",
              "          0.00000000e+00, -1.97083485e+00,  0.00000000e+00,\n",
              "          3.09288409e-31,  3.63989830e+00, -1.22796074e+02,\n",
              "         -1.06463289e+01,  0.00000000e+00, -0.00000000e+00,\n",
              "          0.00000000e+00,  1.52326233e+02, -6.13411427e-01,\n",
              "         -1.31800199e+00,  5.11115372e-01,  3.95650327e-01,\n",
              "          4.47818041e+00,  0.00000000e+00, -2.82499511e-02,\n",
              "         -1.50106781e+02, -9.98390656e+01, -9.97066736e-01,\n",
              "          0.00000000e+00, -1.47781253e+00, -1.52559143e+02,\n",
              "          5.97162971e+01, -1.15213966e+00, -6.47213936e+00,\n",
              "          3.40338945e+00,  5.45292816e+01, -1.35125923e+00,\n",
              "          0.00000000e+00, -9.55139637e+00,  5.35434306e-01,\n",
              "         -4.20049953e+00, -0.00000000e+00,  8.07391281e+01,\n",
              "         -0.00000000e+00, -1.34347717e+02, -2.83787346e+00,\n",
              "          8.53425522e+01,  1.00287371e+01, -0.00000000e+00,\n",
              "          3.89394760e-02,  7.21604767e+01, -2.84739571e+01,\n",
              "          1.49858141e+00,  8.66559505e-01,  0.00000000e+00,\n",
              "          5.27181764e-23,  0.00000000e+00,  0.00000000e+00,\n",
              "         -2.94841194e+00, -2.42356211e-01,  7.82024956e+00,\n",
              "          0.00000000e+00, -2.48831773e+00,  2.10909100e+01,\n",
              "          1.51339096e+02, -7.86861944e+00,  0.00000000e+00,\n",
              "         -1.09752560e+00, -8.30106125e+01,  1.05638433e+00,\n",
              "         -1.44126923e+02, -1.39781160e+01, -0.00000000e+00,\n",
              "          0.00000000e+00,  9.87113647e+01,  4.58214712e+00,\n",
              "          3.48192382e+00,  0.00000000e+00,  9.85414684e-02,\n",
              "         -5.12531340e-01,  1.79604130e+01,  1.56015730e+01,\n",
              "          4.37174606e+00, -3.79684687e-01,  1.07262921e+00,\n",
              "         -7.01352775e-01,  0.00000000e+00, -7.02870488e-01,\n",
              "          4.47108955e+01,  1.00930511e+02,  0.00000000e+00,\n",
              "         -6.31350219e-01,  1.52941370e+00,  1.47939575e+02,\n",
              "          9.42515259e+01, -7.03328323e+00,  1.59019928e+01,\n",
              "         -1.40816402e+00, -1.14104644e-30, -4.34135765e-01,\n",
              "          5.03126860e-01,  3.51720482e-05, -6.04815602e-01,\n",
              "         -2.18326032e-01, -1.26794418e+02,  1.80413389e+00,\n",
              "         -1.26950867e+02, -2.67322235e+01, -1.13064095e-01,\n",
              "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "          0.00000000e+00, -1.75762594e+00, -7.57123709e-01,\n",
              "         -0.00000000e+00,  5.61144233e-01,  4.04905224e+00,\n",
              "          5.16400814e-01, -4.60827446e+00, -4.16803002e-01,\n",
              "         -1.49123383e+02,  3.38845491e+00,  3.63321020e-03,\n",
              "          0.00000000e+00, -7.07973862e+01, -1.48403478e+00,\n",
              "         -4.20976996e-01, -9.46586132e-01, -5.55929899e+00,\n",
              "          0.00000000e+00, -3.66741586e+00, -1.50491745e+02,\n",
              "         -1.30656372e+02,  1.25606880e+02, -6.02356613e-01,\n",
              "          8.84825349e-01,  0.00000000e+00,  0.00000000e+00,\n",
              "          5.57098269e-01,  2.85795659e-01, -7.32855141e-01,\n",
              "          1.58492327e+00,  0.00000000e+00,  1.47897632e-36,\n",
              "          0.00000000e+00, -8.95267391e+00,  4.56423424e-02,\n",
              "         -6.76231384e-01,  1.21814432e+01,  0.00000000e+00,\n",
              "          6.62447023e+00, -6.24138975e+00, -1.23128414e+00,\n",
              "          1.51434036e+02, -1.85090661e-01,  1.33262224e+01,\n",
              "          9.35149908e-01, -8.43520508e+01, -0.00000000e+00,\n",
              "         -1.87059665e+00, -1.36203461e+02, -1.15526062e+02,\n",
              "          0.00000000e+00,  0.00000000e+00, -5.28743744e+00,\n",
              "         -4.28142399e-03,  0.00000000e+00,  5.57438431e+01,\n",
              "         -1.12444803e-01, -7.98213363e-01, -3.45038652e-01,\n",
              "         -9.80087146e-02,  1.07140183e+00,  0.00000000e+00,\n",
              "          1.19289026e+01,  8.01095104e+00, -1.92479014e+00,\n",
              "          2.59952545e+00,  1.58299446e+00,  0.00000000e+00,\n",
              "         -2.12397432e+00,  0.00000000e+00,  5.42274475e-01,\n",
              "         -2.21527672e+00,  0.00000000e+00,  1.75964146e+01,\n",
              "         -2.75903702e+00, -1.19178796e+00,  0.00000000e+00,\n",
              "         -3.62383866e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         -7.28727102e+00,  1.40233490e+02,  1.38978500e+02,\n",
              "         -1.32205475e+02,  0.00000000e+00,  1.07438957e+02,\n",
              "         -1.68638859e+01, -1.54776020e+01,  5.51546693e-01,\n",
              "         -3.50762100e+01,  3.59794855e+00,  0.00000000e+00,\n",
              "         -1.29502472e+02,  0.00000000e+00,  1.01906616e+02,\n",
              "         -1.14045391e+01,  3.80389661e-01,  5.82386672e-01,\n",
              "         -2.35989819e+01, -0.00000000e+00, -0.00000000e+00,\n",
              "         -6.28876114e+00, -1.21654216e-02, -0.00000000e+00,\n",
              "         -1.35992233e+02,  5.54244578e-01,  0.00000000e+00,\n",
              "          6.99225161e-03,  5.47310486e+01,  1.03944087e+00,\n",
              "         -2.93411875e+00, -7.10304642e+00, -0.00000000e+00,\n",
              "          0.00000000e+00, -1.34732662e-04, -1.46412430e+01,\n",
              "          2.07393646e+00,  0.00000000e+00, -5.52425041e+01,\n",
              "         -4.39005566e+00,  1.45118043e-01,  2.00735474e+00,\n",
              "          1.41495163e+02,  2.80820861e-30,  1.17513478e-01,\n",
              "         -2.34038830e-01,  3.31161946e-01, -7.67882442e+00,\n",
              "         -9.32745590e+01,  2.23502254e+01, -1.27744579e+01,\n",
              "          0.00000000e+00,  2.05016117e+01, -7.43866488e-02,\n",
              "         -3.58693957e+00]], dtype=float32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJ3WbpinoRgH",
        "colab_type": "code",
        "outputId": "e057efd1-d414-45b8-c3ae-35fd3f9822a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.history.history"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'acc': [0.90837691092862,\n",
              "  0.9354104222362614,\n",
              "  0.9352390464916024,\n",
              "  0.9354153230626095,\n",
              "  0.9354104250135388,\n",
              "  0.9354104203135309,\n",
              "  0.9354104260817223,\n",
              "  0.9354985636194975,\n",
              "  0.9354740763650573,\n",
              "  0.9357874575054347,\n",
              "  0.9370115768952182,\n",
              "  0.9379859761097953,\n",
              "  0.9379174260682958,\n",
              "  0.9392345844631127,\n",
              "  0.9392052049277931,\n",
              "  0.9407378007861449,\n",
              "  0.9414135181348383,\n",
              "  0.9425886760475815,\n",
              "  0.94316646681037,\n",
              "  0.9442583754071198,\n",
              "  0.9448704377724706,\n",
              "  0.945830150530757,\n",
              "  0.9463393848429444,\n",
              "  0.9472452385023931,\n",
              "  0.9479160537429181,\n",
              "  0.9490373484977257,\n",
              "  0.9495367941890566,\n",
              "  0.9505209937745098,\n",
              "  0.9513484948852157,\n",
              "  0.9521123431489459,\n",
              "  0.952788058574909,\n",
              "  0.9535029479252395,\n",
              "  0.9545067294524134,\n",
              "  0.9551873378001661,\n",
              "  0.9555986517219133,\n",
              "  0.9564947028741188,\n",
              "  0.9572438636133748,\n",
              "  0.9581693034872787,\n",
              "  0.9589380525773571,\n",
              "  0.9600250680386806,\n",
              "  0.9607056838637185,\n",
              "  0.9617731156742274,\n",
              "  0.9628699238155051,\n",
              "  0.9635407493105926,\n",
              "  0.9643927425893831,\n",
              "  0.9656119639300959,\n",
              "  0.9662093338573278,\n",
              "  0.9673306288257721,\n",
              "  0.9682854411849838,\n",
              "  0.9693332959674166,\n",
              "  0.9702979027156762,\n",
              "  0.9709491358008436,\n",
              "  0.9718402938176227,\n",
              "  0.9728587699192827,\n",
              "  0.9735638546260027,\n",
              "  0.9750034322020829,\n",
              "  0.9753266011087698,\n",
              "  0.9763058887587653,\n",
              "  0.9772019497382598,\n",
              "  0.9780343557344116,\n",
              "  0.9786317256616435,\n",
              "  0.9775593919566028,\n",
              "  0.9804727996976572,\n",
              "  0.9809379712227853,\n",
              "  0.9807323230210171,\n",
              "  0.9815059616146976,\n",
              "  0.9820298909286445,\n",
              "  0.9826272614967866,\n",
              "  0.983200155919598,\n",
              "  0.9838073153222333,\n",
              "  0.9843067580226502,\n",
              "  0.9844830275436456,\n",
              "  0.9850706022272828,\n",
              "  0.9854574263309492,\n",
              "  0.9859127979124745,\n",
              "  0.9859519750413929,\n",
              "  0.9863485916113768,\n",
              "  0.9866374837882202,\n",
              "  0.9869459647431595,\n",
              "  0.9870928637015777,\n",
              "  0.9873034133706041,\n",
              "  0.9874649953671254,\n",
              "  0.9877979580219501,\n",
              "  0.9881309202495014,\n",
              "  0.9882043713309859,\n",
              "  0.9882729111179229,\n",
              "  0.9884149111727233,\n",
              "  0.988522636847684,\n",
              "  0.9881896669292108,\n",
              "  0.9888262088580798,\n",
              "  0.9888360090153192,\n",
              "  0.9890808236641696,\n",
              "  0.9890808277232672,\n",
              "  0.9892375140207216,\n",
              "  0.9892571047215479,\n",
              "  0.9891395863666329,\n",
              "  0.9892522053906567,\n",
              "  0.9893550225483474,\n",
              "  0.9895606775864906,\n",
              "  0.9896292286961736],\n",
              " 'loss': [1.0096772083458507,\n",
              "  0.4459582566146782,\n",
              "  0.5313804481832785,\n",
              "  0.45794955141655436,\n",
              "  0.4284211137602406,\n",
              "  0.43195175376844236,\n",
              "  0.4269149436959229,\n",
              "  0.4198300125991999,\n",
              "  0.40419935496477244,\n",
              "  0.38667374582273556,\n",
              "  0.40547882933770457,\n",
              "  0.35477240901694074,\n",
              "  0.38163947621126754,\n",
              "  0.34341190418889445,\n",
              "  0.3816452809345765,\n",
              "  0.32727003749126177,\n",
              "  0.34477989936387665,\n",
              "  0.31009486867749136,\n",
              "  0.3107977184770782,\n",
              "  0.296052504397635,\n",
              "  0.353442058875142,\n",
              "  0.2829253629757939,\n",
              "  0.2789535340869726,\n",
              "  0.2722864289864844,\n",
              "  0.26841296271611287,\n",
              "  0.25975411120922337,\n",
              "  0.2558082861285056,\n",
              "  0.2514205320761623,\n",
              "  0.2440831141018953,\n",
              "  0.23859162762173616,\n",
              "  0.23346964635729361,\n",
              "  0.23040057777503914,\n",
              "  0.22234650623841098,\n",
              "  0.21799792605702595,\n",
              "  0.21396246621899281,\n",
              "  0.2089877690465647,\n",
              "  0.20390964910975493,\n",
              "  0.19830190708133055,\n",
              "  0.19320864094200954,\n",
              "  0.18858892174177272,\n",
              "  0.18335340911769524,\n",
              "  0.17914873393633032,\n",
              "  0.17309058684602005,\n",
              "  0.1693868309686688,\n",
              "  0.16545846102271883,\n",
              "  0.15966681254807338,\n",
              "  0.15639500052911834,\n",
              "  0.15184295978597415,\n",
              "  0.14713666884488957,\n",
              "  0.14274392450582168,\n",
              "  0.13894881623192928,\n",
              "  0.13497098740924643,\n",
              "  0.13157488849000692,\n",
              "  0.12706793801972516,\n",
              "  0.12411574992654999,\n",
              "  0.12010558029656769,\n",
              "  0.11675590840184988,\n",
              "  0.11371284181178684,\n",
              "  0.11013143573717404,\n",
              "  0.1075243863901357,\n",
              "  0.10419286979782966,\n",
              "  0.10961638249483587,\n",
              "  0.09778666661845313,\n",
              "  0.09583109470358031,\n",
              "  0.09459021919837562,\n",
              "  0.09179557988079645,\n",
              "  0.08991102660642303,\n",
              "  0.08756189670507199,\n",
              "  0.08541964699504195,\n",
              "  0.0829613200759375,\n",
              "  0.08074136524324349,\n",
              "  0.07920727577047108,\n",
              "  0.07697737513370412,\n",
              "  0.0753729007470565,\n",
              "  0.07351873160797208,\n",
              "  0.07251052630524482,\n",
              "  0.07038084967696111,\n",
              "  0.06892894291215473,\n",
              "  0.06748521960871194,\n",
              "  0.06643989495265441,\n",
              "  0.06475451943801723,\n",
              "  0.06376133218247404,\n",
              "  0.06274024730942156,\n",
              "  0.06147409663085015,\n",
              "  0.060475144401780166,\n",
              "  0.0594091140214474,\n",
              "  0.05859340213265898,\n",
              "  0.05770886611981204,\n",
              "  0.05873666495405218,\n",
              "  0.05549039102850422,\n",
              "  0.05519866908643408,\n",
              "  0.05436327612848692,\n",
              "  0.05406563385893794,\n",
              "  0.05292724322621113,\n",
              "  0.05244507819520957,\n",
              "  0.05285052322251822,\n",
              "  0.05200746125378062,\n",
              "  0.051306821218955474,\n",
              "  0.0502657447023631,\n",
              "  0.04984785213158549]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHKk1sM1GiVg",
        "colab_type": "code",
        "outputId": "35d39507-bb81-48c9-c21a-d654ac863903",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "for seq_index in range(2):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = encoder_input_data[seq_index: seq_index + 5]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', input_texts[seq_index])\n",
        "    print('Decoded sentence:', decoded_sentence)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: 하르방 \n",
            "Decoded sentence:  감물들인인옷\n",
            "\n",
            "-\n",
            "Input sentence: 할망 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLt1JGEtHGQK",
        "colab_type": "code",
        "outputId": "f13c1c8a-ce68-4f6a-d43f-5a4e9e25c703",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "decoded_sentence[1:3]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'감물'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkuN6RnpGPHz",
        "colab_type": "code",
        "outputId": "dd320e72-d6f3-4351-e0da-33e89b69818b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "# Define sampling models\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)\n",
        "\n",
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict(\n",
        "    (i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict(\n",
        "    (i, char) for char, i in target_token_index.items())\n",
        "\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_char == '\\n' or\n",
        "           len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence\n",
        "\n",
        "\n",
        "for seq_index in range(100):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = encoder_input_data[seq_index: seq_index + 10]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', input_texts[seq_index])\n",
        "    print('Decoded sentence:', decoded_sentence)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: 하르방 \n",
            "Decoded sentence:  감물들인인옷\n",
            "\n",
            "-\n",
            "Input sentence: 할망 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 아방 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 어멍 \n",
            "Decoded sentence:  감물들인인옷\n",
            "\n",
            "-\n",
            "Input sentence: 비바리 \n",
            "Decoded sentence:  돼지\n",
            "\n",
            "-\n",
            "Input sentence: 괸당 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 걸바시 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 넹바리 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 다슴아돌 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 말젯놈 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 소나이 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 성님 \n",
            "Decoded sentence:  여자\n",
            "\n",
            "-\n",
            "Input sentence: 작산 거 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 좀녀 \n",
            "Decoded sentence:  여자\n",
            "\n",
            "-\n",
            "Input sentence: 촐람생이 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 홀아방 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 가달 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 꼴랑지 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 구뚱배기 \n",
            "Decoded sentence:  잠잠하다\n",
            "\n",
            "-\n",
            "Input sentence: 꽝 \n",
            "Decoded sentence:  돼지\n",
            "\n",
            "-\n",
            "Input sentence: 굴레 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 대망생이 \n",
            "Decoded sentence:  잠잠하다\n",
            "\n",
            "-\n",
            "Input sentence: 등땡이 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 또꼬망 \n",
            "Decoded sentence:  여\n",
            "\n",
            "-\n",
            "Input sentence: 모감지 \n",
            "Decoded sentence:  돼지\n",
            "\n",
            "-\n",
            "Input sentence: 베 봉탱이 \n",
            "Decoded sentence:  어 지\n",
            "\n",
            "-\n",
            "Input sentence: 베아지 볼라불라\n",
            "Decoded sentence:  어 지\n",
            "\n",
            "-\n",
            "Input sentence: 상판이 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 야게기 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 야굴탁 \n",
            "Decoded sentence:  잠잠하다\n",
            "\n",
            "-\n",
            "Input sentence: 임댕이 \n",
            "Decoded sentence:  잠잠하다\n",
            "\n",
            "-\n",
            "Input sentence: 정겡이 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 저껭이 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 조금태기 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 좀짐팽이 \n",
            "Decoded sentence:  여\n",
            "\n",
            "-\n",
            "Input sentence: 허운데기 \n",
            "Decoded sentence:  잠하다\n",
            "\n",
            "-\n",
            "Input sentence: 허벅다리 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 놋 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 간수메 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 개역 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 것 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 괴기 \n",
            "Decoded sentence:  여\n",
            "\n",
            "-\n",
            "Input sentence: 바당괴기 \n",
            "Decoded sentence:  여자\n",
            "\n",
            "-\n",
            "Input sentence: 돗괴기 \n",
            "Decoded sentence:  여자\n",
            "\n",
            "-\n",
            "Input sentence: 쇠괴기 \n",
            "Decoded sentence:  여자\n",
            "\n",
            "-\n",
            "Input sentence: 도괴기 \n",
            "Decoded sentence:  여\n",
            "\n",
            "-\n",
            "Input sentence: 곤떡 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 곤밥 \n",
            "Decoded sentence:  여\n",
            "\n",
            "-\n",
            "Input sentence: 놈삐 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 대사니김치 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 마농 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 마농 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 조배기 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 촐래 \n",
            "Decoded sentence:  여\n",
            "\n",
            "-\n",
            "Input sentence: 촘지금 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 짐치 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 촙쏠 \n",
            "Decoded sentence:  여자\n",
            "\n",
            "-\n",
            "Input sentence: 조팝 \n",
            "Decoded sentence:  돼지\n",
            "\n",
            "-\n",
            "Input sentence: 갈옷 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 갈 적삼 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 갈 중이 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 강알터진 바지 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 게와 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 단취 \n",
            "Decoded sentence:  여자\n",
            "\n",
            "-\n",
            "Input sentence: 밀랑 페랭이 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 보선 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 소중이 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 신착 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 찍신 \n",
            "Decoded sentence:  잠잠하다\n",
            "\n",
            "-\n",
            "Input sentence: 좀뱅이 \n",
            "Decoded sentence:  잠잠하다\n",
            "\n",
            "-\n",
            "Input sentence: 등지게 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 고장중이 \n",
            "Decoded sentence:  돼지\n",
            "\n",
            "-\n",
            "Input sentence: 도폭 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 두루막 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 베불레기 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 우장 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 저구리 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 지성귀 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 지서귀 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 쪼께 \n",
            "Decoded sentence:  여자\n",
            "\n",
            "-\n",
            "Input sentence: 치메 \n",
            "Decoded sentence:  돼지\n",
            "\n",
            "-\n",
            "Input sentence: 건대 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 사모관대 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 시미옷 \n",
            "Decoded sentence:  여자\n",
            "\n",
            "-\n",
            "Input sentence: 제복 \n",
            "Decoded sentence:  여\n",
            "\n",
            "-\n",
            "Input sentence: 망근 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 방립 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 벙것 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 상갓 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 탕근 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 풍뎅이 \n",
            "Decoded sentence:  여\n",
            "\n",
            "-\n",
            "Input sentence: 휘양 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 낭저 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 달리 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 빈네 \n",
            "Decoded sentence:  여자\n",
            "\n",
            "-\n",
            "Input sentence: 상퉁이 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 얼레기 \n",
            "Decoded sentence:  잠잠하다\n",
            "\n",
            "-\n",
            "Input sentence: 얼레빗 \n",
            "Decoded sentence:  잠잠하다\n",
            "\n",
            "-\n",
            "Input sentence: 쪽도리 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 쳉빗 \n",
            "Decoded sentence:  돼지\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}