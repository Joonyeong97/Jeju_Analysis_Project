{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kor_jeju.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEIky_zshPjR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://neurowhai.tistory.com/292"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa4IReo6Cc7a",
        "colab_type": "code",
        "outputId": "686f61e0-9abf-499a-c8d6-73dd43f1dbed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "import tensorflow as tf\n",
        "device_lib.list_local_devices()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 3876883829019089860, name: \"/device:XLA_CPU:0\"\n",
              " device_type: \"XLA_CPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 5933003659533636849\n",
              " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n",
              " device_type: \"XLA_GPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 12620358854082576940\n",
              " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 15956161332\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 11978132078730237417\n",
              " physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNlo6VGXFllH",
        "colab_type": "code",
        "outputId": "c3e2a26a-3935-4014-ec1f-81e907e0ed24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xGdGylMFawA",
        "colab_type": "code",
        "outputId": "cabbb8e0-f291-4755-b4c6-9840de8cff14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_Q2fuUMCOaO",
        "colab_type": "code",
        "outputId": "7c7c0044-e480-4239-8361-2ed11ba8e4c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras import layers, models\n",
        "from __future__ import print_function\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Bidirectional\n",
        "import numpy as np\n",
        "from keras import datasets\n",
        "from keras import backend as K\n",
        "from keras.utils import plot_model\n",
        "import matplotlib\n",
        "from matplotlib import ticker\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "batch_size = 32  # Batch size for training.\n",
        "epochs = 100  # Number of epochs to train for.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "num_samples = 10000  # Number of samples to train on.\n",
        "# Path to the data txt file on disk.\n",
        "data_path = '/content/dataset.txt'\n",
        "\n",
        "# Vectorize the data.\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text = line.split('\\t')\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    target_text = '\\t' + target_text + '\\n'\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "# 전처리\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "print('Number of samples:', len(input_texts))\n",
        "print('Number of unique input tokens:', num_encoder_tokens)\n",
        "print('Number of unique output tokens:', num_decoder_tokens)\n",
        "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
        "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
        "\n",
        "\n",
        "# 문자 -> 숫자 변환용 사전\n",
        "input_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "# 학습에 사용할 데이터를 담을 3차원 배열\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "\n",
        "# 문장을 문자 단위로 원 핫 인코딩하면서 학습용 데이터를 만듬\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
        "    for t, char in enumerate(target_text):\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
        "        if t > 0:\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
        "\n",
        "# 숫자 -> 문자 변환용 사전\n",
        "reverse_input_char_index = dict(\n",
        "    (i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict(\n",
        "    (i, char) for char, i in target_token_index.items())\n",
        "\n",
        "def RepeatVectorLayer(rep, axis):\n",
        "  return layers.Lambda(lambda x: K.repeat_elements(K.expand_dims(x, axis), rep, axis),\n",
        "                      lambda x: tuple((x[0],) + x[1:axis] + (rep,) + x[axis:]))\n",
        "\n",
        "\n",
        "# 인코더 생성\n",
        "encoder_inputs = layers.Input(shape=(max_encoder_seq_length, num_encoder_tokens))\n",
        "encoder = layers.GRU(latent_dim, return_sequences=True, return_state=True)\n",
        "encoder_outputs, state_h = encoder(encoder_inputs)\n",
        "\n",
        "# 디코더 생성\n",
        "decoder_inputs = layers.Input(shape=(max_decoder_seq_length, num_decoder_tokens))\n",
        "decoder = layers.GRU(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _ = decoder(decoder_inputs, initial_state=state_h)\n",
        "\n",
        "# attention 생성\n",
        "\n",
        "repeat_d_layer = RepeatVectorLayer(max_encoder_seq_length, 2)\n",
        "repeat_d = repeat_d_layer(decoder_outputs)\n",
        "\n",
        "repeat_e_layer = RepeatVectorLayer(max_decoder_seq_length, 1)\n",
        "repeat_e = repeat_e_layer(encoder_outputs)\n",
        "\n",
        "concat_for_score_layer = layers.Concatenate(axis=-1)\n",
        "concat_for_score = concat_for_score_layer([repeat_d, repeat_e])\n",
        "\n",
        "dense1_t_score_layer = layers.Dense(latent_dim // 2, activation='tanh')\n",
        "dense1_score_layer = layers.TimeDistributed(dense1_t_score_layer)\n",
        "dense1_score = dense1_score_layer(concat_for_score)\n",
        "dense2_t_score_layer = layers.Dense(1)\n",
        "dense2_score_layer = layers.TimeDistributed(dense2_t_score_layer)\n",
        "dense2_score = dense2_score_layer(dense1_score)\n",
        "dense2_score = layers.Reshape((max_decoder_seq_length, max_encoder_seq_length))(dense2_score)\n",
        "\n",
        "softmax_score_layer = layers.Softmax(axis=-1)\n",
        "softmax_score = softmax_score_layer(dense2_score)\n",
        "\n",
        "repeat_score_layer = RepeatVectorLayer(latent_dim, 2)\n",
        "repeat_score = repeat_score_layer(softmax_score)\n",
        "\n",
        "permute_e = layers.Permute((2, 1))(encoder_outputs)\n",
        "repeat_e_layer = RepeatVectorLayer(max_decoder_seq_length, 1)\n",
        "repeat_e = repeat_e_layer(permute_e)\n",
        "\n",
        "attended_mat_layer = layers.Multiply()\n",
        "attended_mat = attended_mat_layer([repeat_score, repeat_e])\n",
        "\n",
        "context_layer = layers.Lambda(lambda x: K.sum(x, axis=-1),\n",
        "                             lambda x: tuple(x[:-1]))\n",
        "context = context_layer(attended_mat)\n",
        "\n",
        "concat_context_layer = layers.Concatenate(axis=-1)\n",
        "concat_context = concat_context_layer([context, decoder_outputs])\n",
        "\n",
        "attention_dense_output_layer = layers.Dense(latent_dim, activation='tanh')\n",
        "attention_output_layer = layers.TimeDistributed(attention_dense_output_layer)\n",
        "attention_output = attention_output_layer(concat_context)\n",
        "\n",
        "decoder_dense = layers.Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(attention_output)\n",
        "\n",
        "\n",
        "# 모델 생성\n",
        "model = models.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "che = 'keras_model1.model'\n",
        "point = ModelCheckpoint(filepath=che , monitor='val_loss', verbose=1, save_best_only=True)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=100)\n",
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "history = model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_split=0.2,\n",
        "                    verbose=1,callbacks=[point,early_stopping])\n",
        "# Save model\n",
        "# model.save('s2s.h5')\n",
        "# \n",
        "\n",
        "\n",
        "# Next: inference mode (sampling).\n",
        "# Here's the drill:\n",
        "# 1) encode input and retrieve initial decoder state\n",
        "# 2) run one step of decoder with this initial state\n",
        "# and a \"start of sequence\" token as target.\n",
        "# Output will be the next target token\n",
        "# 3) Repeat with the current target token and current states\n",
        "\n",
        "# Define sampling models\n",
        "encoder_model = models.Model(encoder_inputs, [encoder_outputs, state_h])\n",
        "encoder_outputs_input = layers.Input(shape=(max_encoder_seq_length, latent_dim))\n",
        "\n",
        "decoder_inputs = layers.Input(shape=(1, num_decoder_tokens))\n",
        "decoder_state_input_h = layers.Input(shape=(latent_dim,))\n",
        "decoder_outputs, decoder_h = decoder(decoder_inputs, initial_state=decoder_state_input_h)\n",
        "\n",
        "repeat_d_layer = RepeatVectorLayer(max_encoder_seq_length, 2)\n",
        "repeat_d = repeat_d_layer(decoder_outputs)\n",
        "\n",
        "repeat_e_layer = RepeatVectorLayer(1, axis=1)\n",
        "repeat_e = repeat_e_layer(encoder_outputs_input)\n",
        "\n",
        "concat_for_score_layer = layers.Concatenate(axis=-1)\n",
        "concat_for_score = concat_for_score_layer([repeat_d, repeat_e])\n",
        "\n",
        "dense1_score_layer = layers.TimeDistributed(dense1_t_score_layer)\n",
        "dense1_score = dense1_score_layer(concat_for_score)\n",
        "\n",
        "dense2_score_layer = layers.TimeDistributed(dense2_t_score_layer)\n",
        "dense2_score = dense2_score_layer(dense1_score)\n",
        "dense2_score = layers.Reshape((1, max_encoder_seq_length))(dense2_score)\n",
        "\n",
        "softmax_score_layer = layers.Softmax(axis=-1)\n",
        "softmax_score = softmax_score_layer(dense2_score)\n",
        "\n",
        "repeat_score_layer = RepeatVectorLayer(latent_dim, 2)\n",
        "repeat_score = repeat_score_layer(softmax_score)\n",
        "\n",
        "permute_e = layers.Permute((2, 1))(encoder_outputs_input)\n",
        "repeat_e_layer = RepeatVectorLayer(1, axis=1)\n",
        "repeat_e = repeat_e_layer(permute_e)\n",
        "\n",
        "attended_mat_layer = layers.Multiply()\n",
        "attended_mat = attended_mat_layer([repeat_score, repeat_e])\n",
        "\n",
        "context_layer = layers.Lambda(lambda x: K.sum(x, axis=-1),\n",
        "                             lambda x: tuple(x[:-1]))\n",
        "context = context_layer(attended_mat)\n",
        "\n",
        "concat_context_layer = layers.Concatenate(axis=-1)\n",
        "concat_context = concat_context_layer([context, decoder_outputs])\n",
        "\n",
        "attention_output_layer = layers.TimeDistributed(attention_dense_output_layer)\n",
        "attention_output = attention_output_layer(concat_context)\n",
        "\n",
        "decoder_att_outputs = decoder_dense(attention_output)\n",
        "\n",
        "decoder_model = models.Model([decoder_inputs, decoder_state_input_h, encoder_outputs_input],\n",
        "                            [decoder_outputs, decoder_h, decoder_att_outputs])\n",
        "\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "  # 입력 문장을 인코딩\n",
        "  enc_outputs, states_value = encoder_model.predict(input_seq)\n",
        " \n",
        "  # 디코더의 입력으로 쓸 단일 문자\n",
        "  target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "  # 첫 입력은 시작 문자인 '\\t'로 설정\n",
        "  target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        " \n",
        "  # 문장 생성\n",
        "  stop_condition = False\n",
        "  decoded_sentence = ''\n",
        "  while not stop_condition:\n",
        "    # 이전의 출력, 상태를 디코더에 넣어서 새로운 출력, 상태를 얻음\n",
        "    # 이전 문자와 상태로 다음 문자와 상태를 얻는다고 보면 됨.\n",
        "    dec_outputs, h, output_tokens = decoder_model.predict(\n",
        "        [target_seq, states_value, enc_outputs])\n",
        " \n",
        "    # 사전을 사용해서 원 핫 인코딩 출력을 실제 문자로 변환\n",
        "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "    sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "    decoded_sentence += sampled_char\n",
        " \n",
        "    # 종료 문자가 나왔거나 문장 길이가 한계를 넘으면 종료\n",
        "    if (sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length):\n",
        "      stop_condition = True\n",
        " \n",
        "    # 디코더의 다음 입력으로 쓸 데이터 갱신\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    target_seq[0, 0, sampled_token_index] = 1.\n",
        "    \n",
        "    states_value = h\n",
        " \n",
        "  return decoded_sentence\n",
        "\n",
        "for seq_index in range(30):\n",
        "  input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
        "  decoded_sentence = decode_sequence(input_seq)\n",
        "  print('\"{}\" -> \"{}\"'.format(input_texts[seq_index], decoded_sentence.strip()))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of samples: 1203\n",
            "Number of unique input tokens: 787\n",
            "Number of unique output tokens: 749\n",
            "Max sequence length for inputs: 165\n",
            "Max sequence length for outputs: 183\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 962 samples, validate on 241 samples\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "962/962 [==============================] - 32s 33ms/step - loss: 0.3414 - acc: 0.0128 - val_loss: 0.5583 - val_acc: 0.0254\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.55828, saving model to keras_model1.model\n",
            "Epoch 2/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.3064 - acc: 0.0137 - val_loss: 0.5449 - val_acc: 0.0272\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.55828 to 0.54485, saving model to keras_model1.model\n",
            "Epoch 3/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.2930 - acc: 0.0153 - val_loss: 0.5217 - val_acc: 0.0288\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.54485 to 0.52167, saving model to keras_model1.model\n",
            "Epoch 4/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.2784 - acc: 0.0164 - val_loss: 0.4990 - val_acc: 0.0280\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.52167 to 0.49898, saving model to keras_model1.model\n",
            "Epoch 5/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.2689 - acc: 0.0175 - val_loss: 0.4758 - val_acc: 0.0318\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.49898 to 0.47575, saving model to keras_model1.model\n",
            "Epoch 6/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.2606 - acc: 0.0186 - val_loss: 0.4775 - val_acc: 0.0317\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.47575\n",
            "Epoch 7/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.2538 - acc: 0.0196 - val_loss: 1.0699 - val_acc: 0.0186\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.47575\n",
            "Epoch 8/100\n",
            "962/962 [==============================] - 30s 32ms/step - loss: 0.2476 - acc: 0.0205 - val_loss: 0.4413 - val_acc: 0.0373\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.47575 to 0.44128, saving model to keras_model1.model\n",
            "Epoch 9/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.2398 - acc: 0.0218 - val_loss: 0.4434 - val_acc: 0.0381\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.44128\n",
            "Epoch 10/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.2334 - acc: 0.0227 - val_loss: 0.4280 - val_acc: 0.0420\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.44128 to 0.42802, saving model to keras_model1.model\n",
            "Epoch 11/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.2259 - acc: 0.0239 - val_loss: 0.4567 - val_acc: 0.0369\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.42802\n",
            "Epoch 12/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.2192 - acc: 0.0244 - val_loss: 0.4219 - val_acc: 0.0425\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.42802 to 0.42188, saving model to keras_model1.model\n",
            "Epoch 13/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.2144 - acc: 0.0253 - val_loss: 0.4198 - val_acc: 0.0404\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.42188 to 0.41981, saving model to keras_model1.model\n",
            "Epoch 14/100\n",
            "962/962 [==============================] - 30s 32ms/step - loss: 0.2068 - acc: 0.0262 - val_loss: 0.4083 - val_acc: 0.0437\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.41981 to 0.40830, saving model to keras_model1.model\n",
            "Epoch 15/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.2008 - acc: 0.0272 - val_loss: 0.4077 - val_acc: 0.0444\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.40830 to 0.40773, saving model to keras_model1.model\n",
            "Epoch 16/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.1944 - acc: 0.0282 - val_loss: 0.4314 - val_acc: 0.0415\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.40773\n",
            "Epoch 17/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.1893 - acc: 0.0294 - val_loss: 0.4115 - val_acc: 0.0442\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.40773\n",
            "Epoch 18/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.1831 - acc: 0.0302 - val_loss: 0.3996 - val_acc: 0.0464\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.40773 to 0.39956, saving model to keras_model1.model\n",
            "Epoch 19/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.1764 - acc: 0.0315 - val_loss: 0.4123 - val_acc: 0.0448\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.39956\n",
            "Epoch 20/100\n",
            "962/962 [==============================] - 30s 32ms/step - loss: 0.1717 - acc: 0.0317 - val_loss: 0.4083 - val_acc: 0.0462\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.39956\n",
            "Epoch 21/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.1655 - acc: 0.0331 - val_loss: 0.4111 - val_acc: 0.0451\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.39956\n",
            "Epoch 22/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.1597 - acc: 0.0340 - val_loss: 0.4986 - val_acc: 0.0385\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.39956\n",
            "Epoch 23/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.1569 - acc: 0.0349 - val_loss: 0.4456 - val_acc: 0.0402\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.39956\n",
            "Epoch 24/100\n",
            "962/962 [==============================] - 30s 32ms/step - loss: 0.1502 - acc: 0.0357 - val_loss: 0.4143 - val_acc: 0.0471\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.39956\n",
            "Epoch 25/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.1444 - acc: 0.0365 - val_loss: 0.4320 - val_acc: 0.0452\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.39956\n",
            "Epoch 26/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.1396 - acc: 0.0375 - val_loss: 0.4208 - val_acc: 0.0463\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.39956\n",
            "Epoch 27/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.1343 - acc: 0.0384 - val_loss: 0.4128 - val_acc: 0.0477\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.39956\n",
            "Epoch 28/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.1293 - acc: 0.0395 - val_loss: 0.4245 - val_acc: 0.0466\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.39956\n",
            "Epoch 29/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.1243 - acc: 0.0402 - val_loss: 0.4475 - val_acc: 0.0424\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.39956\n",
            "Epoch 30/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.1197 - acc: 0.0411 - val_loss: 0.4274 - val_acc: 0.0479\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.39956\n",
            "Epoch 31/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.1148 - acc: 0.0419 - val_loss: 0.4440 - val_acc: 0.0467\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.39956\n",
            "Epoch 32/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.1100 - acc: 0.0428 - val_loss: 0.4588 - val_acc: 0.0456\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.39956\n",
            "Epoch 33/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.1052 - acc: 0.0438 - val_loss: 0.4401 - val_acc: 0.0472\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.39956\n",
            "Epoch 34/100\n",
            "962/962 [==============================] - 30s 32ms/step - loss: 0.1004 - acc: 0.0451 - val_loss: 0.4401 - val_acc: 0.0465\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.39956\n",
            "Epoch 35/100\n",
            "962/962 [==============================] - 30s 32ms/step - loss: 0.0958 - acc: 0.0459 - val_loss: 0.4593 - val_acc: 0.0459\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.39956\n",
            "Epoch 36/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0911 - acc: 0.0468 - val_loss: 0.4537 - val_acc: 0.0470\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.39956\n",
            "Epoch 37/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0870 - acc: 0.0477 - val_loss: 0.4598 - val_acc: 0.0470\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.39956\n",
            "Epoch 38/100\n",
            "962/962 [==============================] - 30s 32ms/step - loss: 0.0830 - acc: 0.0487 - val_loss: 0.4623 - val_acc: 0.0467\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.39956\n",
            "Epoch 39/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0786 - acc: 0.0497 - val_loss: 0.4767 - val_acc: 0.0454\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.39956\n",
            "Epoch 40/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0742 - acc: 0.0507 - val_loss: 0.4946 - val_acc: 0.0425\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.39956\n",
            "Epoch 41/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0707 - acc: 0.0517 - val_loss: 0.5108 - val_acc: 0.0436\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.39956\n",
            "Epoch 42/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0669 - acc: 0.0523 - val_loss: 0.4998 - val_acc: 0.0444\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.39956\n",
            "Epoch 43/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0632 - acc: 0.0534 - val_loss: 0.5114 - val_acc: 0.0444\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.39956\n",
            "Epoch 44/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0595 - acc: 0.0543 - val_loss: 0.5036 - val_acc: 0.0458\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.39956\n",
            "Epoch 45/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0560 - acc: 0.0553 - val_loss: 0.5131 - val_acc: 0.0450\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.39956\n",
            "Epoch 46/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0526 - acc: 0.0562 - val_loss: 0.5231 - val_acc: 0.0439\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.39956\n",
            "Epoch 47/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0488 - acc: 0.0570 - val_loss: 0.5363 - val_acc: 0.0421\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.39956\n",
            "Epoch 48/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0460 - acc: 0.0578 - val_loss: 0.5290 - val_acc: 0.0451\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.39956\n",
            "Epoch 49/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0428 - acc: 0.0586 - val_loss: 0.5351 - val_acc: 0.0456\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.39956\n",
            "Epoch 50/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0401 - acc: 0.0592 - val_loss: 0.5353 - val_acc: 0.0449\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.39956\n",
            "Epoch 51/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0373 - acc: 0.0598 - val_loss: 0.5397 - val_acc: 0.0448\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.39956\n",
            "Epoch 52/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0345 - acc: 0.0606 - val_loss: 0.5575 - val_acc: 0.0431\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.39956\n",
            "Epoch 53/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0323 - acc: 0.0610 - val_loss: 0.5574 - val_acc: 0.0450\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.39956\n",
            "Epoch 54/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0305 - acc: 0.0614 - val_loss: 0.5488 - val_acc: 0.0458\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.39956\n",
            "Epoch 55/100\n",
            "962/962 [==============================] - 30s 32ms/step - loss: 0.0267 - acc: 0.0623 - val_loss: 0.5668 - val_acc: 0.0447\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.39956\n",
            "Epoch 56/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0258 - acc: 0.0624 - val_loss: 0.5676 - val_acc: 0.0442\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.39956\n",
            "Epoch 57/100\n",
            "962/962 [==============================] - 30s 32ms/step - loss: 0.0229 - acc: 0.0631 - val_loss: 0.5799 - val_acc: 0.0429\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.39956\n",
            "Epoch 58/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0213 - acc: 0.0636 - val_loss: 0.5962 - val_acc: 0.0429\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.39956\n",
            "Epoch 59/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0201 - acc: 0.0637 - val_loss: 0.5819 - val_acc: 0.0436\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.39956\n",
            "Epoch 60/100\n",
            "962/962 [==============================] - 30s 32ms/step - loss: 0.0183 - acc: 0.0640 - val_loss: 0.5847 - val_acc: 0.0450\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.39956\n",
            "Epoch 61/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0168 - acc: 0.0643 - val_loss: 0.6242 - val_acc: 0.0414\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.39956\n",
            "Epoch 62/100\n",
            "962/962 [==============================] - 30s 32ms/step - loss: 0.0156 - acc: 0.0647 - val_loss: 0.6051 - val_acc: 0.0440\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.39956\n",
            "Epoch 63/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0138 - acc: 0.0650 - val_loss: 0.6163 - val_acc: 0.0446\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.39956\n",
            "Epoch 64/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0133 - acc: 0.0651 - val_loss: 0.6208 - val_acc: 0.0439\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.39956\n",
            "Epoch 65/100\n",
            "962/962 [==============================] - 31s 32ms/step - loss: 0.0118 - acc: 0.0653 - val_loss: 0.6168 - val_acc: 0.0453\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.39956\n",
            "Epoch 66/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0108 - acc: 0.0656 - val_loss: 0.6313 - val_acc: 0.0446\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.39956\n",
            "Epoch 67/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0101 - acc: 0.0655 - val_loss: 0.6211 - val_acc: 0.0454\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.39956\n",
            "Epoch 68/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0091 - acc: 0.0657 - val_loss: 0.6269 - val_acc: 0.0443\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.39956\n",
            "Epoch 69/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0083 - acc: 0.0659 - val_loss: 0.6518 - val_acc: 0.0435\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.39956\n",
            "Epoch 70/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0077 - acc: 0.0659 - val_loss: 0.6317 - val_acc: 0.0451\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.39956\n",
            "Epoch 71/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0070 - acc: 0.0661 - val_loss: 0.6467 - val_acc: 0.0443\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.39956\n",
            "Epoch 72/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0065 - acc: 0.0661 - val_loss: 0.6430 - val_acc: 0.0449\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.39956\n",
            "Epoch 73/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0061 - acc: 0.0661 - val_loss: 0.6505 - val_acc: 0.0442\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.39956\n",
            "Epoch 74/100\n",
            "962/962 [==============================] - 30s 32ms/step - loss: 0.0055 - acc: 0.0662 - val_loss: 0.6608 - val_acc: 0.0448\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.39956\n",
            "Epoch 75/100\n",
            "962/962 [==============================] - 30s 32ms/step - loss: 0.0050 - acc: 0.0663 - val_loss: 0.6567 - val_acc: 0.0446\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.39956\n",
            "Epoch 76/100\n",
            "962/962 [==============================] - 31s 32ms/step - loss: 0.0042 - acc: 0.0664 - val_loss: 0.6601 - val_acc: 0.0440\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.39956\n",
            "Epoch 77/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0045 - acc: 0.0663 - val_loss: 0.6624 - val_acc: 0.0447\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.39956\n",
            "Epoch 78/100\n",
            "962/962 [==============================] - 30s 32ms/step - loss: 0.0039 - acc: 0.0664 - val_loss: 0.6697 - val_acc: 0.0455\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.39956\n",
            "Epoch 79/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0035 - acc: 0.0664 - val_loss: 0.6859 - val_acc: 0.0446\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.39956\n",
            "Epoch 80/100\n",
            "962/962 [==============================] - 30s 32ms/step - loss: 0.0034 - acc: 0.0664 - val_loss: 0.6712 - val_acc: 0.0448\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.39956\n",
            "Epoch 81/100\n",
            "962/962 [==============================] - 30s 32ms/step - loss: 0.0030 - acc: 0.0664 - val_loss: 0.6931 - val_acc: 0.0452\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.39956\n",
            "Epoch 82/100\n",
            "962/962 [==============================] - 30s 32ms/step - loss: 0.0031 - acc: 0.0664 - val_loss: 0.6751 - val_acc: 0.0454\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.39956\n",
            "Epoch 83/100\n",
            "962/962 [==============================] - 30s 32ms/step - loss: 0.0025 - acc: 0.0665 - val_loss: 0.6936 - val_acc: 0.0444\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.39956\n",
            "Epoch 84/100\n",
            "962/962 [==============================] - 30s 32ms/step - loss: 0.0026 - acc: 0.0664 - val_loss: 0.6826 - val_acc: 0.0451\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.39956\n",
            "Epoch 85/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0025 - acc: 0.0664 - val_loss: 0.6959 - val_acc: 0.0442\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.39956\n",
            "Epoch 86/100\n",
            "962/962 [==============================] - 31s 32ms/step - loss: 0.0021 - acc: 0.0665 - val_loss: 0.6802 - val_acc: 0.0458\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.39956\n",
            "Epoch 87/100\n",
            "962/962 [==============================] - 30s 32ms/step - loss: 0.0022 - acc: 0.0664 - val_loss: 0.6842 - val_acc: 0.0459\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.39956\n",
            "Epoch 88/100\n",
            "962/962 [==============================] - 30s 32ms/step - loss: 0.0018 - acc: 0.0665 - val_loss: 0.6932 - val_acc: 0.0464\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.39956\n",
            "Epoch 89/100\n",
            "962/962 [==============================] - 30s 32ms/step - loss: 0.0019 - acc: 0.0664 - val_loss: 0.7064 - val_acc: 0.0455\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.39956\n",
            "Epoch 90/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0019 - acc: 0.0664 - val_loss: 0.6947 - val_acc: 0.0461\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.39956\n",
            "Epoch 91/100\n",
            "962/962 [==============================] - 30s 32ms/step - loss: 0.0016 - acc: 0.0665 - val_loss: 0.7017 - val_acc: 0.0453\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.39956\n",
            "Epoch 92/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0014 - acc: 0.0665 - val_loss: 0.7142 - val_acc: 0.0457\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.39956\n",
            "Epoch 93/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0014 - acc: 0.0665 - val_loss: 0.7053 - val_acc: 0.0456\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.39956\n",
            "Epoch 94/100\n",
            "962/962 [==============================] - 30s 32ms/step - loss: 0.0013 - acc: 0.0665 - val_loss: 0.7121 - val_acc: 0.0458\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.39956\n",
            "Epoch 95/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0013 - acc: 0.0666 - val_loss: 0.7109 - val_acc: 0.0458\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.39956\n",
            "Epoch 96/100\n",
            "962/962 [==============================] - 30s 32ms/step - loss: 0.0018 - acc: 0.0664 - val_loss: 0.7155 - val_acc: 0.0446\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.39956\n",
            "Epoch 97/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0014 - acc: 0.0665 - val_loss: 0.7089 - val_acc: 0.0455\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.39956\n",
            "Epoch 98/100\n",
            "962/962 [==============================] - 30s 32ms/step - loss: 0.0010 - acc: 0.0666 - val_loss: 0.7172 - val_acc: 0.0451\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.39956\n",
            "Epoch 99/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0014 - acc: 0.0665 - val_loss: 0.7136 - val_acc: 0.0446\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.39956\n",
            "Epoch 100/100\n",
            "962/962 [==============================] - 30s 31ms/step - loss: 0.0012 - acc: 0.0666 - val_loss: 0.7127 - val_acc: 0.0459\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.39956\n",
            "\"하르방 \" -> \"할아버지\"\n",
            "\"할망 \" -> \"할머니\"\n",
            "\"아방 \" -> \"아버지\"\n",
            "\"어멍 \" -> \"어머니\"\n",
            "\"비바리 \" -> \"처녀\"\n",
            "\"괸당 \" -> \"친척\"\n",
            "\"걸바시 \" -> \"거지\"\n",
            "\"넹바리 \" -> \"시집간색시\"\n",
            "\"다슴아돌 \" -> \"의붓아들\"\n",
            "\"말젯놈 \" -> \"세번째자식\"\n",
            "\"소나이 \" -> \"사나이\"\n",
            "\"성님 \" -> \"형님\"\n",
            "\"작산 거 \" -> \"어른이된 사람\"\n",
            "\"좀녀 \" -> \"해녀\"\n",
            "\"촐람생이 \" -> \"경솔한사람\"\n",
            "\"홀아방 \" -> \"홀아비\"\n",
            "\"가달 \" -> \"다리\"\n",
            "\"꼴랑지 \" -> \"꼬리\"\n",
            "\"구뚱배기 \" -> \"귀쪽뺨\"\n",
            "\"꽝 \" -> \"뼈\"\n",
            "\"굴레 \" -> \"입\"\n",
            "\"대망생이 \" -> \"머리\"\n",
            "\"등땡이 \" -> \"등어리\"\n",
            "\"또꼬망 \" -> \"똥구멍\"\n",
            "\"모감지 \" -> \"멱살\"\n",
            "\"베 봉탱이 \" -> \"배 불뚝이\"\n",
            "\"베아지 볼라불라\" -> \"배 밟아버린다\"\n",
            "\"상판이 \" -> \"얼굴\"\n",
            "\"야게기 \" -> \"목\"\n",
            "\"야굴탁 \" -> \"턱\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7YRQPGp0G-O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "09f6ad11-9ee5-4bf3-e7e1-19ba3b452ea4"
      },
      "source": [
        "for seq_index in range(1000):\n",
        "  input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
        "  decoded_sentence = decode_sequence(input_seq)\n",
        "  print('\"{}\" -> \"{}\"'.format(input_texts[seq_index], decoded_sentence.strip()))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"하르방 \" -> \"할아버지\"\n",
            "\"할망 \" -> \"할머니\"\n",
            "\"아방 \" -> \"아버지\"\n",
            "\"어멍 \" -> \"어머니\"\n",
            "\"비바리 \" -> \"처녀\"\n",
            "\"괸당 \" -> \"친척\"\n",
            "\"걸바시 \" -> \"거지\"\n",
            "\"넹바리 \" -> \"시집간색시\"\n",
            "\"다슴아돌 \" -> \"의붓아들\"\n",
            "\"말젯놈 \" -> \"세번째자식\"\n",
            "\"소나이 \" -> \"사나이\"\n",
            "\"성님 \" -> \"형님\"\n",
            "\"작산 거 \" -> \"어른이된 사람\"\n",
            "\"좀녀 \" -> \"해녀\"\n",
            "\"촐람생이 \" -> \"경솔한사람\"\n",
            "\"홀아방 \" -> \"홀아비\"\n",
            "\"가달 \" -> \"다리\"\n",
            "\"꼴랑지 \" -> \"꼬리\"\n",
            "\"구뚱배기 \" -> \"귀쪽뺨\"\n",
            "\"꽝 \" -> \"뼈\"\n",
            "\"굴레 \" -> \"입\"\n",
            "\"대망생이 \" -> \"머리\"\n",
            "\"등땡이 \" -> \"등어리\"\n",
            "\"또꼬망 \" -> \"똥구멍\"\n",
            "\"모감지 \" -> \"멱살\"\n",
            "\"베 봉탱이 \" -> \"배 불뚝이\"\n",
            "\"베아지 볼라불라\" -> \"배 밟아버린다\"\n",
            "\"상판이 \" -> \"얼굴\"\n",
            "\"야게기 \" -> \"목\"\n",
            "\"야굴탁 \" -> \"턱\"\n",
            "\"임댕이 \" -> \"이마\"\n",
            "\"정겡이 \" -> \"종아리\"\n",
            "\"저껭이 \" -> \"겨드랑이\"\n",
            "\"조금태기 \" -> \"간지롭게\"\n",
            "\"좀짐팽이 \" -> \"종아리\"\n",
            "\"허운데기 \" -> \"얼굴\"\n",
            "\"허벅다리 \" -> \"넓적다리\"\n",
            "\"놋 \" -> \"얼굴\"\n",
            "\"간수메 \" -> \"통조림\"\n",
            "\"개역 \" -> \"미숫가루\"\n",
            "\"것 \" -> \"동물먹이\"\n",
            "\"괴기 \" -> \"고기\"\n",
            "\"바당괴기 \" -> \"바닷고기\"\n",
            "\"돗괴기 \" -> \"돼지\"\n",
            "\"쇠괴기 \" -> \"쇠고기\"\n",
            "\"도괴기 \" -> \"돼지고기\"\n",
            "\"곤떡 \" -> \"쌀로만든하얀떡\"\n",
            "\"곤밥 \" -> \"흰쌀밥\"\n",
            "\"놈삐 \" -> \"무우\"\n",
            "\"대사니김치 \" -> \"마늘장아찌\"\n",
            "\"마농 \" -> \"마늘\"\n",
            "\"마농 \" -> \"마늘\"\n",
            "\"조배기 \" -> \"메밀\"\n",
            "\"촐래 \" -> \"반찬\"\n",
            "\"촘지금 \" -> \"참기름\"\n",
            "\"짐치 \" -> \"김치\"\n",
            "\"촙쏠 \" -> \"찹쌀\"\n",
            "\"조팝 \" -> \"조밥\"\n",
            "\"갈옷 \" -> \"감물들인옷\"\n",
            "\"갈 적삼 \" -> \"감물들인 적삼\"\n",
            "\"갈 중이 \" -> \"감물들인 고의\"\n",
            "\"강알터진 바지 \" -> \"개구멍 바지\"\n",
            "\"게와 \" -> \"호주머니\"\n",
            "\"단취 \" -> \"단추\"\n",
            "\"밀랑 페랭이 \" -> \"밀짚 모자\"\n",
            "\"보선 \" -> \"버선\"\n",
            "\"소중이 \" -> \"속옷\"\n",
            "\"신착 \" -> \"신짝\"\n",
            "\"찍신 \" -> \"짚신\"\n",
            "\"좀뱅이 \" -> \"잠방\"\n",
            "\"등지게 \" -> \"감물들인베옷\"\n",
            "\"고장중이 \" -> \"고쟁이\"\n",
            "\"도폭 \" -> \"우두머니\"\n",
            "\"두루막 \" -> \"두루마기\"\n",
            "\"베불레기 \" -> \"아기옷\"\n",
            "\"우장 \" -> \"비온날\"\n",
            "\"저구리 \" -> \"저고\"\n",
            "\"지성귀 \" -> \"기저귀\"\n",
            "\"지서귀 \" -> \"기저귀\"\n",
            "\"쪼께 \" -> \"조끼\"\n",
            "\"치메 \" -> \"치마\"\n",
            "\"건대 \" -> \"건대\"\n",
            "\"사모관대 \" -> \"사모관대\"\n",
            "\"시미옷 \" -> \"손자용상복\"\n",
            "\"제복 \" -> \"상제옷\"\n",
            "\"망근 \" -> \"망건\"\n",
            "\"방립 \" -> \"상갓닮은거\"\n",
            "\"벙것 \" -> \"벙거지\"\n",
            "\"상갓 \" -> \"삿갓\"\n",
            "\"탕근 \" -> \"탕건\"\n",
            "\"풍뎅이 \" -> \"남자모자\"\n",
            "\"휘양 \" -> \"여자모자\"\n",
            "\"낭저 \" -> \"나무그늘\"\n",
            "\"달리 \" -> \"땋은놈의머리\"\n",
            "\"빈네 \" -> \"비녀\"\n",
            "\"상퉁이 \" -> \"상투\"\n",
            "\"얼레기 \" -> \"머리\"\n",
            "\"얼레빗 \" -> \"머리\"\n",
            "\"쪽도리 \" -> \"족두리\"\n",
            "\"쳉빗 \" -> \"참빗\"\n",
            "\"과지 \" -> \"무당입는옷\"\n",
            "\"남신 \" -> \"나막신\"\n",
            "\"송락 \" -> \"무당들 쓰는 모자\"\n",
            "\"요령 \" -> \"방울\"\n",
            "\"복치메 \" -> \"상복치마\"\n",
            "\"구덕 \" -> \"바구니\"\n",
            "\"고량착 \" -> \"대로만든채롱\"\n",
            "\"낭푼이 \" -> \"나무그늘\"\n",
            "\"그릇대배기 \" -> \"물긷는그릇\"\n",
            "\"물구루마 \" -> \"마차\"\n",
            "\"바농 \" -> \"바늘\"\n",
            "\"박새기 \" -> \"바가지\"\n",
            "\"숟구락 \" -> \"수저\"\n",
            "\"제끄락 \" -> \"젓갈\"\n",
            "\"주멩기 \" -> \"주머니\"\n",
            "\"허벅 \" -> \"물을길어나르는통\"\n",
            "\"남죽 \" -> \"큰밥주걱\"\n",
            "\"대접 \" -> \"국사발\"\n",
            "\"도고리 \" -> \"함지박\"\n",
            "\"소리바굼지 \" -> \"도시락통\"\n",
            "\"솔박 \" -> \"바가지\"\n",
            "\"수박귀 \" -> \"밥사발\"\n",
            "\"우금 \" -> \"작은밥자\"\n",
            "\"작박 \" -> \"바가지\"\n",
            "\"장탱이 \" -> \"설거지통\"\n",
            "\"항 \" -> \"항아리\"\n",
            "\"함박 \" -> \"나무로만든도고리\"\n",
            "\"가레죽 \" -> \"삽\"\n",
            "\"곡괭이 \" -> \"곡괭이\"\n",
            "\"골갱이 \" -> \"호미\"\n",
            "\"골각지 \" -> \"호미\"\n",
            "\"군데 \" -> \"고무래\"\n",
            "\"날레군데\" -> \"날레 젓는 거\"\n",
            "\"가레 \" -> \"맷돌\"\n",
            "\"갈체 \" -> \"상태기\"\n",
            "\"돌끌 \" -> \"돌 발르는 끌\"\n",
            "\"다께 \" -> \"도리깨\"\n",
            "\"멕 \" -> \"멱서리\"\n",
            "\"말방에 \" -> \"연자마\"\n",
            "\"벤줄레 \" -> \"쐐괘\"\n",
            "\"부게 \" -> \"씨앗\"\n",
            "\"쉐스렁 \" -> \"쉐거름낼때\"\n",
            "\"씻부게 \" -> \"씨앗\"\n",
            "\"잠데 \" -> \"쟁기\"\n",
            "\"호미 \" -> \"낫\"\n",
            "\"고냥 독생이 \" -> \"굴뚝새\"\n",
            "\"놀개기 \" -> \"날개\"\n",
            "\"박생이 \" -> \"동박새\"\n",
            "\"독 \" -> \"닭\"\n",
            "\"독새기 \" -> \"닭\"\n",
            "\"고냉이 \" -> \"고양\"\n",
            "\"송애기 \" -> \"송아지\"\n",
            "\"몽생이 \" -> \"망아지\"\n",
            "\"돗 \" -> \"돼지\"\n",
            "\"도새기 \" -> \"돼지\"\n",
            "\"밭갈쇠 \" -> \"밭을 가는 소\"\n",
            "\"부랭이 \" -> \"숫소\"\n",
            "\"염송애기 \" -> \"염소\"\n",
            "\"강생이 \" -> \"강아지\"\n",
            "\"중이 \" -> \"쥐\"\n",
            "\"갓돔 \" -> \"도미\"\n",
            "\"겡이 \" -> \"게\"\n",
            "\"구젱기 \" -> \"소라\"\n",
            "\"물꾸럭 \" -> \"문어\"\n",
            "\"게염지 \" -> \"개미\"\n",
            "\"멩마구리 \" -> \"맹꽁이\"\n",
            "\"베랭이 \" -> \"버러지\"\n",
            "\"베염 \" -> \"뱀\"\n",
            "\"빙애기 \" -> \"병아리\"\n",
            "\"생이 \" -> \"새\"\n",
            "\"나람쥐 \" -> \"다람쥐\"\n",
            "\"죙이 \" -> \"쥐\"\n",
            "\"여호 \" -> \"여우\"\n",
            "\"노리 \" -> \"노루\"\n",
            "\"빙아리 \" -> \"병아리\"\n",
            "\"쉐 \" -> \"소\"\n",
            "\"쉐막 \" -> \"외양간\"\n",
            "\"도세기 \" -> \"돼지\"\n",
            "\"두테비 \" -> \"두꺼비\"\n",
            "\"골개비 \" -> \"개구리\"\n",
            "\"젱비리 \" -> \"올챙이\"\n",
            "\"송키 \" -> \"채소\"\n",
            "\"지실 \" -> \"감자\"\n",
            "\"감저 \" -> \"고구마\"\n",
            "\"어욱 \" -> \"억새\"\n",
            "\"태역 \" -> \"잔디\"\n",
            "\"모물 \" -> \"메밀\"\n",
            "\"굴묵낭 \" -> \"느티나무\"\n",
            "\"가라지 \" -> \"강아지풀\"\n",
            "\"꿩마농 \" -> \"달래\"\n",
            "\"도채비고장 \" -> \"산수국\"\n",
            "\"숙대낭 \" -> \"삼나무\"\n",
            "\"퐁낭 \" -> \"팽나무\"\n",
            "\"가베또롱호다 \" -> \"가뿐하다\"\n",
            "\"간드랑호다 \" -> \"시원하다\"\n",
            "\"간세호다 \" -> \"게으르다\"\n",
            "\"거령청호다 \" -> \"분수가없다\"\n",
            "\"곧작호다 \" -> \"곧다\"\n",
            "\"과랑과랑호다 \" -> \"쨍쨍하다\"\n",
            "\"금착호다 \" -> \"조마조마하다\"\n",
            "\"고렵다 \" -> \"가렵다\"\n",
            "\"돌코롬호다 \" -> \"달콤하다\"\n",
            "\"메뜩호다 \" -> \"통쾌하다\"\n",
            "\"멘도롱호다 \" -> \"따뜻하다\"\n",
            "\"모드락호다 \" -> \"한곳에모여있다\"\n",
            "\"몰치락호다 \" -> \"포동포동하다\"\n",
            "\"벤조롱호다 \" -> \"젊게보이다\"\n",
            "\"부치럽다 \" -> \"부끄럽다\"\n",
            "\"돌글랑호다 \" -> \"동그랗다\"\n",
            "\"봉끄랑호다 \" -> \"퉁퉁하다\"\n",
            "\"보랑지다 \" -> \"부지런하다\"\n",
            "\"펀드렁호다 \" -> \"시치미를뗀모양하다\"\n",
            "\"해또록호다 \" -> \"희끔하다\"\n",
            "\"산도록호다 \" -> \"시원하다\"\n",
            "\"썸찌근호다 \" -> \"지긋지긋하다\"\n",
            "\"실렵다 \" -> \"차겁다\"\n",
            "\"심드랑호다 \" -> \"관심이없다\"\n",
            "\"슴두렁펀펀호다 \" -> \"아무런가치없다\"\n",
            "\"솔치다 \" -> \"살찌다\"\n",
            "\"앙살호다 \" -> \"사납게소리치다\"\n",
            "\"앨록호다 \" -> \"더럽다\"\n",
            "\"어중구랑호다 \" -> \"흐리멍텅하다\"\n",
            "\"얼랍지다 \" -> \"당황하다\"\n",
            "\"얼랍지다 \" -> \"당황하다\"\n",
            "\"엄부랑호다 \" -> \"엄청나다\"\n",
            "\"오들랑호다 \" -> \"작아서위로치켜뜬모양하다\"\n",
            "\"옴탕호다 \" -> \"오목하다\"\n",
            "\"왁왁호다 \" -> \"캄캄하다\"\n",
            "\"왕왕작작호다 \" -> \"떠들썩하다\"\n",
            "\"요망지다 \" -> \"똑똑하다\"\n",
            "\"조짝호다 \" -> \"불쑥나타나있다\"\n",
            "\"좀질다 \" -> \"가늘다\"\n",
            "\"좀좀호다 \" -> \"잠잠하다\"\n",
            "\"패랍다 \" -> \"까다롭다\"\n",
            "\"그저끼 \" -> \"그저\"\n",
            "\"아척 \" -> \"아침\"\n",
            "\"그지게초냑 \" -> \"그저께\"\n",
            "\"언처냑 \" -> \"엊저녁\"\n",
            "\"요뉠모리 \" -> \"내일모레\"\n",
            "\"소시 \" -> \"요사이\"\n",
            "\"숭년 \" -> \"흉년\"\n",
            "\"저슬들언 \" -> \"겨울 된\"\n",
            "\"갈라지다 \" -> \"헤어지다\"\n",
            "\"데싸지다 \" -> \"자빠지다\"\n",
            "\"감장돌다 \" -> \"맴돌다\"\n",
            "\"거느리왕상호다 \" -> \"남의일을비평하다\"\n",
            "\"거들락거리다 \" -> \"꺼떡거리다\"\n",
            "\"거찌다 \" -> \"건드리다\"\n",
            "\"걸러지다 \" -> \"거꾸러지다\"\n",
            "\"고른배기 \" -> \"호다박자\"\n",
            "\"고쩌사다 \" -> \"비켜서다\"\n",
            "\"곤작싸다 \" -> \"재주넘다\"\n",
            "\"꼴성그리다 \" -> \"얼굴\"\n",
            "\"곱지다 \" -> \"숨기다\"\n",
            "\"구숭호다 \" -> \"불평을말하다\"\n",
            "\"꾸러박아지다 \" -> \"거꾸로박히다\"\n",
            "\"귀태우다 \" -> \"은밀히알려주다\"\n",
            "\"고끼다 \" -> \"숨막히다\"\n",
            "\"골메드리다 \" -> \"차례로바꾸다\"\n",
            "\"늘짝거리다 \" -> \"늑장부리다\"\n",
            "\"끼리다 \" -> \"미끄러지다\"\n",
            "\"다울리다 \" -> \"재촉하다\"\n",
            "\"댕기다 \" -> \"다니다\"\n",
            "\"데껴불다 \" -> \"던져버리다\"\n",
            "\"되싸지다 \" -> \"갈라지다\"\n",
            "\"들럭퀴다 \" -> \"날 뛰다\"\n",
            "\"몽캐다 \" -> \"느리다\"\n",
            "\"몽캐다 \" -> \"느리다\"\n",
            "\"베르싸다 \" -> \"속이보이게열다\"\n",
            "\"부수닥질호다 \" -> \"부수어버리다\"\n",
            "\"부에나다 \" -> \"화나다\"\n",
            "\"쎄울르다 \" -> \"고함지르다\"\n",
            "\"소곤닥호다 \" -> \"소근거리다\"\n",
            "\"소도리호다 \" -> \"소문내다\"\n",
            "\"속솜호다 \" -> \"잠잠하다\"\n",
            "\"심벡호다 \" -> \"겨루다\"\n",
            "\"악살호다 \" -> \"욕설하다\"\n",
            "\"용심내다 \" -> \"성내다\"\n",
            "\"조글리다 \" -> \"간지럽히다\"\n",
            "\"저들다 \" -> \"걱정하다\"\n",
            "\"다솟 \" -> \"다섯\"\n",
            "\"예늘곱 \" -> \"예닐곱\"\n",
            "\"요든 \" -> \"여든\"\n",
            "\"일고  \" -> \"일곱\"\n",
            "\"여답 \" -> \"여덟\"\n",
            "\"요나문 \" -> \"여나믄\"\n",
            "\"요섯 \" -> \"여섯\"\n",
            "\"호나 \" -> \"하나\"\n",
            "\"혼잠녀 \" -> \"해녀\"\n",
            "\"상군 \" -> \"물질잘하는사람\"\n",
            "\"요년난거 \" -> \"어린처녀욕\"\n",
            "\"볼망텡이 \" -> \"밤따귀\"\n",
            "\"미리내 \" -> \"은하수\"\n",
            "\"비주제 \" -> \"소나기\"\n",
            "\"상고지 \" -> \"무지개\"\n",
            "\"우네 \" -> \"안개\"\n",
            "\"노릇 \" -> \"공기\"\n",
            "\"낭강알 \" -> \"나무가늘\"\n",
            "\"돌 \" -> \"한 달\"\n",
            "\"밸 \" -> \"별\"\n",
            "\"돌생기 \" -> \"돌맹이\"\n",
            "\"작지왓 \" -> \"자갈밭\"\n",
            "\"구들 \" -> \"방\"\n",
            "\"통시 \" -> \"화장실\"\n",
            "\"벤소 \" -> \"화장실\"\n",
            "\"석 \" -> \"줄\"\n",
            "\"밭돌랭이 \" -> \"작은밭\"\n",
            "\"돌 \" -> \"한 달\"\n",
            "\"데맹이\" -> \"머리\"\n",
            "\"우뚝지\" -> \"어깨\"\n",
            "\"동무릅\" -> \"무릎\"\n",
            "\"발꼽데기\" -> \"발\"\n",
            "\"버래기\" -> \"넘치도록\"\n",
            "\"강생이\" -> \"강아지\"\n",
            "\"부각허다\" -> \"부글부글하다\"\n",
            "\"강알\" -> \"가랭이\"\n",
            "\"부끄다\" -> \"부풀어 오르다\"\n",
            "\"개끔\" -> \"거품\"\n",
            "\"분시몰랑\" -> \"정황도 모르고\"\n",
            "\"개작개작\" -> \"밥을 추하게 먹는 모습\"\n",
            "\"삐암데기\" -> \"뺨\"\n",
            "\"검질\" -> \"잡초\"\n",
            "\"속슴허라\" -> \"말하지말라\"\n",
            "\"게미융허다\" -> \"희미하다\"\n",
            "\"솜쫄르멍\" -> \"숨말힐듯한 상황 참으면서\"\n",
            "\"게작헌\" -> \"입이 큰 모양\"\n",
            "\"쉰달이\" -> \"쉰 밥으로만든 유산식품\"\n",
            "\"고라불켜\" -> \"고자질한다\"\n",
            "\"심토맥이\" -> \"마음 씀씀이\"\n",
            "\"곡기다\" -> \"숨막히다\"\n",
            "\"영\" -> \"이렇게\"\n",
            "\"골다\" -> \"잘게부수다\"\n",
            "\"골다\" -> \"잘게부수다\"\n",
            "\"왁왁허다\" -> \"캄캄하다\"\n",
            "\"곱지다\" -> \"숨기다\"\n",
            "\"요망지다\" -> \"똑똑하다\"\n",
            "\"과랑과랑\" -> \"햇살이 눈부시게 비추는모습\"\n",
            "\"우영밭\" -> \"텃밭\"\n",
            "\"괸당\" -> \"친족\"\n",
            "\"웃뜨리\" -> \"산간마을\"\n",
            "\"굽\" -> \"밑바닥\"\n",
            "\"재짝재짝\" -> \"걷는 모습\"\n",
            "\"기시리다\" -> \"초벌로 불에 태우다\"\n",
            "\"제라헌\" -> \"제대로 된\"\n",
            "\"꽝\" -> \"뼈\"\n",
            "\"조그물다\" -> \"어떤 결심을 하기위한 다짐\"\n",
            "\"눈물촐래\" -> \"눈물의 반찬\"\n",
            "\"주레사니\" -> \"보리피리\"\n",
            "\"니치름\" -> \"침\"\n",
            "\"촘아가라\" -> \"어이없다\"\n",
            "\"데껴불켜\" -> \"던져버린다\"\n",
            "\"칭원허다\" -> \"원통하다\"\n",
            "\"두렁청허다\" -> \"얼떨떨하다\"\n",
            "\"콥데사니\" -> \"마늘\"\n",
            "\"들러켬쩌\" -> \"날뛰다\"\n",
            "\"코시롱허다\" -> \"향기가 좋다\"\n",
            "\"듬삭하다\" -> \"기름기가 넉넉하다\"\n",
            "\"탈\" -> \"딸기\"\n",
            "\"멜라지켜\" -> \"찌그러지다\"\n",
            "\"통시\" -> \"돼지우리가 있는 변소\"\n",
            "\"메기독딱\" -> \"아무것도 없음\"\n",
            "\"트멍\" -> \"틈새\"\n",
            "\"모소완\" -> \"무서워\"\n",
            "\"하근디\" -> \"이곳저곳\"\n",
            "\"몬뜨글락\" -> \"홀랑벗은모습\"\n",
            "\"하다\" -> \"많다\"\n",
            "\"몬직당\" -> \"만지다가\"\n",
            "\"행글랑이\" -> \"힘없이 쓰러진 모양\"\n",
            "\"몬짝\" -> \"전부\"\n",
            "\"허당말타\" -> \"하다가 안 할 것이냐\"\n",
            "\"몰명허다\" -> \"멍청하다\"\n",
            "\"헌저글라\" -> \"빨리가자\"\n",
            "\"몽케다\" -> \"늦장부리다\"\n",
            "\"호썰\" -> \"조금\"\n",
            "\"무리다\" -> \"맛이가다\"\n",
            "\"호야\" -> \"등\"\n",
            "\"무사\" -> \"왜?\"\n",
            "\"흐랑허다\" -> \"축 늘어지다\"\n",
            "\"배롱허다\" -> \"어둠속에 빛이 희미하다\"\n",
            "\"허운데기\" -> \"얼굴\"\n",
            "\"배지금허다\" -> \"고소하다\"\n",
            "\"쪼광\" -> \"모습\"\n",
            "\"물옷\" -> \"잠수복\"\n",
            "\"센 바당\" -> \"거칠고 노한 바다\"\n",
            "\"앞바르\" -> \"가까운 바다\"\n",
            "\"난바르\" -> \"먼 바다\"\n",
            "\"산목 졸르면 비온다\" -> \"한라산의 중턱에 띠같은 구름이 끼면 비가 온다\"\n",
            "\"돌 갓쓰민 우친다\" -> \"달무리지면 비가 온다\"\n",
            "\"곰새기 들럭 켬져\" -> \"돌고래가 날뛴다\"\n",
            "\"토끼썼다\" -> \"황사현상\"\n",
            "\"큰눈\" -> \"물안경\"\n",
            "\"소살\" -> \"작살\"\n",
            "\"머정좋다\" -> \"해산물을 많이 잡았다\"\n",
            "\"제수가 좋다\" -> \"해산물을 많이 잡았다\"\n",
            "\"궐 면했다\" -> \"겨우 한두개 잡았다\"\n",
            "\"마당 바랏져\" -> \"바다가 잔잔하다\"\n",
            "\"큰누\" -> \"산더미 같은 파도\"\n",
            "\"작은누\" -> \"작은 파도\"\n",
            "\"절 치대긴다\" -> \"물결이 친다\"\n",
            "\"고찌 글라 고찌 가게 \" -> \"같이 가요 함께 해요\"\n",
            "\"느영 고찌 글민 지꺼짐이 열배여\" -> \"너와 함께 하면 즐거움이 열배야\"\n",
            "\"고찌 글라 고찌 가게 \" -> \"같이 가요 함께 해요\"\n",
            "\"느영 고찌 글민 지꺼짐이 백배여\" -> \"너와 함께 하면 즐거움이 열배야\"\n",
            "\"영도 곱닥헌 날 공기 좋고 사람 좋고\" -> \"이렇게 예쁜 날 공기 좋고 사람 좋고\"\n",
            "\"느영 나영 고찌 글민 무신 걱정시냐 \" -> \"너와 함께 하니 무슨 걱정 있으랴\"\n",
            "\"하영 골민 존다니\" -> \"많이 말하면 잔소리야\"\n",
            "\"먹당 보난 살아 지곡 살당 보난 먹어 져라\" -> \"먹다 보니 살게 되고 살다 보니 먹게 되지\"\n",
            "\"굶엉 보난 알아 져라 배고픔도 인생인 걸\" -> \"굶어 보면 알게 되지 배고픔도 인생인 걸\"\n",
            "\"어시민 어신양 그저 겅 살곡\" -> \"없으면 없는대로 그저 그렇게 살고\"\n",
            "\"이시민 이신양 나누멍 살주\" -> \"있으면 있는대로 나누면서 살지\"\n",
            "\"먹엄직이 살암직이 시상\" -> \"먹어볼만한 살아볼만한 세상\"\n",
            "\"우시멍 사는 시상이 사름 사는 시상이주\" -> \"웃으며 사는 세상이 사람 사는 세상이지\"\n",
            "\"울당 보난 살아 져라 \" -> \"울다 보니 살게 되었지\"\n",
            "\"웃당 보난 살아 져라\" -> \"웃다 보니 살게 되었지\"\n",
            "\"우는 삶도 웃는 삶도 고마운 인생이주\" -> \"우는 삶도 웃는 삶도 고마운 인생이지\"\n",
            "\"어시민 어신양 그저 겅 살곡\" -> \"없으면 없는대로 그저 그렇게 살고\"\n",
            "\"이시민 이신양 나누멍 살주\" -> \"있으면 있는대로 나누면서 살지\"\n",
            "\"먹엄직이 살암직이 시상\" -> \"먹어볼만한 살아볼만한 세상\"\n",
            "\"우시멍 사는 시상이 사름 사는 시상이주\" -> \"웃으며 사는 세상이 사람 사는 세상이지\"\n",
            "\"무명천 할머니 어디 감수과\" -> \"무명천 할머니 어디 가십니까\"\n",
            "\"무명천 할머니 펜안 허우꽈\" -> \"무명천 할머니 편안 하십니까\"\n",
            "\"4. 3 소건때 총 맞앙 턱을 잃엉 경 살았주\" -> \"4. 3 사건때 총 맞아서 턱을 잃었네 그렇게 살았네\"\n",
            "\"55년 간 말도 못허곡 음식도 못 먹으멍\" -> \"55년 간 말도 못하고 음식도 못 먹으면서\"\n",
            "\"상처 난 얼굴 보일 수 어성 무명천을 둘렁\" -> \"상처 난 얼굴 보일 수 없어 무명천을 둘렀네\"\n",
            "\"목심은 건졌주만 살아도 산게 아니랐주\" -> \"목숨은 건졌지만 살아도 산게 아니었네\"\n",
            "\"무명천 할망 이랜 날 불렀주만 \" -> \"무명천 할머니 라고 날 불렀지만\"\n",
            "\"나신디도 이름이 이서났주 \" -> \"내게도 이름이 있었네\"\n",
            "\"진아영 이우다\" -> \"진아영 입니다\"\n",
            "\"놈삐 좀질게 썰엉 솖앙\" -> \"무채 썰어 끓는물에 데쳐\"\n",
            "\"패마농이영 꿰고루 놩 섞엉\" -> \"파와 깨가루 넣어 섞어서\"\n",
            "\"모멀고루 풀엉 얄룹게 지졍\" -> \"메밀가루 반죽하여 얇게 지져\"\n",
            "\"그 우터레 놩 몰민\" -> \"그 위에 놓고 말면\"\n",
            "\"빙떡 이주게\" -> \"빙떡 이지\"\n",
            "\"두렁청이 어드레 가젠 햄시냐 \" -> \"앞뒤분간못하고 어디로 가려고 하니\"\n",
            "\"곱들락호게 촐려입어그네 이드레 와그네\" -> \"예쁘게 차려입고 이쪽으로 와서는\"\n",
            "\"느영나영 모다들어그네 터졍 도르게\" -> \"너와내가 한데모여서 힘껏 달리자\"\n",
            "\"빙삭허게 몬딱 베리난 보뎌감시녜\" -> \"수줍게웃으며 모두 쳐다보니 정다워지네\"\n",
            "\"도르라 조들지 마랑 도르라 몬딱 도르라\" -> \"달려라 걱정하지 말고 달려라 모두 달려라\"\n",
            "\"도드라 조들지 마랑 도르라 몬딱 도르라\" -> \"달려라 걱정하지 말고 달려라 모두  달려라\"\n",
            "\"용심 내멍 도톼 보난 알아졈시냐\" -> \"화를 내며 다퉈 보니까 알겠니?\"\n",
            "\"느영나영 심벡 해봐사 페들락 햄시녜\" -> \"너와내가 경쟁해 봐야 감정상하기만할 뿐이네\"\n",
            "\"돈 한 추룩 뺄라진 추룩 오시록 헌 추룩\" -> \"돈이 많은 척 잘난 척 점 잖은 척\"\n",
            "\"경 해 봐사 핏짝 허난 얼렁 뎅기게\" -> \"그렇게 해 봐야 삐칠 뿐이니 달어 돌아다니자\"\n",
            "\"도르라 조들지 마랑 도르라 몬딱 도르라\" -> \"달려라 걱정하지 말고 달려라 모두 달려라\"\n",
            "\"도드라 조들지 마랑 도르라 몬딱 도르라\" -> \"달려라 걱정하지 말고 달려라 모두  달려라\"\n",
            "\"아야 머리여 아야 머리여\" -> \"아야 머리야 아야 머리야\"\n",
            "\"입담허멍 두드렴시민\" -> \"입담하며 두들기노라면\"\n",
            "\"수꾸락 조루 거수루 심엉\" -> \"숟가락 자루 거꾸로 잡고\"\n",
            "\"차반이 바위 두드렴시민\" -> \"채롱 옆을 두들기노라면\"\n",
            "\"훌구룽 몰케염지덜\" -> \"굵다란 말개미들이\"\n",
            "\"고루삭삭 삐어지국\" -> \"사방으로 흩어지고\"\n",
            "\"어떵허코 어떵허코\" -> \"어떻게하지 어떻게하지\"\n",
            "\"핵교 가당 오줌 싸비언\" -> \"학교 가는길에 오줌을 쌌어\"\n",
            "\"어떵허코 어떵허코\" -> \"어떻게하지 어떻게하지\"\n",
            "\"핵교 가당 오줌 싸비언\" -> \"학교 가는길에 오줌을 쌌어\"\n",
            "\"어떵허코\" -> \"어떻게하지\"\n",
            "\"핵교 가당 오줌 싸비언\" -> \"학교 가는길에 오줌을 쌌어\"\n",
            "\"어떵 허코 어떵 허코\" -> \"어떻게 하지 어떻게 하지\"\n",
            "\"촘당 촘당 촘지 못행 나와부런게\" -> \"참다가 참다가 참지 못해 나와버렸어\"\n",
            "\"어떵 허코 어떵 허코\" -> \"어떻게 하지 어떻게 하지\"\n",
            "\"비가 왐시난 비 맞았덴 골라게\" -> \"비가 내리니 비 맞았다고 말하렴.\"\n",
            "\"나오는 걸 어떵 헐 수 이시냐\" -> \"나오는 걸 어떻게 할 수 있겠니\"\n",
            "\"어떵 허코 어떵 허코\" -> \"어떻게 하지 어떻게 하지\"\n",
            "\"겡이 잡당 맨질락허연\" -> \"게를 잡다가 미끄러졌어\"\n",
            "\"어떵 허코 어떵 허코\" -> \"어떻게 하지 어떻게 하지\"\n",
            "\"겡이 잡당 맨질락허연\" -> \"게를 잡다가 미끄러졌어\"\n",
            "\"어떵 허코\" -> \"어떻게 하지\"\n",
            "\"겡이 잡당 멘질락허연\" -> \"게를 잡다가 미끄러졌어\"\n",
            "\"어떵허당 어떵허당\" -> \"어쩌다가 어쩌다가\"\n",
            "\"곱은 겡이 지달리당 와락 덮쳤주\" -> \"숨어있는 게를 기다리다가 갑자기 덮쳤지\"\n",
            "\"경해그네 어떵 되연\" -> \"어떻게 되었어\"\n",
            "\"완존 뽈라라\" -> \"정말 빨랐어\"\n",
            "\"눈 감았당 터 보난\" -> \"눈 감았다 떠 보니\"\n",
            "\"물 쏘곱이 나만 빠졍 이서라\" -> \"물 속에 나만 빠져 있었어\"\n",
            "\"어떵허코 어떵허코 어떵허코 어떵허코\" -> \"어떡하지 어떡하지 어떡하지 어떡하지\"\n",
            "\"바당 바당 바당 바당 보름 보름 보름 보름\" -> \"바다 바다 바다 바다 바람 바람 바람 바람\"\n",
            "\"바당 바당 바당 바당 보름 보름 보름\" -> \"바다 바다 바다 바다 바람 바람 바람 바람\"\n",
            "\"날 막 좋앙 바당드레 강 모살밧듸 맨발로 걸엄쩌\" -> \"날이 정말 좋아 바다에 가서 모래밭을 맨발로 걸었네\"\n",
            "\"겡이 호나 나를 뵈련게 절지치는 디로 도람쩌\" -> \"게 한 마리가 나를 보더니 파도치는 쪽으로 도망가네\"\n",
            "\"어떵허코 잡아 보카 잡당 실수허민 아야 물릴건디\" -> \"어쩌지 잡아 볼까 잡다 실수하면 아야 물릴 텐데.\"\n",
            "\"바당 바당 바당 바당 보름 보름 보름 보름\" -> \"바다 바다 바다 바다 바람 바람 바람 바람\"\n",
            "\"바당 바당 제주 바당 보름 보름 보름\" -> \"바다 바다 바다 바다 바람 바람 바람 바람\"\n",
            "\"생이 생이 생이 생이 오름 오름 오름 오름\" -> \"새 새 새 새 오름 오름 오름\"\n",
            "\"생이 생이 생이 생이 오름 오름 오름\" -> \"새 새 새 새 오름 오름 오름\"\n",
            "\"어멍이영 손 꼭 심엉 용눈이 오름 올람쩌\" -> \"엄마랑 손 꼭 잡고 용눈이 오름을 오르네\"\n",
            "\"생이 호나 나를 뵈련게 짹짹짹짹 소릴 호염쩌\" -> \"새 한 마리 나를 보더니 짹짹짹짹 노래를 하네\"\n",
            "\"어떵허코 잡아 보카 아맹 잡젠 해도 택도 어실건디\" -> \"어쩌지 잡아 볼까 아무리 잡으려 해도 어림없을 텐데\"\n",
            "\"이녁 가슴 쏘곱엔\" -> \"당신의 마음 속엔\"\n",
            "\"고은 꽃덜이 만발혼 생이라\" -> \"고운 꽃들이 만발한 모양이야\"\n",
            "\"영 골아도 빙세기 웃곡\" -> \"이렇게 말해도 빙그레 웃고\"\n",
            "\"정 골아도 빙세기 웃곡\" -> \"저렇게 말해도 빙그레 웃고\"\n",
            "\"이녁 가슴  쏘곱엔\" -> \"당신의 마음 속엔\"\n",
            "\"황소 혼마리 들어앉은 생이라\" -> \"황소 한 마리 들어앉은 모양이야\"\n",
            "\"영 골아도 속솜\" -> \"이렇게 말해도 잠잠\"\n",
            "\"정 골아도 속솜\" -> \"저렇게 말해도 잠잠\"\n",
            "\"이녁 가슴  쏘곱에 들어가 보젠\" -> \"당신의 마음 속에 들어가 보려고\"\n",
            "\"욕심이영 미움이영 \" -> \"욕심과 미움이\"\n",
            "\"손  불끈 잡아둠서\" -> \"손  불끈 잡아두고\"\n",
            "\"기웃 기웃 거렴신디\" -> \"기웃 기웃 거려보는데\"\n",
            "\"꿀벌이영 일벌이영 \" -> \"꿀벌과 일벌이\"\n",
            "\"날아 댕기는걸 보난\" -> \"날아다니는 걸 보니\"\n",
            "\"이녁 가슴 쏘곱엔 \" -> \"당신의 마음 속엔\"\n",
            "\"송이꿀보다도 더 단\" -> \"송이꿀보다도 더 달디단\"\n",
            "\"무언가로 솜 빡혼  생이라\" -> \"무언가로 꽉 들어찬  모양이야\"\n",
            "\"휘이이\" -> \"휘이이\"\n",
            "\"보름 분다\" -> \"바람 분다\"\n",
            "\"보름 불어\" -> \"바람 불어\"\n",
            "\"저 바당이\" -> \"저 바다가\"\n",
            "\"저 보름이\" -> \"저 바람이\"\n",
            "\"손짓햄쩌\" -> \"손짓하네\"\n",
            "\"말 골암쩌\" -> \"말을 하네\"\n",
            "\"테왁 메영 골갱이 들렁\" -> \"테왁 메고 호미를 들어\"\n",
            "\"바당이영 춤을 추엄쩌\" -> \"바다와 춤을 추네\"\n",
            "\"우리 어멍 우리 바당\" -> \"우리 엄마 우리 바다\"\n",
            "\"혼몸 되엉 춤을 추엄쩌\" -> \"한 몸 되어 춤을 추네\"\n",
            "\"바당아 보름아\" -> \"바다야 바람아\"\n",
            "\"눈물 남쩌 눈물 남서\" -> \"눈물 나네 눈물이 나\"\n",
            "\"우리 어멍 지켜도라\" -> \"우리 엄마 지켜다오\"\n",
            "\"이어도 이어도 사나\" -> \"이어도 이어도 사나\"\n",
            "\"이어도 이어도 사나\" -> \"이어도 이어도 사나\"\n",
            "\"이어도 이어도 사나\" -> \"이어도 이어도 사나\"\n",
            "\"이어도 이어도 사나\" -> \"이어도 이어도 사나\"\n",
            "\"보름은 팡팡 불곡\" -> \"바람은 세차게 불고\"\n",
            "\"덜 요문 조코 고리\" -> \"덜 익은 조 열매가\"\n",
            "\"이레 배령 인사허곡\" -> \"이쪽을 보면서 인사하고\"\n",
            "\"저레 배령 인사허곡\" -> \"저쪽을 보면서 인사하고\"\n",
            "\"낭가쟁이덜은 서로 심벡허멍 좁아댕기곡\" -> \"나뭇가지들은 서로 다투며 잡아당기고\"\n",
            "\"절지치는 요메기 바당엔 빈 구젱기 껍데기 둥글어 댕기곡\" -> \"파도치는 요메기 바다엔 빈 소라가 뒹굴어 다니고\"\n",
            "\"담우이 걸쳐진 감저줄에\" -> \"담위를 걸쳐친 고구마줄기에\"\n",
            "\"동글동글 도라진 감저동가리 \" -> \"동글동글 매달려있는 작은고구마\"\n",
            "\"이래 도라앉작 저래 도라앉작\" -> \"이리 흔들리고 저리 흔들리고\"\n",
            "\"워어허허 보름 보름 보름 부는 제주의 하늘아\" -> \"워어허허 바람 바람 바람 부는 제주의 하늘아\"\n",
            "\"먹쿨낭 꼭대기에 앚인 먹쿨 생이 안 털어져보잰 낭가쟁이 꽉심엉 있곡\" -> \"먹구슬나무 꼭대기에 앉은 먹구슬 새 안 떨어지려고 나뭇가지를 꽉잡고 있고\"\n",
            "\"조밭 볼리는 몽생이 조름에 일어나는 흙구듬은 노미 밧더레 넘어가곡\" -> \"조밭 밟는 망아지 뒤로 일어나는 흙먼지는 남의 밭으로 넘어가고\"\n",
            "\"동네 큰 폭낭 아래 검불령 가젠허민\" -> \"동네 큰 팽나무 아래 땀좀식히고 가려고하면\"\n",
            "\"불어오는 저 보름은 무사 영 건드러운지\" -> \"불어오는 저 바람은 왜 그리 간질이는지\"\n",
            "\"기여 혼저 가라\" -> \"그래 어서 가렴\"\n",
            "\"기여 혼저 가라\" -> \"그래 어서 가렴\"\n",
            "\"보름아\" -> \"바람아\"\n",
            "\"해녀 삼춘 또시 또시 물에 감쩌\" -> \"해녀 삼촌 다시 다시 바다 가네\"\n",
            "\"테왁 메영 골갱이 들렁 좀수복 입엉 뒤뚱뒤뚱\" -> \"테왁 메고 호미 들고 잠수복 입고 뒤뚱뒤뚱\"\n",
            "\"절이치는 바당더레 강 보난\" -> \"파도치는 바다로 가 보니\"\n",
            "\"테왁덜이 동동동동 춤을 췀쩌\" -> \"테왁들이 동동동동 춤을 추네\"\n",
            "\"해녀 삼춘덜 또시 물에들었쩌\" -> \"해녀 삼촌들 다시 물속으로\"\n",
            "\"숨비소리 쏟아가멍 또시들멘\" -> \"숨비소리 쏟아가며 물속으로\"\n",
            "\"궤기추룩 홀랑홀랑 꼬릴 췀쩌\" -> \"고기처럼 홀랑홀랑 꼬릴 치네\"\n",
            "\"나도 확 벗엉 물쏘곱이 들어가보카\" -> \"나도 확 벗고 물속으로 뛰어들까\"\n",
            "\"테왁 들렁 이짝드레 온거 보난\" -> \"테왁 들고 이쪽으로 온거 보니\"\n",
            "\"성게도 지깍 구젱기영 전복이영\" -> \"성게도 가득 전복이랑 소라랑\"\n",
            "\"아고 이건 뭐라 물꾸럭도 잡앙 와신게\" -> \"아고 이건 뭐지 문어도 잡아 왔네요\"\n",
            "\"아고 삼촌 물꾸럭 나 얼마마씸?\" -> \"아고 삼촌 문어 한마리 얼마죠?\"\n",
            "\"아고 조케야 느랑 기냥 거정가불라\" -> \"아고 조카야 너는 그냥 가져가라\"\n",
            "\"에고 무신 소리꽈 게민 조냑에랑 당근파당 거정안내주마씀\" -> \"에고 무슨 말씀을 그럼 저녁에 당근파다 드릴게요\"\n",
            "\"물빡으로 물먹으민 시염 안돋나. \" -> \"물바가지로 물 마시면 수염 안난다.\"\n",
            "\"물애기 옷을 막개로 뽈민 애기놀랜다. \" -> \"갓난아이 옷을 방망이로 빨면 아기가 놀랜다.\"\n",
            "\"밤에 얼래기질 호민 모솜탄다. \" -> \"밤에 머리 빗으면 무서움 탄다.\"\n",
            "\"비는 놈안틴 지여사 혼다. \" -> \"비는 자에게는 용서해 주어야 한다.\"\n",
            "\"비 온날 쇠총지 이레 착 저레 착 혼다. \" -> \"비오는 날 쇠꼬리 이리 착 저리 착 흔든다.\"\n",
            "\"빈 고래질 호민 숭년 든다. \" -> \"빈 맷돌질 하면 흉년 든다.\"\n",
            "\"가난한 사람 못할 일 읏나\" -> \"가난한 사람 못할 일 없다.\"\n",
            "\"까메기 까옥 호민 촘새도 조조조 혼다\" -> \"까마귀가 까옥하면 참새도 조조 한다. 즉, 자기 자신을 알고 처신하라.\"\n",
            "\"까메기 똥 케우리듯\" -> \"까마귀 똥 헤집듯\"\n",
            "\"까메기 모른 식게\" -> \"까마귀 모른 제사\"\n",
            "\"까메기 알라구리 털어질 소리\" -> \"까마귀 아래턱 떨어질 소리\"\n",
            "\"가시어멍 눈 멜를 사위\" -> \"장모 눈 망가뜨릴 사위\"\n",
            "\"가시어멍 장 읏인 깐에, 사위 국 실픈 깐에\" -> \"장모 된장 없는 터에, 사위 국 먹기 싫은 터에\"\n",
            "\"각시 아꼬우민 처갯 칩 몰팡 돌에 절혼다.\" -> \"아내 아까우면 처가 집에 절한다.\"\n",
            "\"각시 읏인 건 안 섭섭해도 남통머리 잃른 건 섭섭호다.\" -> \"아내가 없는 것은 안 섭섭해도 담배통 잃는 건 더 섭섭하다.\"\n",
            "\"각시 읏인 처갯칩 가나 마나 \" -> \"아내 없는 처가집 가나 마나\"\n",
            "\"갈치가 갈치 꼴랭이 그차 먹나\" -> \"갈치가 갈치 꼬리 끓어 먹는다.\"\n",
            "\"갭인년 숭년에도 먹당 남은 게 물이여\" -> \"갑이년 흉년 도 갑니다.\"\n",
            "\"갓쟁이 헌 갓 씨곡, 심방 놈 빌엉 굿 허곡\" -> \"갓장사 헌갓 쓰고, 무당 남 빌어 굿하고\"\n",
            "\"강생이 똥은 똥 아니가\" -> \"강아지 똥은 똥이 아닌가\"\n",
            "\"개가 똥을 촘주\" -> \"개가 똥을 참지\"\n",
            "\"개도 비치락으로 안 또린다.\" -> \"개도 빗자루로 안 때린다.\"\n",
            "\"개똥을 모소왕 피 호느냐, 더러웡 피호주\" -> \"개똥을 무서워서 피하느냐, 더러워서 피하지\"\n",
            "\"개똥도 약에 쓰젠 허민 귀혼다\" -> \"하찮은 개 똥도 약으로 쓸려면 귀하다.\"\n",
            "\"건지 먹은 놈이나, 국물 먹은 놈이나 \" -> \"건더기 먹은 놈이나, 국물 먹은 놈이나 매 한가지\"\n",
            "\"검은 독도 흰 독새기 난다\" -> \"검은 닭도 흰 달걀 낳는 수 있다.\"\n",
            "\"것 구숭 호로 조식, 글 구숭 양반 조식\" -> \"음식 타박 쌍놈, 글 구숭 양반자식\"\n",
            "\"것 멋을 땐 개도 안 또린다\" -> \"음 식을 먹을 때는 개도 안 때린다.\"\n",
            "\"것 박접허면 죄 짓나\" -> \"음식을 박대하면 죄 짓는다.\"\n",
            "\"게드레기도 집이 싯나\" -> \"소라, 게도 집이 있다.\"\n",
            "\"곤 사름은 맥을 씌어도 곱나\" -> \"고운 사람은 멱서리 써도 곱다.\"\n",
            "\"곤 년 잡아 들이렌 허난 솔친년 잡아 들인다\" -> \"고운 여자 잡아 들이라고 하니 살찐 여자 잡아 들인다.\"\n",
            "\"곧은 낭은 가운디 산다\" -> \"곧은 나무는 한가운데 선다.\"\n",
            "\"골체 부지런은 하늘도 못막나\" -> \"삼태기 부지런으로 부자 되는 것은 하늘도 못 막는다.\"\n",
            "\"공께엥 허민 눈도 뻘겅, 코도 벌겅\" -> \"공짜하면 눈도 벌겋고, 코도 벌겋고\"\n",
            "\"공껀 씨여도 돈다.\" -> \"공짜는 맛이 써도 달다\"\n",
            "\"공부허렌 허난 개잡는걸 뱁나\" -> \"공부하라고 하니 개잡는걸 배운다.\"\n",
            "\"고튼 품이민 홀어멍 칩 머슴산다.\" -> \"같은 값이면 과부집 머슴을 산다.\"\n",
            "\"구젱기 똥누레 가불민 게드레기가 기어 든다.\" -> \"소라가 똥누러 가버리면 소라게가 차지 한다.\"\n",
            "\"국 하영 먹으민 가시어멍 눈 멜라진다.\" -> \"국 많이 먹으면 장모 눈 망가진다.\"\n",
            "\"굼벵이도 꿈불 재주 있나\" -> \"하찮은 굼벵이라도 구부리는 재주가 있다.\"\n",
            "\"귀막앙 삼년, 눈 어둑엉 삼년, 말몰랑 삼년 살암서사 가렝 오렝 말 읏나\" -> \"시집가서 귀머거리 삼년, 장님 삼년, 벙어리 삼년을 살아야 가라 오라는 말이 없다.\"\n",
            "\"귀신 대접 잘 호영 그른디 읏나\" -> \"귀신 대접 잘 해두면 잘못 될 일 없다\"\n",
            "\"귀소문 말앙 눈소문 호라\" -> \"귀소문 하지 말고 눈소문 하라, 실제로 보고 확인한 것 아니면 말하지 말라.\"\n",
            "\"귀신도 빌민 듣나\" -> \"귀신도 사정하면 들어 준다.\"\n",
            "\"괸당은 옷 우이 보름\" -> \"일가는 옷위의 바람\"\n",
            "\"그 날 액은 망데기 안에 아자도 깬다\" -> \"그 날 액은 독안에 앉아도 깬다.\"\n",
            "\"그짓말도 외삼춘 보다 낫나\" -> \"거짓말도 외삼촌 보다도 낫다.\"\n",
            "\"글 못허는 놈 붓 골린다.\" -> \"글 잘 못하는 놈 붓 고른다. = 붓타령 한다.\"\n",
            "\"글 배우렌 호난 홀어멍칩 강생일 또린다. \" -> \"글 배우라고 했더니 엉뚱하게 과부집 강아지 때린다.\"\n",
            "\"기도 못호는게 놀젱 혼다\" -> \"기어 다니지도 못하는게 나려고 한다.\"\n",
            "\"깅이광 보말도 지집 싯나\" -> \"게와 고둥도 자기 집은 있다.\"\n",
            "\"기시린 도새기 도라멘 도새기 타령혼다.\" -> \"그을린 돼지가 달아멘 돼지를 나무란다.\"\n",
            "\"골체 부지런은 하늘도 못막나\" -> \"삼태기 부지런으로 부자 되는 것은 하늘도 못 막는다.\"\n",
            "\"고튼 품이민 홀 어멍칩 머슴산다.\" -> \"같은 값이면 과부집 머슴을 산다.\"\n",
            "\"고늘이 먹곡 고늘이 쓰라\" -> \"가늘게 먹고 가늘게 쓰라\"\n",
            "\"관덕정이 설렁탕도 먹어난 놈이 먹나\" -> \"관덕정 설렁탕도 먹어본 놈이 먹는다.\"\n",
            "\"끅 걷으레 간 놈이 정당 벌립 망 돌른다.\" -> \"칡 걷으러 간 놈이 정당벌레 테 엮는다.\"\n",
            "\"나간 놈 찍신 셔도 자는 놈 찍신 읏나\" -> \"나간 놈 몫은 있어도 잠자는 놈 몫은 없다.\"\n",
            "\"나댕기는 개가 꽝 물어 온다. \" -> \"나  다니는 개가 뼈 물어 온다.\"\n",
            "\"나 땅 까메긴 검어도 아깝나\" -> \"나의 땅에 있는 까마귀는 검어도 아깝다.\"\n",
            "\"나서 화목은 남조가 허곡 들어 화목은 여조가 혼다.\" -> \"나가서 화목은 남자가 하고, 들어서 화목은 여자가 한다.\"\n",
            "\"나 한 아재비 지라.\" -> \"나이  많은 사람이 양보하라.\"\n",
            "\"남조가 디딘 풀은 유울곡, 여조가 디딘 풀은 안 유운다.\" -> \"남자가 밟은 풀은 이울고, 여자가 밟은 풀은 안 이운다.\"\n",
            "\"낭도 늙엉 고목되민 놀단 생이도 아니 온다.\" -> \"나무도 늙어 고목되면 새들도 아니 온다.\"\n",
            "\"낭이 좀 먹주 세월 좀 안먹나\" -> \"나무가 좀먹지 세월은 좀 안먹는다. =  느긋한 시간 관념을 부추길 때\"\n",
            "\"나 똘이 고와사 사윌 골른다.\" -> \"내 딸이 고와야 잘난 사위를 고른다.\"\n",
            "\"너미 골리당 눈 까진 사윌 골린다.\" -> \"너무 고르다가 눈이 멜라진 사위를 고른다.\"\n",
            "\"노름쟁이 뒤랑 대곡, 먹는 놈 뒤랑 대지말라.\" -> \"노름꾼 뒤는 대어도 , 먹는 놈 뒤는 대지 말라\"\n",
            "\"노름은 신 신을 때 봐사 안다.\" -> \"노름은 끝나서 신발 신을 때 보아야 승부를 안다.\"\n",
            "\"노름쟁인 망호여도 흥호진 못혼다.\" -> \"노름 쟁이는 망하여도 흥하지는 못한다.\"\n",
            "\"노리 또린 막땡이 3년 우려 먹나\" -> \"노루 때린 막대기 3년을 울겨 먹는다.\"\n",
            "\"노리궤기 혼점 먹젠 호당 지 궤기 열 점 일른다.노루고기 한 점 얻어 먹으려다가 자기의 고기 열점을 잃은다.\" -> \"\"\n",
            "\"놈 곱진 것 쉐도 못 찾나\" -> \"남이 숨긴건 소도 못 찾는다.\"\n",
            "\"놈광 심백은 호곡, 게심이랑 허지 말라.\" -> \"남과 경쟁은 하되 시기는 하지 마라.\"\n",
            "\"놈 싼 훼에 깅이잡기.\" -> \"남이 켠 횃불에 게 잡는다.\"\n",
            "\"놈이 눈에 피 내우젠 허민 이녁 눈엔 고름 나사 혼다.\" -> \"남의 눈에 피 흘리게 하면, 자기 눈에는 고름이 나와야 한다.\"\n",
            "\"놈이 쉐 들럭키는 건 보기 좋나.\" -> \"남의 소 날뛰는 건 보기 좋다.\"\n",
            "\"놈이 숭 털민 이녁 숭 된다.\" -> \"남의 흉내 내면 자기 흉 된다.\"\n",
            "\"놈이 집광 관장살인 궤던 밥도 내부러동 돋나.\" -> \"남의 집과 벼슬 살이는 끓는 밥도 두고 간다.\"\n",
            "\"놋 시칠 때에 물 하영 쓰민 죽엉 가민 다 먹어사 혼다.\" -> \"세수 할 때에 물을 많이 사용하면 죽어서 가면 다 마셔야 한다.\"\n",
            "\"눈 까진 똘 고졍 사윌 고른다.\" -> \"애꾸눈 딸 가지고 사위를 고른다.\"\n",
            "\"눈 낭에 요름 요는냐?\" -> \"누운 나무에 열매 열리는가?\"\n",
            "\"눈 썹에 불 붙어도 끌 조를이 읏나.\" -> \"눈썹에 불이 붙어도 끌 겨를이 없다.\"\n",
            "\"늘근 쉐 콩 주엉 말덴 호느냐?\" -> \"늙은 소 콩을 주면 안 받겠다고 하느냐?\"\n",
            "\"늑신네가 젊은 첩 호민 불 본 나비 놉뜨듯 혼다.\" -> \"늙은 놈이 젊은 첩 하면 불 본 나비 날뛰듯 한다.\"\n",
            "\"니 아픈 부릅씨랑 허곡, 눈 아픈 부릅씨랑 말라.\" -> \"이 아픈 심부름은 하고, 눈 아픈 심부름은 하지 마라.\"\n",
            "\"다심 아덜은 콩 죽 멕이곡, 원 아덜은 폿 죽 멕인다.\" -> \"의붓 아들은 콩죽 먹이고, 친 아들은 팥죽 먹인다.\"\n",
            "\"다심아방 궤기 써는 디랑 가고, 원아방 낭깨는 디랑 얼씬을 말라.\" -> \"의붓아버지 고기 써는데는 가고, 친아버지 장작 패는데는 가지를 말라.\"\n",
            "\"대천 바당도 건너 봐사 안다.\" -> \"넓은 바다도 건너 봐야 안다.\"\n",
            "\"대정 몽생이 요망진다.\" -> \"대정 망아지 야무지고 알차다 = 대정은 서귀포시 대정읍 지명임.\"\n",
            "\"덴디 꺼럭 아니 돗나\" -> \"덴 곳에 털 안난다.\"\n",
            "\"도 터질 밭디 모쉬 들듯.\" -> \"들목 터진 밭에 마소 들듯\"\n",
            "\"도둑놈광 샛뭇은 묶엉 세와노민 똑 닮나.\" -> \"도둑놈과 띠 다발은 묶어서 세워 놓으면 꼭 닮다.\"\n",
            "\"고맙습니다\" -> \"고맙쑤다\"\n",
            "\"가십시오\" -> \"갑써양\"\n",
            "\"제가 당신을 무척 사랑합니다\" -> \"나 이녁 소못 소랑헴수다\"\n",
            "\"남편 옆에 같이 앉으세요\" -> \"서방 조끝에 고치 앉즙써\"\n",
            "\"사람 있습니까\" -> \"사름 잇수과\"\n",
            "\"잘 있었습니까\" -> \"잘 이십디강?\"\n",
            "\"아버지 어머니 모두 편안하셨습니까\" -> \"아방 어멍 다 펜안햇수과\"\n",
            "\"머리 조심하세요\" -> \"데멩이 맹심헙써\"\n",
            "\"어디갔다가 오십니까\" -> \"어디 갓당 왐쑤과\"\n",
            "\"자기 것은 작게 보인다\" -> \"지 것은 족아 벤다\"\n",
            "\"누구십니까\" -> \"누게꽈\"\n",
            "\"제 얼굴이 예쁘지요\" -> \"나 상우댕이 고우꽝\"\n",
            "\"별로 이쁘지 않네요\" -> \"고우멍 말멍 허우다\"\n",
            "\"오천원입니다\" -> \"오천원마씀\"\n",
            "\"큰일 났습니다\" -> \"큰일 낫수다\"\n",
            "\"어떻게 생겼던가\" -> \"어떵 생겨서\"\n",
            "\"어디로 모실까요\" -> \"어디가코 마시\"\n",
            "\"똑똑하고 영리하게 생겼습니다\" -> \"요망지게 생겨십디다\"\n",
            "\"제주특별자치도에서 삽시다\" -> \"제주특별자치도에서 살게 마씀\"\n",
            "\"따스합니까\" -> \"맨드롱햇수과\"\n",
            "\"왜 그러십니까\" -> \"무사 마씀\"\n",
            "\"오시느라 고생 많으셨습니다\" -> \"오젠허난 폭삭 속아수다\"\n",
            "\"빨리빨리 더빨리 오세요\" -> \"재기재기 보질 혼 보기 양질하지\"\n",
            "\"참말로 이쁘고 둥실둥실합니다\" -> \"촘말로 곱고 몬트락허우다\"\n",
            "\"이곳은 바람도 많이 불고있네\" -> \"이딘 보름도 하영 불엄져\"\n",
            "\"소문나면 저는 모른다\" -> \"소민나민 난 몰라\"\n",
            "\"이곳에 있으니 엄마 보고싶다\" -> \"이디 와부난 어멍 보지그립다\"\n",
            "\"지쳐서 잠이 온다\" -> \"지천 자지 그립다\"\n",
            "\"햇볕은 쨍쨍 모래알은 반짝반짝\" -> \"뱃은 과랑과랑 모살은 삔찍삔찍\"\n",
            "\"아무리 이야기하여도 귀 눈이 캄캄\" -> \"아무거앤 고라도 귀 눈이 왁왁\"\n",
            "\"그러기에 말입니다\" -> \"게메 마씀\"\n",
            "\"나를 보십시오\" -> \"날 봅서\"\n",
            "\"잘되어 버렸다\" -> \"제나콰니여\"\n",
            "\"어마어떻게 하지\" -> \"으마떵호리\"\n",
            "\"제주도 사투리로 말 호난\" -> \"제주도 사투리 정말로 나하고 말하는만 사람하라.\"\n",
            "\"무신거랜 고람 신지 몰르쿠게\" -> \"뭐라고 말하는지 모르겠지요\"\n",
            "\"게메 마씀\" -> \"글세 말입니다\"\n",
            "\"귀 눈이 왁왁하우다\" -> \"귀와 눈이 캄캄합니다\"\n",
            "\"경해도 고만히 생각호멍 들으민 호끔씩 알아집니다.\" -> \"그래도 가만히 생각하며 들으면 조금씩 알 게 됩니다\"\n",
            "\"오랜만이우다\" -> \"오랜만이예요\"\n",
            "\"몸은 펜안 하우꽈\" -> \"편안하십니까\"\n",
            "\"얼굴이 펜안해 보염수다\" -> \"얼굴이 좋아보이네요\"\n",
            "\"혼저옵서예\" -> \"어서오십시요\"\n",
            "\"제주도 오잰하난 폭삭 속아수다\" -> \"제주도 오느라 수고하셨어요\"\n",
            "\"제주도 오난 촘말로 좋수다\" -> \"제주도에 오니까 정말로 좋습니다\"\n",
            "\"산이영 바당이영 몬딱 좋은게 마씀\" -> \"산이랑 바다랑 모두가 좋습니다\"\n",
            "\"많이많이봥으네 서울에 갈 때랑 하영 담앙 갑서\" -> \"많이 보고, 서울에 갈 때는 많이 담아서 가십시오\"\n",
            "\"게메 양\" -> \"그러게 말입니다\"\n",
            "\"경 해시민 얼마나 좋코 마씀\" -> \"그렇게 했으면 얼마나 좋겠습니까\"\n",
            "\"안트레 들어 왕, 밥 먹엉 갑서\" -> \"안으로 들어 오셔서, 식사 하고 가십시오\"\n",
            "\"제주도 와시난 전복죽드셔봥 가살꺼 아니꽝\" -> \"제주도 오셨으니, 전복죽드셔보고 가셔야죠\"\n",
            "\"혼저 왕 드셩갑서\" -> \"어서 와서 드세요\"\n",
            "\"맨도롱 했수과\" -> \"따뜻합니까\"\n",
            "\"뜻뜻혼 게 먹기 똑 좋았수다\" -> \"따끈따끈한 것이 먹기에 꼭 좋았습니다\"\n",
            "\"맛있수다\" -> \"맛 좋네요\"\n",
            "\"여기예\" -> \"여기요\"\n",
            "\"이거 키로에 얼마마씨?\" -> \"이거 1kg에 얼마예요?\"\n",
            "\"사천원마씨\" -> \"사천원입니다\"\n",
            "\"기꽝?\" -> \"그래요?\"\n",
            "\"사천원치 줍서\" -> \"사천원어치주세요\"\n",
            "\"하영줍서예\" -> \"많이주세요\"\n",
            "\"많이 담암수다양\" -> \"많이 드립니다\"\n",
            "\"맛있게 드십써 양\" -> \"맛있게 드세요\"\n",
            "\"삼성혈가잰하는디예\" -> \"삼성혈에 가려는데요\"\n",
            "\"어디로 가면 되마씨?\" -> \"어디로 가면 되지요?\"\n",
            "\"여기서 10분만 밑트레강 칼호텔조끄티서 바로 왼펜돌아가민 되우다\" -> \"여기서 10분 밑으로가서 칼호텔 근처에서 돌아서 왼쪽으로 가면 됩니다.\"\n",
            "\"알아지쿠가?\" -> \"알겠어요?\"\n",
            "\"거기강 봥으네, 아무사람이나 잡앙 물어보민 잘 말해 줄꺼우다\" -> \"거기가서 아무사람이나 물어보면 잘 말해 줄꺼예요\"\n",
            "\"관광 잘행 갑써예\" -> \"관광 잘하고 가세요\"\n",
            "\"혼저옵서예\" -> \"어서오십시요\"\n",
            "\"고만이십서게\" -> \"잠깐 있으십시오\"\n",
            "\"고만 이시라게\" -> \"잠깐있어라\"\n",
            "\"무신거 햄쑤꽈?\" -> \"무엇을 하십니까?\"\n",
            "\"무신거 햄시니?\" -> \"무엇을 하느냐?\"\n",
            "\"무사 경 급호꽈?\" -> \"왜 그렇게 급하십니까?\"\n",
            "\"무사 경 급호냐?\" -> \"왜 그렇게 급하냐?\"\n",
            "\"확 옵서게\" -> \"빨리 오십시오\"\n",
            "\"어떵 살아 점수꽈?\" -> \"어떻게 살고 있습니까?\"\n",
            "\"무싱거엔 고람쑤꽈?\" -> \"무엇이라고 말하십니까?\"\n",
            "\"귀 눈이 왁왁하우다.\" -> \"귀와 눈이 캄캄합니다\"\n",
            "\"맨도롱 혼때 호로록 들여 싸 붑써\" -> \"따뜻할 때 후루룩 잡수십시오\"\n",
            "\"무싱 걸 몽캐미꽈?\" -> \"뭘 그리 늦장 부리십니까?\"\n",
            "\"왕 봅서, 고랑은 몰라 마씀\" -> \"와서 보십시오,말로 해서는 모릅니다\"\n",
            "\"곱들락 호게 몬뜰락 벗엉 옵서\" -> \"아름답게 모조리 벗어서 오십시오\"\n",
            "\"어디 갔당 왐수꽈?\" -> \"어디 갔다가 오십니까?\"\n",
            "\"어드레 감수꽈?\" -> \"어디로 가십니까?\"\n",
            "\"둥그리멍 키웁서, 경 해사 혼저 큽니다.\" -> \"고생시키면서 키웁십시오.그래야 빨리 자랍니다\"\n",
            "\"어떵 살코, 저들지 맙서, 촘앙 살민 살아집니다\" -> \"어떻게 살까,걱정 마십시오.참고 살면 살수 있습니다\"\n",
            "\"느영 나영 두리둥실 소랑호게 마씸\" -> \"너하고 나하고 둘이 둥실 사랑합시다\"\n",
            "\"혼저 옵서\" -> \"어서 오십시오\"\n",
            "\"제주도에 오난 어떵 하우꽈?\" -> \"제주도에 오니 어떠하십니까?\"\n",
            "\"촘말로 좋수다\" -> \"정말로 좋습니다\"\n",
            "\"왕 봅서\" -> \"와서 보십시오\"\n",
            "\"고랑은 몰라마씀\" -> \"말로해서는 모릅니다\"\n",
            "\"무상 경 고람수꽈?\" -> \"왜 그렇게 말하십니까?\"\n",
            "\"몽캐지 말앙 혼저 오라게\" -> \"꾸물대지 말고 어서 오너라\"\n",
            "\"호꼼만 이십서게\" -> \"조금만 계십시오\"\n",
            "\"혼저 왕 먹읍서\" -> \"어서 와서 먹으십시오\"\n",
            "\"맨드롱 했수꽈?\" -> \"따뜻합니까?\"\n",
            "\"맨도롱 홀 때 호로록 들여 싸붑서\" -> \"따뜻할 때 후루룩 마셔 버리십시오\"\n",
            "\"혼저 옵서.\" -> \"어서 오십시오.\"\n",
            "\"제주도 사투리로 말 호난, 무신 거옌 고람 신디 몰르쿠게?\" -> \"제주도 사투리 정말로 귀하고 아름지운 살나를 그렇지요?\"\n",
            "\"게메 마씀.\" -> \"글쎄 말입니다.\"\n",
            "\"귀 눈이 왁왁하우다.\" -> \"귀와 눈이 캄캄합니다\"\n",
            "\"경해도 고만히 생각호멍 들으민 조금씩 알아집니다.\" -> \"그래도 가만히 생각하며 들으면 조금씩 알게됩니다.\"\n",
            "\"제주도 사투리 촘말로 귀하고 아름다운 보물이우다.\" -> \"제주도 사투리 정말로 귀하고 아름다운 보물입니다.\"\n",
            "\"펜안 하우꽈?\" -> \"안녕하십니까?\"\n",
            "\"제주도에 오난 어떵 하우꽈?\" -> \"제주도에 오니 어떠하십니까?\"\n",
            "\"촘말로 좋수다.\" -> \"정말로 좋습니다.\"\n",
            "\"공기도 맑고, 산이영 바당이영 몬딱 좋은게 마씀.\" -> \"공기도 맑고, 산이랑 바다랑 모두가 좋습니다.\"\n",
            "\"서울에 갈 때랑 하영 담앙 갑서.\" -> \"서울에 갈 때는 많이 담아서 가십시오.\"\n",
            "\"게메, 양.\" -> \"그러게 말입니다.\"\n",
            "\"경 해시민 얼마나 좋코 마씀?\" -> \"그렇게 했으면 얼마나 좋겠습니까?\"\n",
            "\"저기, 물허벅 정 가는 거, 비바리덜 아니꽈?\" -> \"저기, 바구니 지고 가는 거, 처녀들 아닙니까?\"\n",
            "\"맞수다.\" -> \"맞습니다.\"\n",
            "\"비바리도 있고, 넹바리도 있수다.\" -> \"처녀도 있고, 시집 간 여자도 있습니다.\"\n",
            "\"비바리덜 곱들락 호고 놀씬하우다 양!\" -> \"처녀들 곱고 날씬하군요!\"\n",
            "\"경 합주게, 산 좋고 물 좋은 제주도에 사난 모심도 착하고 얼굴도 곱들락 홉니다.\" -> \"그렇지요, 산 좋고 물 좋은 제주도에 사니 마음도 착하고 얼굴도 곱습니다.\"\n",
            "\"고랑은 몰라 마씀.\" -> \"말로 해서는 모릅니다.\"\n",
            "\"제주도에 왕 봐사 알아짐니다.\" -> \"제주도에 와서 보아야 알 수 있습니다.\"\n",
            "\"돌도 많고, 보롬도 많고, 비바리도 많고, 유채꽃도 곱드락 호게 피었수다.\" -> \"돌도 많고, 바람도 많고, 처녀도 많고, 유채꽃도 아름답게 피었습니다.\"\n",
            "\"아명 고라도 몰라 마씀.\" -> \"아무리 말해도 모릅니다.\"\n",
            "\"혼저 왕 봅서.\" -> \"어서 와서 보십시오.\"\n",
            "\"제주도엔 보름이 많이 있수다.\" -> \"제주도에는 바람이 많이 있습니다.\"\n",
            "\"동쪽에서 부는 보름, 샛보름.\" -> \"동쪽에서 부는 바람, 샛바람.\"\n",
            "\"서쪽에서 부는 보름, 갈보름.\" -> \"서쪽에서 부는 바람, 갈바람.\"\n",
            "\"남쪽에서 부는 보름, 마포름.\" -> \"남쪽에서 부는 바람, 마파람.\"\n",
            "\"북쪽에서 부는 보름, 하늬보름 이우다.\" -> \"북쪽에서 부는 바람, 하늬바람 입니다.\"\n",
            "\"한락산에 올랑 봅서.\" -> \"한라산에 올라서 보십시오.\"\n",
            "\"발 아래 구름들이 왔닥갔닥 호곡 아득혼게 꿈속 고틉니다.\" -> \"발 아래 구름들이 오락가락 하고 아듣하니 꿈속 같습니다.\"\n",
            "\"옛날에 신선들이 힌 사슴 타멍 놀았댄 해연 백록담 아니꽈.\" -> \"옛날에 신선들이 흰 사슴을 타며 놀았다고 해서 백록담 아닙니까.\"\n",
            "\"제주 사롬들은 돌 하르방 보멍 살았수다.\" -> \"제주 사람들은 돌 하르방 보면서 살았습니다.\"\n",
            "\"경 호난, 거짓말 홀 줄 모르곡 서로 믿으멍 착하게만 살았수게.\" -> \"그러니까, 거짓말 할 줄 모르고 서로 믿으면서 착하게만 살았습니다.\"\n",
            "\"봅서, 어시민 도와 주곡, 이시민 나누엉 먹곡, 인정 많고, 소랑 많은 돌하르방 손지들 이우다.\" -> \"보십시오, 없으면 도와 주고, 있으면 나누어서 먹으면서, 인정 많고, 사랑이 많은 돌하르방 손자들 입니다.\"\n",
            "\"장게간 날 밤, 새 서방이 말했수게.\" -> \"장가간 날 밤, 새 신랑이 말했습니다.\"\n",
            "\"`새 각시야, 혼저 오라게.`\" -> \"`색씨야, 어서 오너라.`\"\n",
            "\"`무사 마씀?`\" -> \"`왜 그러십니까?`\"\n",
            "\"`호꼼만 이십서게.`\" -> \"`조금만 계십시오.`\"\n",
            "\"`뭉캐지 말앙 혼저 오라게.`\" -> \"`꾸물대지 말고 어서 오너라.`\"\n",
            "\"새 각시가 새 서방 품에서 솔째기 말해십주.\" -> \"신부가 신랑 품에서 살그머니 말했답니다.\"\n",
            "\"`날 얼마나 소랑햄쑤과?`\" -> \"`나를 얼마나 사랑하십니까?`\"\n",
            "\"`저기 한락산만큼, 또 바당만큼 소랑햄쩌.`\" -> \"`저기 한라산만큼, 또 바다만큼 사랑한다.`\"\n",
            "\"`경 마랑, 요 가슴패기 만큼만 소랑해 줍서.`\" -> \"`그러지 말고 이 가슴팍 만큼만 사랑해 주십시오.`\"\n",
            "\"`기여, 느영 나영 두리 둥실 소랑호게.`\" -> \"`그래, 너하고 나하고 둘이 둥실 사랑하자.`\"\n",
            "\"신랑이 신부에게 말했수게.\" -> \"신랑이 신부에게 말했답니다.\"\n",
            "\"`이 조끄뜨레 오라게.`\" -> \"`여기 가까이 오너라.`\"\n",
            "\"`무사 조끄뜨레만 오랜 햄수과?`\" -> \"`왜 가까이만 오라고 하십니까?`\"\n",
            "\"`호꼼이라도 고치만 있고 싶언.`\" -> \"`조금이라도 같이만 있고 싶어서.`\"\n",
            "\"`놈덜 웃읍니다.`\" -> \"`남들이 웃습니다.`\"\n",
            "\"`어떵 호느냐? 소랑에는 부치룸이 엇나.`\" -> \"`어떡하느냐? 사랑에는 부끄러움이 없단다.`\"\n",
            "\"무사 경 고람쑤과?\" -> \"왜 그렇게 말하십니까?\"\n",
            "\"혼저 왕 소랑하멍 살게 마씀.\" -> \"어서 와서 사랑하면서 살아 봅시다.\"\n",
            "\"성 내멍 살아도 혼 세상 저들멍 살아도 혼 세상 이우다.\" -> \"화를 내면서 살아도 한 세상 걱정하면서 살아도 한 세상 입니다.\"\n",
            "\"가난해서 웃으멍 살게 마씀.\" -> \"가난해도 웃으면서 살아 봅시다.\"\n",
            "\"삼다도옌 행게마는 촘말 돌이 많쑤다.\" -> \"삼다도라고 하더니만 정말 돌이 많습니다.\"\n",
            "\"양?\" -> \"예?\"\n",
            "\"옛날엔 한락산이 화산이란 돌이 많쑤게.\" -> \"옛날에는 한라산이 화산이었기 때문에 돌이 많답니다.\"\n",
            "\"아이고 게, 저기 돌고망에 핀 꽃 봅서 돌고망으로 보는 바당은 꼭 그림 답쑤다.\" -> \"아이고, 저기 돌구멍에 핀 꽃 보십시오. 돌구멍으로 보는 바다는 꼭 그림 같군요.\"\n",
            "\"바다에 강 봐사.\" -> \"바다에 가서 보아야.\"\n",
            "\"풍랑을 알아 지곡 조식을 나 봐사.\" -> \"풍랑을 알게 되고 자식을 낳아 보아야.\"\n",
            "\"부모 모심 알아 지곡 사롬은 만낭 살아 봐사.\" -> \"부모 마음 알게 되고 사람은 만나서 살아 보아야.\"\n",
            "\"진심을 알아 지곡 소랑을 해 봐사, 눈물도 알아 짐니다.\" -> \"진짜 마음을 알게 되고 사랑을 해 보아야, 눈물도 알게 된답니다.\"\n",
            "\"사롬은 서로 서로 도우멍 사는 거여!\" -> \"사람은 서로 서로 도우며 사는 것이야!\"\n",
            "\"욕은 고냉이 밤눈 어둡덴 혼다.\" -> \"약은 고양이가 밤눈이 어둡다고 한다.\"\n",
            "\"이녁만 욕은 체 호멍 살당 보민.\" -> \"자기만 약은 체 하면서 살다 보면.\"\n",
            "\"지 욕심에 걸령 지가 넘어진다.\" -> \"자기 욕심에 걸려서 자기가 넘어진단다.\"\n",
            "\"서귀포 바당에 가민 알아 짐니다.\" -> \"서귀포 바다에 가면 알수 있습니다.\"\n",
            "\"무신것이 소랑이고.\" -> \"무엇이 사랑이고.\"\n",
            "\"무신 것이 그리움인지.\" -> \"무엇이 그리움인지.\"\n",
            "\"안개 낀 부둣가에 뱃고동 울어 주민\" -> \"안개 낀 부둣가에 뱃고동 울어 주면.\"\n",
            "\"옛님이 그리웡 눈물이 잘도 납니다.\" -> \"옛님이 그리워서 눈물이 많이도 납니다.\"\n",
            "\"조냥 하멍 살아사 혼다.\" -> \"아끼면서 살아야 한다.\"\n",
            "\"오늘보다 내일 생각호곡.\" -> \"오늘보다 내일을 생각하고.\"\n",
            "\"내일보다 모레 생각호곡.\" -> \"내일보다 모레를 생각하고.\"\n",
            "\"경 해사 일생을 펜안호게 살아진다.\" -> \"그렇게 해사 일생을 편히 살수 있단다.\"\n",
            "\"요새 아이덜.\" -> \"요사이 아이들.\"\n",
            "\"너무 호강 햄쩌 경 호난\" -> \"너무 편안히 산다 그러니까.\"\n",
            "\"호꼼호민 아프곡.\" -> \"조금하면 아프고.\"\n",
            "\"울곡 햄시녜.\" -> \"울고 하고 있다.\"\n",
            "\"둥구리멍 질룬 독새시가 빙애기 된댕 혼다.\" -> \"뒹글리면서 기른 달걀이 병아리가 된다고 한다.\"\n",
            "\"어진 때 고생도 호곡 해사 큰 사롬 된다.\" -> \"어릴 때 고생도 하고 해야 큰 사람 된다.\"\n",
            "\"혼저 왕 먹읍서.\" -> \"어서 와서 먹으십시오.\"\n",
            "\"맨도롱 했수과?\" -> \"따뜻합니까?\"\n",
            "\"똣똣혼 게 먹기 똑 좋았수다.\" -> \"따끈따끈한 것이 먹기에 꼭 좋았습니다.\"\n",
            "\"맨도롱 홀 때 호로록 들여 싸붑서.\" -> \"따뜻할 때 후루룩 마셔 버리십시오.\"\n",
            "\"동 텃저, 어서 글라.\" -> \"날이 밝았다, 어서 가자.\"\n",
            "\"일 해사 먹엉 산다.\" -> \"일 해야 먹고 산다.\"\n",
            "\"부지런 혼 건 하늘도 못 막낸 해라.\" -> \"부지런한 것은 하늘도 못 막는다고 하더라.\"\n",
            "\"혼저 촐령 가사, 일 홀 거 아니가.\" -> \"뻘리 준비해서 가야, 일 할 것 아니냐.\"\n",
            "\"시집 가 보난 영 해라.\" -> \"시집을 가 보니까 이렇드라.\"\n",
            "\"돌탱이 고튼 시아방에.\" -> \"돌덩이 같이 고집 센 시아버지에.\"\n",
            "\"물폐기 고튼 시어멍에.\" -> \"살모사 같이 독한 시어머니에.\"\n",
            "\"물꾸럭 고튼 서방에.\" -> \"문어 같이 물렁물렁한 남편에.\"\n",
            "\"고냥독새 고튼 시누이에.\" -> \"굴뚝새 같이 약아 빠진 시누이에.\"\n",
            "\"가도 가도 고생이라라.\" -> \"살아도 살아도 고생이더라.\"\n",
            "\"어드레 감디?\" -> \"어디로 가느냐?\"\n",
            "\"바당에 어멍 안티 감쑤다.\" -> \"바다에 어머니 한테 갑니다.\"\n",
            "\"경 호건, 이거 가정 가라.\" -> \"그러면, 이거 가지고 가라.\"\n",
            "\"무싱 거꽈?\" -> \"무엇입니까?\"\n",
            "\"느네 어멍, 바당에서 물질 호멍 먹을거여.\" -> \"너희 어머니, 바다에서 물질하면서 먹을거다.\"\n",
            "\"이추룩 억울할 때가 있수과?\" -> \"이렇게 억울할 때가 있습니까?\"\n",
            "\"소정해도 시원호지 않을 놈이 큰 소리라 마씸.\" -> \"사정을 해도 시원하지 않을 놈이 큰 소리 합니다.\"\n",
            "\"초마 가라, 그거 도둑놈 심뽀구나.\" -> \"아니 원, 그거 도둑놈 마음이구나.\"\n",
            "\"게메 마씸, 그런 사롬이 어디 있수과?\" -> \"그러게 말입니다, 그런 사람이 어디 있습니까?\"\n",
            "\"시험에 떨어진 아들이 아방 안티 말했수다.\" -> \"시험에 떨어진 아들이 아버지께 말했습니다.\"\n",
            "\"`난, 아명 해도 안되쿠다.`\" -> \"`난, 아무리 해도 안되겠습니다.`\"\n",
            "\"`무사, 안된댄 생각 햄시냐?`\" -> \"`왜, 안된다고 생각하느냐?`\"\n",
            "\"`해도 해도 안되는디, 어떵홉니까?`\" -> \"`해도 해도 안되는걸, 어떻게 합니까?`\"\n",
            "\"`먹돌도 똘람 시민 고망이 난다. 햄시민 된다.`\" -> \"`차돌도 뚫고 있으면 구멍이 난다. 하고 있으면 된다.`\"\n",
            "\"똘년이 돌으멍 가단 푸더전 마씀.\" -> \"딸이 뛰어 가다가 쓰러졌답니다.\"\n",
            "\"으마떵 호리.\" -> \"이런, 어떻게 하나.\"\n",
            "\"경 호연 어떵 해서 ?\" -> \"그래서 어떻게 했는가?\"\n",
            "\"병원에 갔수게.\" -> \"병원에 갔습니다.\"\n",
            "\"간 보난, 꽝이 뿌서졌덴 마씀.\" -> \"가서 보니까 뼈가 부러졌답니다.\"\n",
            "\"혼, 열흘간 입원 해사 혼덴 햄쑤다.\" -> \"한, 열흘간 입원 해야 한다고 합니다.\"\n",
            "\"족댕 나무래지 말라.\" -> \"작다고 놀리지 마라.\"\n",
            "\"조근게 요망진다.\" -> \"작은 것이 똑똑하고 영리하다.\"\n",
            "\"큰 건 보롬만 불어도.\" -> \"큰 것은 바람만 불어도.\"\n",
            "\"홍글랑 홍글랑 호지만 호꼴락 혼 건.\" -> \"흔들 흔들 거리지만 조그만 한 건.\"\n",
            "\"똔똔호곡 펜주룽 혼다.\" -> \"단단하고 태연하게 있는다.\"\n",
            "\"어드레 감수과?\" -> \"어느 곳으로 가십니까?\"\n",
            "\"바당에 괴기 사레 마씀.\" -> \"바다에 고기 사려고 말입니다.\"\n",
            "\"무사, 누게 왔수과?\" -> \"왜, 누가 왔습니까?\"\n",
            "\"서울서 족은 아방네 완 마씀.\" -> \"서울에서 작은 아버지네가 왔습니다.\"\n",
            "\"게민, 멩심허영 갔당 옵서.\" -> \"그럼, 조심해서 갔다 오십시오.\"\n",
            "\"어멍이 해 준 밥 먹어 봅서.\" -> \"어머니가 해 준 밥을 먹어 보십시오.\"\n",
            "\"촘말로 맛이 좋수다.\" -> \"정말로 맛이 좋습니다.\"\n",
            "\"무상고 마씀?\" -> \"무엇 때문인가요?\"\n",
            "\"그건 어머님의 사랑호는 모심이 밥속에 고득 고득 들어 이시난 아니꽈.\" -> \"나그 난 어머니들 사랑이 입니다.\"\n",
            "\"소도리 행 댕기지 맙서.\" -> \"소문 옮기면서 다니지 마십시오.\"\n",
            "\"소도리는 놈덜 싸움만 맨듭니다.\" -> \"소문은 남들 싸움만 만듭니다.\"\n",
            "\"옛부터 싸움은 멀리곡.\" -> \"예부터 싸움은 말리고.\"\n",
            "\"흥정은 부치렌 했수다.\" -> \"흥정은 붙이라고 했습니다.\"\n",
            "\"게난, 좋은 소린 크게 호곡.\" -> \"그러니, 좋은 소리는 크게 하고.\"\n",
            "\"궂은 소린 속솜 해 붑서.\" -> \"나쁜 소리는 조용히 해 버리십시오.\"\n",
            "\"두 가시가 어드레 경 돌암서?\" -> \"두 부부가 어디로 그렇게 달려 가는가?\"\n",
            "\"큰 똘, 애기 낫댄 햄 감쑤다.\" -> \"큰 딸이 아이를 낳았다고 해서 갑니다.\"\n",
            "\"경 했구나.\" -> \"그랬구나.\"\n",
            "\"무슨 애기 나서?\" -> \"무슨 애기를 낳았는가?\"\n",
            "\"아덜 마씀!\" -> \"아들 입니다!\"\n",
            "\"아이고, 잘 됐구나게!\" -> \"아이구, 잘 되었네!\"\n",
            "\"혼저 강 봐 게.\" -> \"어서 가 보게나.\"\n",
            "\"강, 방 옵서.\" -> \"가서 보고 오십시오.\"\n",
            "\"왕, 방 삽서.\" -> \"와서 보고 사십시오.\"\n",
            "\"영 호곡, 정 호곡 홉서.\" -> \"이렇게 하고, 저렇게 하고 하십시오.\"\n",
            "\"경 고라 줍서.\" -> \"그렇게 말해 주십시오.\"\n",
            "\"가지 말앙, 영 호멍 삽서.\" -> \"가지 말고서 이렇게 하면서 사십시오.\"\n",
            "\"서방님, 제주도에서 살게 마씀.\" -> \"여보, 제주도에서 삽시다.\"\n",
            "\"좋주마.\" -> \"좋지요.\"\n",
            "\"널은 바당.\" -> \"넓은 바다.\"\n",
            "\"좋은 공기.\" -> \"좋은 공기.\"\n",
            "\"인심좋은 사롬들광 살민 살 맛 날거라.\" -> \"인심좋은 사람들과 살면 살 맛 날거다.\"\n",
            "\"살당 지치민.\" -> \"살다가 지치면.\"\n",
            "\"풍덩 바당에 들어 가곡 마씀.\" -> \"풍덩 바다에 들어 가고 말입니다.\"\n",
            "\"둘이 먹당, 호나 죽어도 몰르는 건 무신거꽈?\" -> \"둘이 먹다가, 한 명이 죽어도 모르는 건 무엇입니까?\"\n",
            "\"돌코롬 혼 제주도 전복죽이주!\" -> \"달고 맛있는 제주도 전복죽이야!\"\n",
            "\"둘이 보당, 호나 죽어도 몰르는 건 무신거꽈?\" -> \"둘이 보다가, 한 사람이 죽어도 모르는 건 무엇입니까?\"\n",
            "\"곱들락 혼 제주도 비바리주!\" -> \"곱고 예쁘장한 제주도 처녀야!\"\n",
            "\"게민 옵서.\" -> \"그러면 오십시오.\"\n",
            "\"전복죽 먹곡 비바리 보래 가게 마씀.\" -> \"전복죽 먹고서 처녀 보러 갑시다.\"\n",
            "\"과랑 과랑혼 벳디 일 호젠 호난 속았수다.\" -> \"쨍쨍한 햇볕속에 일 하려고 하니 수고 했습니다.\"\n",
            "\"속을 거 있수과?\" -> \"수고 할 거 있습니까?\"\n",
            "\"호꼼, 똠은 났수다만.\" -> \"조금, 땀은 났습니다만.\"\n",
            "\"안트레 들어 왕.\" -> \"안으로 들어 오셔서.\"\n",
            "\"조녁 먹엉 갑서.\" -> \"저녁식사 하고 가십시오.\"\n",
            "\"경 호카 마씀.\" -> \"그렇게 할까요.\"\n",
            "\"제주 바당에 가민 바당이 말을 홉니다.\" -> \"제주 바다에 가면 바다가 말을 합니다.\"\n",
            "\"사롬보다.\" -> \"사람보다.\"\n",
            "\"더 조잘조잘 말을 하곡.\" -> \"더 재잘재잘 말을 하고.\"\n",
            "\"사롬보다.\" -> \"사람보다.\"\n",
            "\"더 소랑도 하영 홉니다.\" -> \"더 사랑도 많이 합니다.\"\n",
            "\"경 호멍.\" -> \"그렇게 하면서.\"\n",
            "\"낮이나 밤이나 꽃을 피웠당.\" -> \"낮이나 밤이나 꽃을 피웠다가.\"\n",
            "\"지웠당 홉니다.\" -> \"지웠다가 합니다.\"\n",
            "\"제주 바당에 강 봅서.\" -> \"제주 바다에 가서 보십시오.\"\n",
            "\"미운 것도 엇어지곡.\" -> \"미운 것도 없어지고.\"\n",
            "\"슬픈것도 엇어지곡.\" -> \"슬픈것도 없어지고.\"\n",
            "\"가슴으로 바당물만 들곡 나곡 호멍.\" -> \"가슴으로 바닷물만 들어오고 나가고 하면서.\"\n",
            "\"두리 둥실.\" -> \"둘이 둥실.\"\n",
            "\"둥그대 당실.\" -> \"둥그대 당실.\"\n",
            "\"소랑만 고득하게.\" -> \"사랑만 가득하게.\"\n",
            "\"담앙 옵니다.\" -> \"담아서 옵니다.\"\n",
            "\"서울서 새각씨 왔수다.\" -> \"서울에서 색씨가 왔습니다.\"\n",
            "\"어떵 생겨서?\" -> \"어떻게 생겼던가?\"\n",
            "\"요망지게 생겨십디다.\" -> \"똑똑하고 영리하게 생겼습니다.\"\n",
            "\"솔째기 웃으멍 말 호는 거 보난.\" -> \"살그머니 웃으면서 말 하는 걸 보니.\"\n",
            "\"착햄직 홉디다.\" -> \"착할 것 같습니다.\"\n",
            "\"그 나쁜 놈!\" -> \"그 나쁜 놈!\"\n",
            "\"볼망텡이를.\" -> \"뺨따귀를\"\n",
            "\"박아 불거 아니꽈?\" -> \"때려 버릴거 아닙니까?\"\n",
            "\"경 했주!\" -> \"그렇게 했어!\"\n",
            "\"모가지 잡안 흥글단.\" -> \"목을 잡아서 흔들다가\"\n",
            "\"대맹이로 뱃대기를.\" -> \"머리로 배를\"\n",
            "\"박아 부난.\" -> \"받아 버렸더니\"\n",
            "\"둥글멍 자빠정게!\" -> \"뒹글며 쓰러져 버리더군!\"\n",
            "\"잘 콰니여!\" -> \"잘 됐어!\"\n",
            "\"그런 놈은.\" -> \"그런 놈은\"\n",
            "\"혼내 줘사 홉니다.\" -> \"혼을 내 줘야 합니다.\"\n",
            "\"쌍둥이 어멍!\" -> \"쌍둥이 어머니!\"\n",
            "\"고생햄쭈, 이?\" -> \"고생하고 있네요, 그렇지요?\"\n",
            "\"둘이가 고름배기 해연.\" -> \"둘이가 서로 이기려고 하니.\"\n",
            "\"죽어지쿠다.\" -> \"죽겠습니다.\"\n",
            "\"경 홀거라.\" -> \"그렇게 할 거에요.\"\n",
            "\"게도, 놈 호고 싸울 땐.\" -> \"그래도, 남하고 싸울 때는.\"\n",
            "\"지네 펜백들멍 싸워 마씀.\" -> \"자기네 편을 들면서 싸웁니다.\"\n",
            "\"요 년 난거 보라.\" -> \"요 계집애 봐라.\"\n",
            "\"놈이 소나이영 말 호곡.\" -> \"남의 남자와 얘기 하고.\"\n",
            "\"벌써부터.\" -> \"벌써부터.\"\n",
            "\"서방질 햄시냐?\" -> \"남자를 밝히느냐?\"\n",
            "\"아니우다 게.\" -> \"아닙니다.\"\n",
            "\"바당드레 가단.\" -> \"바다에 가다가.\"\n",
            "\"질 모른 사롬 이시난.\" -> \"길 모르는 사람이 있어서.\"\n",
            "\"질 고르쳐 줬수게.\" -> \"길 가르켜 줬습니다.\"\n",
            "\"아이구 게.\" -> \"아이구.\"\n",
            "\"큰일 났수다.\" -> \"큰일 났습니다.\"\n",
            "\"무사, 경 들락켬시니?\" -> \"왜, 누가 왔습니까?\"\n",
            "\"와리지 말앙.\" -> \"서울에 갈 버지게 기차게 거으면서.\"\n",
            "\"촌촌이 고라 보게.\" -> \"요 돌 거지만.\"\n",
            "\"경호난 양.\" -> \"그러게 말하십니까\"\n",
            "\"영 호곡.\" -> \"이렇게 하고.\"\n",
            "\"정 호곡 했수게.\" -> \"조금이 정말로 이이 오너라.\"\n",
            "\"게난.\" -> \"요.\"\n",
            "\"욕심 내멍 살민.\" -> \"자식 면서 살자 한다.\"\n",
            "\"경 혼다.\" -> \"그런을 어켜서다\"\n",
            "\"새각씨야.\" -> \"작아들은 멱서\"\n",
            "\"그거 앗아 도라.\" -> \"그건 어머니가 사랑이 만리야 그리 그렇지 말고.\"\n",
            "\"무사.\" -> \"둘이가 둘이.\"\n",
            "\"당신은 손 엇수과?\" -> \"누어도 했어니까?\"\n",
            "\"발 엇수과?\" -> \"죽어죽 안 죽습니다.\"\n",
            "\"경 호지 말앙.\" -> \"그런 것은 하고 가십시오\"\n",
            "\"좀 앗아 주민.\" -> \"살아도 살아도 고생하면.\"\n",
            "\"어떵 호느냐.\" -> \"어떻게 하느냐?\"\n",
            "\"버릇나 마씀!\" -> \"곱다 예쁘장한 제주도 사람은 만예서는 모양이야\"\n",
            "\"밸 놈의 요망진 소리.\" -> \"남자가 밟은 풀은 이울고.\"\n",
            "\"그만 호라.\" -> \"그래도 가만 한생각하라.\"\n",
            "\"이제 가게, 짐 촐리라.\" -> \"이기, 바구니 지고 가는 거, 처녀들 아닙니까.\"\n",
            "\"알았수다.\" -> \"맞습니다.\"\n",
            "\"아이덜도 도랑 가카 마씀?\" -> \"아들어 이고 사려고 해들 아닙니까?\"\n",
            "\"기여, 모딱 고치 가게.\" -> \"여기요 날에는요 그그지 마십시오\"\n",
            "\"하르방, 할망이 좋아할거여.\" -> \"이렇게 하고 이렇게 하면서 사십시오.\"\n",
            "\"야게기에 때 밀라.\" -> \"겨 구리리 같에  배야 잡다가 배야 잡다 둥미를 생겨십시다\"\n",
            "\"저깽이영, 정겡이도 밀곡.\" -> \"이렇게 있으면 구름이 시집 있는 게 밥으로만도 산게 있는다.\"\n",
            "\"옷 벗엉, 강알 트멍도 밀라.\" -> \"틈새\"\n",
            "\"사롬 몸은 곱닥해사 혼다.\" -> \"사람이 어서 사나 사십시오.\"\n",
            "\"무신 걸 먹으코?\" -> \"무엇 때문 아닙니까?\"\n",
            "\"게메 양, 제주도에만 이신 거 먹게 마씀.\" -> \"그러게 말입니다.\"\n",
            "\"그젱기에, 보말에, 옥돔생성, 또 있수다.\" -> \"저, 바에 갔다 가니 한라.\"\n",
            "\"모멀 범벅에 자리젯!\" -> \"치마\"\n",
            "\"먹을 것도 촘 하영 있저.\" -> \"너와 함께 하면 무당들으고 이렇게 살아야!\"\n",
            "\"무사, 난 호꼼만 줨쑤과?\" -> \"왜, 누가 왔습니까?\"\n",
            "\"이놈아!\" -> \"아이구!\"\n",
            "\"욕심 먹은 눈에는.\" -> \"남자가 개과 물들어 피되지며 자기에서 갑시다..\"\n",
            "\"지 껏이 족아 베곡.\" -> \"아버지를 고무를 버지를 버건.\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD_XvYHxfVn7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # 손실 그래프\n",
        "  plt.plot(history.history['loss'], 'y', label='train loss')\n",
        "  plt.plot(history.history['val_loss'], 'r', label='val loss')\n",
        "  plt.legend(loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "  # 정확도 그래프\n",
        "  plt.plot(history.history['acc'], 'y', label='train acc')\n",
        "  plt.plot(history.history['val_acc'], 'r', label='val acc')\n",
        "  plt.legend(loc='upper left')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwfzG6dpwk3k",
        "colab_type": "code",
        "outputId": "c3d525df-71b3-46d5-c62d-d8aea205a719",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print(decoder_inputs[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"strided_slice_2:0\", shape=(?, 734), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCh33atHwvI2",
        "colab_type": "code",
        "outputId": "5591393b-adab-4a2a-e69c-1f7553a29e8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "encoder_model.predict(encoder_input_data[1:2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[ 0.0000000e+00, -0.0000000e+00, -0.0000000e+00,  0.0000000e+00,\n",
              "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  1.6329135e-01,\n",
              "          6.4644217e-04, -0.0000000e+00, -9.2373818e-02,  2.9097532e-28,\n",
              "         -0.0000000e+00, -8.5620570e-01,  0.0000000e+00,  0.0000000e+00,\n",
              "         -9.6190804e-01,  0.0000000e+00,  1.4762883e-31,  0.0000000e+00,\n",
              "         -2.5769413e-02, -1.5891892e-01,  0.0000000e+00, -0.0000000e+00,\n",
              "          0.0000000e+00,  0.0000000e+00, -4.8811927e-01, -5.2259541e-01,\n",
              "          2.4740072e-01,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
              "         -4.8581851e-03, -1.8517128e-01, -0.0000000e+00, -4.0916356e-01,\n",
              "          0.0000000e+00, -9.0105736e-01, -0.0000000e+00,  5.5774748e-03,\n",
              "         -2.9330635e-01, -9.8345417e-01,  0.0000000e+00,  0.0000000e+00,\n",
              "         -0.0000000e+00,  0.0000000e+00, -0.0000000e+00,  3.7912405e-01,\n",
              "         -8.5560732e-02, -0.0000000e+00,  0.0000000e+00, -0.0000000e+00,\n",
              "         -2.9002559e-01, -0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
              "         -0.0000000e+00,  7.6037529e-03,  0.0000000e+00, -0.0000000e+00,\n",
              "          9.0489155e-01,  2.0254247e-01,  0.0000000e+00,  4.6030561e-23,\n",
              "          0.0000000e+00,  0.0000000e+00, -7.6174669e-02, -0.0000000e+00,\n",
              "          0.0000000e+00,  0.0000000e+00, -0.0000000e+00,  2.5714195e-01,\n",
              "          8.8205129e-02, -0.0000000e+00,  0.0000000e+00, -0.0000000e+00,\n",
              "         -0.0000000e+00,  0.0000000e+00, -4.2958260e-02, -4.8764575e-01,\n",
              "         -0.0000000e+00,  0.0000000e+00,  4.7477731e-01,  8.6879867e-01,\n",
              "          1.1599462e-02,  0.0000000e+00,  9.0954005e-04, -1.2454973e-01,\n",
              "          0.0000000e+00,  0.0000000e+00,  9.9968106e-01, -9.1520578e-02,\n",
              "          7.9044974e-01, -2.3096931e-01,  0.0000000e+00, -0.0000000e+00,\n",
              "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00, -0.0000000e+00,\n",
              "          0.0000000e+00,  4.7353655e-01,  4.7984362e-02, -6.8209851e-01,\n",
              "          0.0000000e+00, -7.3482305e-01, -8.5555112e-31, -5.8489289e-02,\n",
              "          0.0000000e+00,  1.3068425e-05, -0.0000000e+00, -5.5376284e-02,\n",
              "         -0.0000000e+00,  0.0000000e+00, -0.0000000e+00, -9.4534010e-02,\n",
              "         -0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
              "          0.0000000e+00, -9.4223726e-01, -4.8242757e-01, -0.0000000e+00,\n",
              "          1.2427284e-01,  0.0000000e+00,  6.2701292e-02, -9.9980128e-01,\n",
              "         -1.8277472e-01, -5.0021201e-02,  0.0000000e+00,  6.6289696e-04,\n",
              "          0.0000000e+00, -3.8783801e-01, -0.0000000e+00, -0.0000000e+00,\n",
              "         -0.0000000e+00, -1.9770602e-02,  0.0000000e+00, -9.9869603e-01,\n",
              "         -0.0000000e+00, -0.0000000e+00,  0.0000000e+00, -2.7131078e-01,\n",
              "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  1.6902500e-01,\n",
              "          0.0000000e+00, -1.5806250e-02,  0.0000000e+00,  0.0000000e+00,\n",
              "          5.6324024e-37,  0.0000000e+00, -0.0000000e+00,  0.0000000e+00,\n",
              "         -0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
              "         -0.0000000e+00, -0.0000000e+00,  0.0000000e+00, -0.0000000e+00,\n",
              "          2.4540097e-02,  7.3298591e-01, -1.1825219e-01, -0.0000000e+00,\n",
              "         -4.2891189e-01, -1.8483192e-02, -0.0000000e+00,  0.0000000e+00,\n",
              "          0.0000000e+00, -0.0000000e+00, -7.6095667e-04,  0.0000000e+00,\n",
              "          3.3993727e-01, -1.1197328e-01, -3.5808769e-01, -0.0000000e+00,\n",
              "         -0.0000000e+00,  5.7189941e-01,  0.0000000e+00,  0.0000000e+00,\n",
              "          1.0680524e-01, -0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
              "          0.0000000e+00, -6.4338589e-01,  0.0000000e+00,  0.0000000e+00,\n",
              "         -9.7646445e-01,  0.0000000e+00,  0.0000000e+00, -0.0000000e+00,\n",
              "         -4.5837779e-03,  0.0000000e+00, -0.0000000e+00,  0.0000000e+00,\n",
              "          0.0000000e+00, -7.2626758e-01,  1.0000000e+00,  0.0000000e+00,\n",
              "         -0.0000000e+00,  0.0000000e+00,  1.8077916e-01, -0.0000000e+00,\n",
              "         -0.0000000e+00,  3.1599915e-01, -7.0703638e-01,  0.0000000e+00,\n",
              "          0.0000000e+00, -3.6475569e-01,  0.0000000e+00,  0.0000000e+00,\n",
              "         -0.0000000e+00,  2.0247820e-01,  9.1961674e-02, -1.0000000e+00,\n",
              "         -0.0000000e+00, -0.0000000e+00, -0.0000000e+00, -4.7337539e-03,\n",
              "         -0.0000000e+00, -0.0000000e+00,  3.1825322e-01,  0.0000000e+00,\n",
              "          3.3561704e-03,  0.0000000e+00,  9.5703237e-02, -9.9436027e-01,\n",
              "         -0.0000000e+00, -0.0000000e+00,  0.0000000e+00, -6.5144464e-05,\n",
              "         -0.0000000e+00,  1.5679682e-02,  0.0000000e+00, -0.0000000e+00,\n",
              "         -0.0000000e+00,  4.9511697e-02,  8.6975589e-02,  2.4072388e-01,\n",
              "          1.7965989e-30,  0.0000000e+00, -0.0000000e+00,  2.3980862e-02,\n",
              "         -0.0000000e+00, -0.0000000e+00,  0.0000000e+00, -0.0000000e+00,\n",
              "          0.0000000e+00,  0.0000000e+00, -5.6163829e-02, -9.9846846e-01]],\n",
              "       dtype=float32),\n",
              " array([[ 0.00000000e+00, -0.00000000e+00, -2.98531342e+01,\n",
              "          0.00000000e+00,  0.00000000e+00,  5.01047707e+01,\n",
              "          1.34669174e+02,  2.75115460e-01,  7.64443817e+01,\n",
              "         -1.37504721e+00, -4.31306332e-01,  5.30959675e-28,\n",
              "         -9.65302825e-01, -1.76891494e+00,  0.00000000e+00,\n",
              "          0.00000000e+00, -1.97083485e+00,  0.00000000e+00,\n",
              "          3.09288409e-31,  3.63989830e+00, -1.22796074e+02,\n",
              "         -1.06463289e+01,  0.00000000e+00, -0.00000000e+00,\n",
              "          0.00000000e+00,  1.52326233e+02, -6.13411427e-01,\n",
              "         -1.31800199e+00,  5.11115372e-01,  3.95650327e-01,\n",
              "          4.47818041e+00,  0.00000000e+00, -2.82499511e-02,\n",
              "         -1.50106781e+02, -9.98390656e+01, -9.97066736e-01,\n",
              "          0.00000000e+00, -1.47781253e+00, -1.52559143e+02,\n",
              "          5.97162971e+01, -1.15213966e+00, -6.47213936e+00,\n",
              "          3.40338945e+00,  5.45292816e+01, -1.35125923e+00,\n",
              "          0.00000000e+00, -9.55139637e+00,  5.35434306e-01,\n",
              "         -4.20049953e+00, -0.00000000e+00,  8.07391281e+01,\n",
              "         -0.00000000e+00, -1.34347717e+02, -2.83787346e+00,\n",
              "          8.53425522e+01,  1.00287371e+01, -0.00000000e+00,\n",
              "          3.89394760e-02,  7.21604767e+01, -2.84739571e+01,\n",
              "          1.49858141e+00,  8.66559505e-01,  0.00000000e+00,\n",
              "          5.27181764e-23,  0.00000000e+00,  0.00000000e+00,\n",
              "         -2.94841194e+00, -2.42356211e-01,  7.82024956e+00,\n",
              "          0.00000000e+00, -2.48831773e+00,  2.10909100e+01,\n",
              "          1.51339096e+02, -7.86861944e+00,  0.00000000e+00,\n",
              "         -1.09752560e+00, -8.30106125e+01,  1.05638433e+00,\n",
              "         -1.44126923e+02, -1.39781160e+01, -0.00000000e+00,\n",
              "          0.00000000e+00,  9.87113647e+01,  4.58214712e+00,\n",
              "          3.48192382e+00,  0.00000000e+00,  9.85414684e-02,\n",
              "         -5.12531340e-01,  1.79604130e+01,  1.56015730e+01,\n",
              "          4.37174606e+00, -3.79684687e-01,  1.07262921e+00,\n",
              "         -7.01352775e-01,  0.00000000e+00, -7.02870488e-01,\n",
              "          4.47108955e+01,  1.00930511e+02,  0.00000000e+00,\n",
              "         -6.31350219e-01,  1.52941370e+00,  1.47939575e+02,\n",
              "          9.42515259e+01, -7.03328323e+00,  1.59019928e+01,\n",
              "         -1.40816402e+00, -1.14104644e-30, -4.34135765e-01,\n",
              "          5.03126860e-01,  3.51720482e-05, -6.04815602e-01,\n",
              "         -2.18326032e-01, -1.26794418e+02,  1.80413389e+00,\n",
              "         -1.26950867e+02, -2.67322235e+01, -1.13064095e-01,\n",
              "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "          0.00000000e+00, -1.75762594e+00, -7.57123709e-01,\n",
              "         -0.00000000e+00,  5.61144233e-01,  4.04905224e+00,\n",
              "          5.16400814e-01, -4.60827446e+00, -4.16803002e-01,\n",
              "         -1.49123383e+02,  3.38845491e+00,  3.63321020e-03,\n",
              "          0.00000000e+00, -7.07973862e+01, -1.48403478e+00,\n",
              "         -4.20976996e-01, -9.46586132e-01, -5.55929899e+00,\n",
              "          0.00000000e+00, -3.66741586e+00, -1.50491745e+02,\n",
              "         -1.30656372e+02,  1.25606880e+02, -6.02356613e-01,\n",
              "          8.84825349e-01,  0.00000000e+00,  0.00000000e+00,\n",
              "          5.57098269e-01,  2.85795659e-01, -7.32855141e-01,\n",
              "          1.58492327e+00,  0.00000000e+00,  1.47897632e-36,\n",
              "          0.00000000e+00, -8.95267391e+00,  4.56423424e-02,\n",
              "         -6.76231384e-01,  1.21814432e+01,  0.00000000e+00,\n",
              "          6.62447023e+00, -6.24138975e+00, -1.23128414e+00,\n",
              "          1.51434036e+02, -1.85090661e-01,  1.33262224e+01,\n",
              "          9.35149908e-01, -8.43520508e+01, -0.00000000e+00,\n",
              "         -1.87059665e+00, -1.36203461e+02, -1.15526062e+02,\n",
              "          0.00000000e+00,  0.00000000e+00, -5.28743744e+00,\n",
              "         -4.28142399e-03,  0.00000000e+00,  5.57438431e+01,\n",
              "         -1.12444803e-01, -7.98213363e-01, -3.45038652e-01,\n",
              "         -9.80087146e-02,  1.07140183e+00,  0.00000000e+00,\n",
              "          1.19289026e+01,  8.01095104e+00, -1.92479014e+00,\n",
              "          2.59952545e+00,  1.58299446e+00,  0.00000000e+00,\n",
              "         -2.12397432e+00,  0.00000000e+00,  5.42274475e-01,\n",
              "         -2.21527672e+00,  0.00000000e+00,  1.75964146e+01,\n",
              "         -2.75903702e+00, -1.19178796e+00,  0.00000000e+00,\n",
              "         -3.62383866e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         -7.28727102e+00,  1.40233490e+02,  1.38978500e+02,\n",
              "         -1.32205475e+02,  0.00000000e+00,  1.07438957e+02,\n",
              "         -1.68638859e+01, -1.54776020e+01,  5.51546693e-01,\n",
              "         -3.50762100e+01,  3.59794855e+00,  0.00000000e+00,\n",
              "         -1.29502472e+02,  0.00000000e+00,  1.01906616e+02,\n",
              "         -1.14045391e+01,  3.80389661e-01,  5.82386672e-01,\n",
              "         -2.35989819e+01, -0.00000000e+00, -0.00000000e+00,\n",
              "         -6.28876114e+00, -1.21654216e-02, -0.00000000e+00,\n",
              "         -1.35992233e+02,  5.54244578e-01,  0.00000000e+00,\n",
              "          6.99225161e-03,  5.47310486e+01,  1.03944087e+00,\n",
              "         -2.93411875e+00, -7.10304642e+00, -0.00000000e+00,\n",
              "          0.00000000e+00, -1.34732662e-04, -1.46412430e+01,\n",
              "          2.07393646e+00,  0.00000000e+00, -5.52425041e+01,\n",
              "         -4.39005566e+00,  1.45118043e-01,  2.00735474e+00,\n",
              "          1.41495163e+02,  2.80820861e-30,  1.17513478e-01,\n",
              "         -2.34038830e-01,  3.31161946e-01, -7.67882442e+00,\n",
              "         -9.32745590e+01,  2.23502254e+01, -1.27744579e+01,\n",
              "          0.00000000e+00,  2.05016117e+01, -7.43866488e-02,\n",
              "         -3.58693957e+00]], dtype=float32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJ3WbpinoRgH",
        "colab_type": "code",
        "outputId": "e057efd1-d414-45b8-c3ae-35fd3f9822a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.history.history"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'acc': [0.90837691092862,\n",
              "  0.9354104222362614,\n",
              "  0.9352390464916024,\n",
              "  0.9354153230626095,\n",
              "  0.9354104250135388,\n",
              "  0.9354104203135309,\n",
              "  0.9354104260817223,\n",
              "  0.9354985636194975,\n",
              "  0.9354740763650573,\n",
              "  0.9357874575054347,\n",
              "  0.9370115768952182,\n",
              "  0.9379859761097953,\n",
              "  0.9379174260682958,\n",
              "  0.9392345844631127,\n",
              "  0.9392052049277931,\n",
              "  0.9407378007861449,\n",
              "  0.9414135181348383,\n",
              "  0.9425886760475815,\n",
              "  0.94316646681037,\n",
              "  0.9442583754071198,\n",
              "  0.9448704377724706,\n",
              "  0.945830150530757,\n",
              "  0.9463393848429444,\n",
              "  0.9472452385023931,\n",
              "  0.9479160537429181,\n",
              "  0.9490373484977257,\n",
              "  0.9495367941890566,\n",
              "  0.9505209937745098,\n",
              "  0.9513484948852157,\n",
              "  0.9521123431489459,\n",
              "  0.952788058574909,\n",
              "  0.9535029479252395,\n",
              "  0.9545067294524134,\n",
              "  0.9551873378001661,\n",
              "  0.9555986517219133,\n",
              "  0.9564947028741188,\n",
              "  0.9572438636133748,\n",
              "  0.9581693034872787,\n",
              "  0.9589380525773571,\n",
              "  0.9600250680386806,\n",
              "  0.9607056838637185,\n",
              "  0.9617731156742274,\n",
              "  0.9628699238155051,\n",
              "  0.9635407493105926,\n",
              "  0.9643927425893831,\n",
              "  0.9656119639300959,\n",
              "  0.9662093338573278,\n",
              "  0.9673306288257721,\n",
              "  0.9682854411849838,\n",
              "  0.9693332959674166,\n",
              "  0.9702979027156762,\n",
              "  0.9709491358008436,\n",
              "  0.9718402938176227,\n",
              "  0.9728587699192827,\n",
              "  0.9735638546260027,\n",
              "  0.9750034322020829,\n",
              "  0.9753266011087698,\n",
              "  0.9763058887587653,\n",
              "  0.9772019497382598,\n",
              "  0.9780343557344116,\n",
              "  0.9786317256616435,\n",
              "  0.9775593919566028,\n",
              "  0.9804727996976572,\n",
              "  0.9809379712227853,\n",
              "  0.9807323230210171,\n",
              "  0.9815059616146976,\n",
              "  0.9820298909286445,\n",
              "  0.9826272614967866,\n",
              "  0.983200155919598,\n",
              "  0.9838073153222333,\n",
              "  0.9843067580226502,\n",
              "  0.9844830275436456,\n",
              "  0.9850706022272828,\n",
              "  0.9854574263309492,\n",
              "  0.9859127979124745,\n",
              "  0.9859519750413929,\n",
              "  0.9863485916113768,\n",
              "  0.9866374837882202,\n",
              "  0.9869459647431595,\n",
              "  0.9870928637015777,\n",
              "  0.9873034133706041,\n",
              "  0.9874649953671254,\n",
              "  0.9877979580219501,\n",
              "  0.9881309202495014,\n",
              "  0.9882043713309859,\n",
              "  0.9882729111179229,\n",
              "  0.9884149111727233,\n",
              "  0.988522636847684,\n",
              "  0.9881896669292108,\n",
              "  0.9888262088580798,\n",
              "  0.9888360090153192,\n",
              "  0.9890808236641696,\n",
              "  0.9890808277232672,\n",
              "  0.9892375140207216,\n",
              "  0.9892571047215479,\n",
              "  0.9891395863666329,\n",
              "  0.9892522053906567,\n",
              "  0.9893550225483474,\n",
              "  0.9895606775864906,\n",
              "  0.9896292286961736],\n",
              " 'loss': [1.0096772083458507,\n",
              "  0.4459582566146782,\n",
              "  0.5313804481832785,\n",
              "  0.45794955141655436,\n",
              "  0.4284211137602406,\n",
              "  0.43195175376844236,\n",
              "  0.4269149436959229,\n",
              "  0.4198300125991999,\n",
              "  0.40419935496477244,\n",
              "  0.38667374582273556,\n",
              "  0.40547882933770457,\n",
              "  0.35477240901694074,\n",
              "  0.38163947621126754,\n",
              "  0.34341190418889445,\n",
              "  0.3816452809345765,\n",
              "  0.32727003749126177,\n",
              "  0.34477989936387665,\n",
              "  0.31009486867749136,\n",
              "  0.3107977184770782,\n",
              "  0.296052504397635,\n",
              "  0.353442058875142,\n",
              "  0.2829253629757939,\n",
              "  0.2789535340869726,\n",
              "  0.2722864289864844,\n",
              "  0.26841296271611287,\n",
              "  0.25975411120922337,\n",
              "  0.2558082861285056,\n",
              "  0.2514205320761623,\n",
              "  0.2440831141018953,\n",
              "  0.23859162762173616,\n",
              "  0.23346964635729361,\n",
              "  0.23040057777503914,\n",
              "  0.22234650623841098,\n",
              "  0.21799792605702595,\n",
              "  0.21396246621899281,\n",
              "  0.2089877690465647,\n",
              "  0.20390964910975493,\n",
              "  0.19830190708133055,\n",
              "  0.19320864094200954,\n",
              "  0.18858892174177272,\n",
              "  0.18335340911769524,\n",
              "  0.17914873393633032,\n",
              "  0.17309058684602005,\n",
              "  0.1693868309686688,\n",
              "  0.16545846102271883,\n",
              "  0.15966681254807338,\n",
              "  0.15639500052911834,\n",
              "  0.15184295978597415,\n",
              "  0.14713666884488957,\n",
              "  0.14274392450582168,\n",
              "  0.13894881623192928,\n",
              "  0.13497098740924643,\n",
              "  0.13157488849000692,\n",
              "  0.12706793801972516,\n",
              "  0.12411574992654999,\n",
              "  0.12010558029656769,\n",
              "  0.11675590840184988,\n",
              "  0.11371284181178684,\n",
              "  0.11013143573717404,\n",
              "  0.1075243863901357,\n",
              "  0.10419286979782966,\n",
              "  0.10961638249483587,\n",
              "  0.09778666661845313,\n",
              "  0.09583109470358031,\n",
              "  0.09459021919837562,\n",
              "  0.09179557988079645,\n",
              "  0.08991102660642303,\n",
              "  0.08756189670507199,\n",
              "  0.08541964699504195,\n",
              "  0.0829613200759375,\n",
              "  0.08074136524324349,\n",
              "  0.07920727577047108,\n",
              "  0.07697737513370412,\n",
              "  0.0753729007470565,\n",
              "  0.07351873160797208,\n",
              "  0.07251052630524482,\n",
              "  0.07038084967696111,\n",
              "  0.06892894291215473,\n",
              "  0.06748521960871194,\n",
              "  0.06643989495265441,\n",
              "  0.06475451943801723,\n",
              "  0.06376133218247404,\n",
              "  0.06274024730942156,\n",
              "  0.06147409663085015,\n",
              "  0.060475144401780166,\n",
              "  0.0594091140214474,\n",
              "  0.05859340213265898,\n",
              "  0.05770886611981204,\n",
              "  0.05873666495405218,\n",
              "  0.05549039102850422,\n",
              "  0.05519866908643408,\n",
              "  0.05436327612848692,\n",
              "  0.05406563385893794,\n",
              "  0.05292724322621113,\n",
              "  0.05244507819520957,\n",
              "  0.05285052322251822,\n",
              "  0.05200746125378062,\n",
              "  0.051306821218955474,\n",
              "  0.0502657447023631,\n",
              "  0.04984785213158549]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHKk1sM1GiVg",
        "colab_type": "code",
        "outputId": "35d39507-bb81-48c9-c21a-d654ac863903",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "for seq_index in range(2):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = encoder_input_data[seq_index: seq_index + 5]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', input_texts[seq_index])\n",
        "    print('Decoded sentence:', decoded_sentence)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: 하르방 \n",
            "Decoded sentence:  감물들인인옷\n",
            "\n",
            "-\n",
            "Input sentence: 할망 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLt1JGEtHGQK",
        "colab_type": "code",
        "outputId": "f13c1c8a-ce68-4f6a-d43f-5a4e9e25c703",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "decoded_sentence[1:3]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'감물'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkuN6RnpGPHz",
        "colab_type": "code",
        "outputId": "dd320e72-d6f3-4351-e0da-33e89b69818b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "# Define sampling models\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)\n",
        "\n",
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict(\n",
        "    (i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict(\n",
        "    (i, char) for char, i in target_token_index.items())\n",
        "\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_char == '\\n' or\n",
        "           len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence\n",
        "\n",
        "\n",
        "for seq_index in range(100):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = encoder_input_data[seq_index: seq_index + 10]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', input_texts[seq_index])\n",
        "    print('Decoded sentence:', decoded_sentence)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: 하르방 \n",
            "Decoded sentence:  감물들인인옷\n",
            "\n",
            "-\n",
            "Input sentence: 할망 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 아방 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 어멍 \n",
            "Decoded sentence:  감물들인인옷\n",
            "\n",
            "-\n",
            "Input sentence: 비바리 \n",
            "Decoded sentence:  돼지\n",
            "\n",
            "-\n",
            "Input sentence: 괸당 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 걸바시 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 넹바리 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 다슴아돌 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 말젯놈 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 소나이 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 성님 \n",
            "Decoded sentence:  여자\n",
            "\n",
            "-\n",
            "Input sentence: 작산 거 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 좀녀 \n",
            "Decoded sentence:  여자\n",
            "\n",
            "-\n",
            "Input sentence: 촐람생이 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 홀아방 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 가달 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 꼴랑지 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 구뚱배기 \n",
            "Decoded sentence:  잠잠하다\n",
            "\n",
            "-\n",
            "Input sentence: 꽝 \n",
            "Decoded sentence:  돼지\n",
            "\n",
            "-\n",
            "Input sentence: 굴레 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 대망생이 \n",
            "Decoded sentence:  잠잠하다\n",
            "\n",
            "-\n",
            "Input sentence: 등땡이 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 또꼬망 \n",
            "Decoded sentence:  여\n",
            "\n",
            "-\n",
            "Input sentence: 모감지 \n",
            "Decoded sentence:  돼지\n",
            "\n",
            "-\n",
            "Input sentence: 베 봉탱이 \n",
            "Decoded sentence:  어 지\n",
            "\n",
            "-\n",
            "Input sentence: 베아지 볼라불라\n",
            "Decoded sentence:  어 지\n",
            "\n",
            "-\n",
            "Input sentence: 상판이 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 야게기 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 야굴탁 \n",
            "Decoded sentence:  잠잠하다\n",
            "\n",
            "-\n",
            "Input sentence: 임댕이 \n",
            "Decoded sentence:  잠잠하다\n",
            "\n",
            "-\n",
            "Input sentence: 정겡이 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 저껭이 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 조금태기 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 좀짐팽이 \n",
            "Decoded sentence:  여\n",
            "\n",
            "-\n",
            "Input sentence: 허운데기 \n",
            "Decoded sentence:  잠하다\n",
            "\n",
            "-\n",
            "Input sentence: 허벅다리 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 놋 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 간수메 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 개역 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 것 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 괴기 \n",
            "Decoded sentence:  여\n",
            "\n",
            "-\n",
            "Input sentence: 바당괴기 \n",
            "Decoded sentence:  여자\n",
            "\n",
            "-\n",
            "Input sentence: 돗괴기 \n",
            "Decoded sentence:  여자\n",
            "\n",
            "-\n",
            "Input sentence: 쇠괴기 \n",
            "Decoded sentence:  여자\n",
            "\n",
            "-\n",
            "Input sentence: 도괴기 \n",
            "Decoded sentence:  여\n",
            "\n",
            "-\n",
            "Input sentence: 곤떡 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 곤밥 \n",
            "Decoded sentence:  여\n",
            "\n",
            "-\n",
            "Input sentence: 놈삐 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 대사니김치 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 마농 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 마농 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 조배기 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 촐래 \n",
            "Decoded sentence:  여\n",
            "\n",
            "-\n",
            "Input sentence: 촘지금 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 짐치 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 촙쏠 \n",
            "Decoded sentence:  여자\n",
            "\n",
            "-\n",
            "Input sentence: 조팝 \n",
            "Decoded sentence:  돼지\n",
            "\n",
            "-\n",
            "Input sentence: 갈옷 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 갈 적삼 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 갈 중이 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 강알터진 바지 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 게와 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 단취 \n",
            "Decoded sentence:  여자\n",
            "\n",
            "-\n",
            "Input sentence: 밀랑 페랭이 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 보선 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 소중이 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 신착 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 찍신 \n",
            "Decoded sentence:  잠잠하다\n",
            "\n",
            "-\n",
            "Input sentence: 좀뱅이 \n",
            "Decoded sentence:  잠잠하다\n",
            "\n",
            "-\n",
            "Input sentence: 등지게 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 고장중이 \n",
            "Decoded sentence:  돼지\n",
            "\n",
            "-\n",
            "Input sentence: 도폭 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 두루막 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 베불레기 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 우장 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 저구리 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 지성귀 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 지서귀 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 쪼께 \n",
            "Decoded sentence:  여자\n",
            "\n",
            "-\n",
            "Input sentence: 치메 \n",
            "Decoded sentence:  돼지\n",
            "\n",
            "-\n",
            "Input sentence: 건대 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 사모관대 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 시미옷 \n",
            "Decoded sentence:  여자\n",
            "\n",
            "-\n",
            "Input sentence: 제복 \n",
            "Decoded sentence:  여\n",
            "\n",
            "-\n",
            "Input sentence: 망근 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 방립 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 벙것 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 상갓 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 탕근 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 풍뎅이 \n",
            "Decoded sentence:  여\n",
            "\n",
            "-\n",
            "Input sentence: 휘양 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 낭저 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 달리 \n",
            "Decoded sentence:  감물들인 옷\n",
            "\n",
            "-\n",
            "Input sentence: 빈네 \n",
            "Decoded sentence:  여자\n",
            "\n",
            "-\n",
            "Input sentence: 상퉁이 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 얼레기 \n",
            "Decoded sentence:  잠잠하다\n",
            "\n",
            "-\n",
            "Input sentence: 얼레빗 \n",
            "Decoded sentence:  잠잠하다\n",
            "\n",
            "-\n",
            "Input sentence: 쪽도리 \n",
            "Decoded sentence:  바지\n",
            "\n",
            "-\n",
            "Input sentence: 쳉빗 \n",
            "Decoded sentence:  돼지\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}