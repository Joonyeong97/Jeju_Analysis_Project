{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 #32, 64\n",
    "epochs = 100\n",
    "latent_dim = 256\n",
    "num_samples = 5000\n",
    "data_path = 'jeju_final.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [] # 제주어\n",
    "target_texts = [] # 표준어\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[:min(num_samples, len(lines)-1)]:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    target_text = target_text.strip() # 타겟 데이터 앞뒤 구분자는 뒤어서 넣어주자\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Komoran\n",
    "from konlpy.tag import Hannanum\n",
    "from konlpy.tag import Mecab\n",
    "from konlpy.tag import Okt\n",
    "import json\n",
    "import os\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_morph_to_json(dataset, json_path, package='Okt'):\n",
    "    if package == 'Kkma':\n",
    "        tokenizer = Kkma()\n",
    "    elif package == 'Komoran':\n",
    "        tokenizer = Komoran()\n",
    "    elif package == 'Hannanum':\n",
    "        tokenizer = Hannanum()\n",
    "    elif package == 'Mecab':\n",
    "        tokenizer = Mecab()\n",
    "    else:\n",
    "        tokenizer = Okt()\n",
    "    tokenized = [tokenizer.morphs(row) for row in dataset]\n",
    "    length = []\n",
    "    for tok in tokenized:\n",
    "        length.append(len(tok))\n",
    "    max_token_length = max(length)\n",
    "    \n",
    "    with open(json_path, 'w', encoding='utf-8') as make_file:\n",
    "        json.dump(tokenized, make_file, ensure_ascii=False, indent='\\t')\n",
    "    return max_token_length\n",
    "\n",
    "#make_morph_to_json(target_texts, 'mecab_seoul.json', Mecab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 정의 사전 okt -> komoran으로 맵핑\n",
    "with open('jeju_vocab.txt', encoding='utf-8-sig') as f:\n",
    "    fn = open('jeju_dic.txt', 'w')\n",
    "    for line in f:\n",
    "        dic, pos = line.split('\\t')\n",
    "        pos = pos.lower().strip()\n",
    "        if pos in 'noun':\n",
    "            pos = 'NN'\n",
    "        elif pos in 'verb':\n",
    "            pos = 'VV'\n",
    "        elif pos in 'adverb' or pos in 'adberb':\n",
    "            pos = 'MA'\n",
    "        elif pos in 'determiner':\n",
    "            pos = 'NP'\n",
    "        elif pos in 'exclamation':\n",
    "            pos = 'IC'\n",
    "        elif pos in 'adjective':\n",
    "            pos = 'VA'\n",
    "        elif pos in 'suffix':\n",
    "            pos = 'XS'\n",
    "        elif pos in 'josa':\n",
    "            pos = 'JK'\n",
    "        elif pos in 'number':\n",
    "            pose = 'SN'\n",
    "        else:\n",
    "            pass\n",
    "        new_line = dic + '\\t' + pos + '\\n'\n",
    "        fn.write(new_line)\n",
    "    fn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 정의 사전 적용해서 형태소 분석 (Komoran)\n",
    "jeju_dic = 'jeju_dic.txt'\n",
    "komoran = Komoran(userdic='jeju_dic.txt')\n",
    "tokenized = [komoran.morphs(row) for row in input_texts]\n",
    "token_length = []\n",
    "for token in tokenized:\n",
    "    token_length.append(len(token))\n",
    "max_token_length = max(token_length)\n",
    "#with open('jeju_userdic.json', 'w', encoding='utf-8') as make_file:\n",
    "#    json.dump(tokenized, make_file, ensure_ascii=False, indent='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json 불러와서 딕셔너리 만들기\n",
    "def make_dictionary_morph(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        morph_dictionary = set()\n",
    "        for sentence in json_data:\n",
    "            for morph in sentence:\n",
    "                if morph not in morph_dictionary:\n",
    "                    morph_dictionary.add(morph)\n",
    "        morph_dictionary = sorted(list(morph_dictionary))\n",
    "    return morph_dictionary, json_data\n",
    "\n",
    "#morph_dictionary, kkma_morphs = make_dictionary_morph(json_path)\n",
    "#target_to_index, target_reverse_index, len_target_tokens = make_token_index(morph_dictionary)\n",
    "#morph_dictionary, json_data = make_dictionary_morph('jeju_userdic.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰 인덱스\n",
    "def make_token_index(dictionary):\n",
    "    to_index = [(char, i) for i, char in enumerate(dictionary)]\n",
    "    to_index.append(('<unk>', len(to_index)))\n",
    "    to_index.append((' ', len(to_index)))\n",
    "    to_index = dict(to_index)\n",
    "    reverse_index = [(i, char) for i, char in enumerate(dictionary)]\n",
    "    reverse_index.append((len(reverse_index), '<unk>'))\n",
    "    reverse_index.append((len(reverse_index), ' '))\n",
    "    reverse_index = dict(reverse_index)\n",
    "    return to_index, reverse_index, len(to_index)\n",
    "\n",
    "#make_token_index(make_dictionary_char(input_texts))\n",
    "#make_token_index(morph_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트를 해봅시다\n",
    "# 코모란으로, 타겟(표준어)은 걍 돌리고 제주어는 사용자 정의한 걸로\n",
    "\n",
    "input_path = 'jeju_userdic.json'\n",
    "target_path = 'seoul_Komoran.json'\n",
    "max_encoder_length = max_token_length\n",
    "max_decoder_length = make_morph_to_json(target_texts, target_path, package='Komoran')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_morph_dic, input_morph_data = make_dictionary_morph(input_path)\n",
    "target_morph_dic, target_morph_data = make_dictionary_morph(target_path)\n",
    "\n",
    "input_index, input_reverse_index, num_encoder_tokens = make_token_index(input_morph_dic)\n",
    "target_index, target_reverse_index, num_decoder_tokens = make_token_index(target_morph_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 건, 총 데이터 길이 / 문장 길이 / 토큰 길이\n",
    "# 원핫인코딩 ㅠㅠ 밖에 못헤ㅐㅇ잉 ㅠㅠㅠ 임베딩은 ... 추후 과제....ㅎ\n",
    "\n",
    "encoder_input_data = np.zeros((len(input_texts), max_encoder_length, num_encoder_tokens),\n",
    "                             dtype='float32')\n",
    "decoder_input_data = np.zeros((len(target_texts), max_decoder_length, num_decoder_tokens),\n",
    "                             dtype='float32')\n",
    "decoder_target_data = np.zeros((len(target_texts), max_decoder_length, num_decoder_tokens),\n",
    "                              dtype='float32')\n",
    "\n",
    "for i, sentence in enumerate(input_morph_data):\n",
    "    for j, token in enumerate(sentence):\n",
    "        encoder_input_data[i, j, input_index[tok]] = 1.\n",
    "    encoder_input_data[i, j+1:, input_index[' ']] = 1.\n",
    "for i, sentence in enumerate(target_morph_data):\n",
    "    for j, token in enumerate(sentence):\n",
    "        decoder_input_data[i, j, target_index[tok]] = 1.\n",
    "        if j > 0:\n",
    "            decoder_target_data[i, j-1, target_index[tok]] = 1.\n",
    "    decoder_input_data[i, j+1:, target_index[' ']] = 1.\n",
    "    decoder_target_data[i, j:, target_index[' ']] = 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(466, 20, 716)\n",
      "(466, 28, 708)\n",
      "(466, 20, 716)\n"
     ]
    }
   ],
   "source": [
    "print(decoder_input_data.shape)\n",
    "print(encoder_input_data.shape)\n",
    "print(decoder_target_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "         batch_size = batch_size, epochs=epochs, validatin_split=0.2)\n",
    "model.savel('k_s2s.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
